{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbfe8b6e-ca81-4bf0-8cbc-e7503784f06a",
   "metadata": {},
   "source": [
    "# CNN ResNet Explainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f393843-8889-4c95-9c49-759a8e264095",
   "metadata": {},
   "source": [
    "Convolutional Neural Nets, or CNNs, learn the pattern in data by sliding small learnable filters across the input data to spot local patterns, like short “n-gram” features, and turns them into higher-level signals. Because of this, CNNs are most commonly used for analyzing spatially structured data, like images or videos, because they can efficiently learn local patterns such as edges, textures, and shapes. They are also used in natural language processing and time-series tasks, where the same idea of sliding filters helps capture local dependencies in text or signal data. Modern architectures extend CNNs to higher-level tasks such as object detection, segmentation, and even audio or biological sequence modeling.\n",
    "\n",
    "For our example we will be taking text and using our embedding layer to add a second dimension to it for the CNN to learn across.  Recall that a discrete convolution of 2 matrices results in summation of a series of element-wise dot products. \n",
    "$$\n",
    "(a * b)_n = \\sum_{\\substack{i,j\\\\ i+j=n}} a_i \\cdot b_j\n",
    "$$\n",
    "\n",
    "In our example, the embedding of the input sequence is, $A$. We pad $A$ so the output has the same length as the input. The learnable kernel weights are (B). Each output value is the dot product between a local patch of $A$ and $B$ running our discrete cross-correlation. We also include the stride controls how far the kernel window moves along $A$ to show how we can downsample A. \n",
    "\n",
    "Because of this, we actually run a different calculation, similar to a convolution called the 2-D discrete cross-correlation. With the input reshaped to $[B,C,1,T]$ and a $1\\times k$ kernel, each output token index $t$ is\n",
    "$$\n",
    "y_{t}=\\sum_{c=1}^{C}\\sum_{u=0}^{k-1} W_{c,u} x_{c,,t+u}\\quad\n",
    "$$\n",
    "\n",
    "\n",
    "To help display how the CNNs works, we'll actually use the c-major note letters for 4 popular songs: [Hot Cross Buns](https://en.wikipedia.org/wiki/Hot_Cross_Buns_(song)), [Twinkle Twinkle Little Star](https://en.wikipedia.org/wiki/Twinkle,_Twinkle,_Little_Star), and [Happy Birthday To You](https://en.wikipedia.org/wiki/Happy_Birthday_to_You), [Mary Had a Little Lamb](https://en.wikipedia.org/wiki/Mary_Had_a_Little_Lamb), and [Frère Jacques](https://en.wikipedia.org/wiki/Fr%C3%A8re_Jacques). \n",
    "\n",
    "In today's notebooks we'll take in 2 different examples and predict the next note from them. In other notebooks you might have seen that we predicted many examples in each batch during a loop. Since we are using our `input X embedding`, we'll just have a single example in each batch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5968de25-053a-45ce-9028-c03e90bdfa7c",
   "metadata": {},
   "source": [
    "## Text Prep/Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2685be2-e06d-4009-97b5-f26a3f61bac4",
   "metadata": {},
   "source": [
    "We'll start with a common preprocessing step of tokenizing the data.  This converts the string text into an array of numbers that can be used during the training loop.  I've built a very small byte-pair encoding that has each unique character that appears and the top 6 merges to give us a total of 15 tokens in our vocab. This keeps our vocab size small and manageable for this example. Typically the vocab size is in the 100K+ range. A great library for this is `tiktoken`. Tokenization simply finds the longest pattern of characters that's in common with what was trained and replaces it with an integer that represents it.  This way we turn the text into a numeric array to simplify computing. import torch\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a6ac3c8-73c9-4b54-b6eb-233b5c405223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4919114e-83f0-4d2a-8a97-c4648da21f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBPETokenizer:\n",
    "    def __init__(self, num_merges=5, eot_token='<|endoftext|>'):\n",
    "        self.num_merges = num_merges\n",
    "        self.eot_token = eot_token\n",
    "        self.eot_id = None\n",
    "        self.merges = []\n",
    "        self.pair_ranks = {}\n",
    "        self.vocab = {}\n",
    "        self.id_to_token = {}\n",
    "\n",
    "    def _add_token(self, tok):\n",
    "        if tok in self.vocab:\n",
    "            return self.vocab[tok]\n",
    "        i = len(self.vocab)\n",
    "        self.vocab[tok] = i\n",
    "        self.id_to_token[i] = tok\n",
    "        return i\n",
    "\n",
    "    def _get_bigrams(self, seq):\n",
    "        for i in range(len(seq) - 1):\n",
    "            yield (seq[i], seq[i + 1])\n",
    "\n",
    "    def _merge_once(self, seq, pair):\n",
    "        a, b = pair\n",
    "        out = []\n",
    "        i = 0\n",
    "        while i < len(seq):\n",
    "            if i < len(seq) - 1 and seq[i] == a and seq[i + 1] == b:\n",
    "                out.append(a + b)\n",
    "                i += 2\n",
    "            else:\n",
    "                out.append(seq[i])\n",
    "                i += 1\n",
    "        return out\n",
    "\n",
    "    def train(self, corpus):\n",
    "        # corpus: list[str]\n",
    "        text = ''.join(corpus).lower()\n",
    "        seq = list(text)\n",
    "        merges = []\n",
    "        for _ in range(self.num_merges):\n",
    "            counts = Counter(self._get_bigrams(seq))\n",
    "            if not counts: break\n",
    "            best_pair, _ = counts.most_common(1)[0]\n",
    "            merges.append(best_pair)\n",
    "            seq = self._merge_once(seq, best_pair)\n",
    "        self.merges = merges\n",
    "        self.pair_ranks = {p: i for i, p in enumerate(self.merges)}\n",
    "\n",
    "        self.vocab = {}\n",
    "        self.id_to_token = {}\n",
    "        for ch in sorted(set(text)):\n",
    "            self._add_token(ch)\n",
    "        for a, b in self.merges:\n",
    "            self._add_token(a + b)\n",
    "        self.eot_id = self._add_token(self.eot_token)\n",
    "\n",
    "    def encode(self, text, force_last_eot=True):\n",
    "        # treat literal eot marker as special; remove it from content\n",
    "        if self.eot_token in text:\n",
    "            text = text.replace(self.eot_token, '')\n",
    "        seq = list(text)\n",
    "\n",
    "        # make sure all seen base chars exist\n",
    "        for ch in set(seq):\n",
    "            if ch not in self.vocab:\n",
    "                self._add_token(ch)\n",
    "\n",
    "        # greedy BPE using learned pair ranks\n",
    "        if self.merges:\n",
    "            while True:\n",
    "                best_pair, best_rank = None, None\n",
    "                for p in self._get_bigrams(seq):\n",
    "                    r = self.pair_ranks.get(p)\n",
    "                    if r is not None and (best_rank is None or r < best_rank):\n",
    "                        best_pair, best_rank = p, r\n",
    "                if best_pair is None:\n",
    "                    break\n",
    "                seq = self._merge_once(seq, best_pair)\n",
    "\n",
    "        # ensure all tokens in seq exist in vocab (e.g., if new chars appeared)\n",
    "        for tok in seq:\n",
    "            if tok not in self.vocab:\n",
    "                self._add_token(tok)\n",
    "\n",
    "        ids = [self.vocab[tok] for tok in seq]\n",
    "\n",
    "        # FORCE: append EOT id if not already last\n",
    "        if force_last_eot:\n",
    "            if not ids or ids[-1] != self.eot_id:\n",
    "                ids.append(self.eot_id)\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # drop trailing EOT if present\n",
    "        if ids and self.eot_id is not None and ids[-1] == self.eot_id:\n",
    "            ids = ids[:-1]\n",
    "        toks = [self.id_to_token[i] for i in ids]\n",
    "        return ''.join(toks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edf21736-94b1-44f1-9e90-833807b38e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "twinkle_twinkle = r'CCGGAAG,FFEEDDC,GGFFEED,GGFFEED,CCGGAAG,FFEEDDC'\n",
    "hot_cross_buns = r'EDC,EDC,CCCC,DDDD,EDC'\n",
    "happy_birthday = r'GGAGCB,GGAGDC,GGGECBA,FFECDC'\n",
    "mary_had_a_little_lamb = r'EDCDEEE,DDD,EGG,EDCDEEE,EDD,EDC'\n",
    "frere_jacques = r'CDEC,CDEC,EFG,EFG,GAGFEC,GAGFEC,CGC,CGC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb02db67-40d1-4a88-95a1-c497e3bfdaad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('e', 'd'), ('c', ','), ('g', 'g'), ('f', 'e'), ('a', 'g'), ('c', 'c')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = SimpleBPETokenizer(num_merges=6)\n",
    "examples = [twinkle_twinkle,hot_cross_buns, happy_birthday, mary_had_a_little_lamb, frere_jacques]\n",
    "tok.train(examples)\n",
    "tok.merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94546825-52f3-4668-bc9c-80bed6d0a444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',': 0,\n",
       " 'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'ed': 8,\n",
       " 'c,': 9,\n",
       " 'gg': 10,\n",
       " 'fe': 11,\n",
       " 'ag': 12,\n",
       " 'cc': 13,\n",
       " '<|endoftext|>': 14}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a47bae6f-3a84-4677-8b9a-5851a27ccff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tok.vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "210d0bdb-2e94-4788-8feb-9a445dfaa12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 13, 10,  1, 12,  0,  6, 11,  8,  4,  9, 10,  6, 11,  8,  0, 10,  6,\n",
       "        11,  8,  0, 13, 10,  1, 12,  0,  6, 11,  8,  4,  3, 14, 14,  8,  9,  8,\n",
       "         9, 13,  3,  9,  4,  4,  4,  4,  0,  8,  3, 14, 14, 10, 12,  3,  2,  0,\n",
       "        10, 12,  4,  9, 10,  7,  5,  3,  2,  1,  0,  6, 11,  3,  4,  3, 14, 14,\n",
       "         8,  3,  4,  5,  5,  5,  0,  4,  4,  4,  0,  5, 10,  0,  8,  3,  4,  5,\n",
       "         5,  5,  0,  8,  4,  0,  8,  3, 14, 14,  3,  4,  5,  9,  3,  4,  5,  9,\n",
       "         5,  6,  7,  0,  5,  6,  7,  0,  7, 12, 11,  9,  7, 12, 11,  9,  3,  7,\n",
       "         9,  3,  7,  3, 14])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eot = tok.eot_id\n",
    "tokens = []\n",
    "for example in examples:\n",
    "    tokens.extend([eot])\n",
    "    tokens.extend(tok.encode(example.lower()))\n",
    "all_tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f45597-e81b-4dfb-a34a-fb210883041d",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51779778-ccaf-4baf-b009-03c11fec951e",
   "metadata": {},
   "source": [
    "A machine learning model forward pass now uses the tokenization information, runs several layers of linear algebra on it, and then \"predicts\" the probability that each token in the vocab is next. When it is noisy (like you will see in this example), this process results in gibberish.  The training process changes the noise to pattern during the \"backward pass\" as you'll see. We'll show 3 steps that are focused on training:\n",
    "1. **Data Loading** `x, y = train_loader.next_batch()` - this step pulls from the raw data enough tokens to complete a forward pass and loss calculation.  If the model is inference only, this step is replaced with taking in the inference input and preparing it similarly as the forward pass.\n",
    "2. **Forward Pass** `logits, loss = model(x, y)` - using the data and the model architecture we run a prediction for the tokens. When training we also compare against the expected to get loss, but in inference, we use the logits to complete the inference task.\n",
    "3. **Back Propagation, aka Backward Pass & Training** `loss.backward(); optimizer.step()` - using differentials to understand what parameters most impact the forward pass' impact on its prediction, comparing that against what is actually right based on the data loading step, and then making very minor adjustments to the impactful parameters with the hope it improves future predictions.\n",
    "\n",
    "The we'll show a final **Forward Pass** with the updated weights we did in #3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aefcbe-b05e-4e9a-a82f-46089bad50a5",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8293b18-5bdf-4693-82b3-466c20d679e9",
   "metadata": {},
   "source": [
    "To start, we need to get enough data to run the forward and backward passes.  Since our total dataset is likely too big to be held in memory all at once in real practice, we will read just enough file information into memory so that we can run the passes, leaving memory and compute to be used on the passes instead of static data holding. \n",
    "To start, we have to identify the batch size and the model context length to determine how much data we need.  Consequently, these dimensions also form 2 of the 3 dimensions in the initial matrix.\n",
    "- **Batch Size (B)** - This is the number of examples you'll train on in a single pass. \n",
    "- **Context Length (T)** - This is the max number of tokens that a model can use in a single pass to generate the next token. If an example is below this length, it can be padded.\n",
    "  \n",
    "*Ideally both B and T are multiples of 2 to work nicely with chip architecture. This is a common theme across the board*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "454fc40b-459c-457c-b5db-db0733549246",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_batch = 2 # Batch\n",
    "T_context = 8 # context length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62799c29-4030-4f06-af11-0192aa61b1e3",
   "metadata": {},
   "source": [
    "To start, we need to pull from our long raw_token list enough tokens for the forward pass. To be able to satisfy training `B_batch` Batches `T_context` context length, we need to pull out `B*T` tokens to slide the context window across the examples enough to satisfy the batch size. Since our training will attempt to predict the next token after the context, we also need 1 more token at the end so that the last last batch can have the next token to validate against. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68c595f6-0538-4a0b-b666-996fdc30f1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 13, 10,  1, 12,  0,  6, 11,  8,  4,  9, 10,  6, 11,  8,  0, 10])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_position = 0\n",
    "tok_for_training = all_tokens[current_position:current_position + B_batch*T_context +1 ]\n",
    "tok_for_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6978a23-ad08-4ad3-870c-079b05a26b64",
   "metadata": {},
   "source": [
    "Now that we have our initial tokens to train on, we now need to convert it to a matrix that's ready for training. In this step we'll need to create our batches and setup two different arrays: 1/ the input, `x`, tokens that will result in 2/ the output `y` tokens. To create each example in the batch, every `T` tokens will be placed into its own row. \n",
    "\n",
    "Recall that training takes in a string of tokens the length of the context and then predicts the next token. Recall that when we extracted `tok_for_training` we added 1 extra token so that we can evaluate the prediction for the last example. Because of this, the input, `x`, will be all of the tokens up to the second to last element `[:-1]`.  \n",
    "\n",
    "\n",
    "Finally, for `y` we will need to extract a token for every batch. That token will be the one immediately following the context length or every token at positions `B*T_context +1` where B corresponds to a multiple of every batch. \n",
    "\n",
    "We will now put this together and do the following:\n",
    "1. Extract the input `x` and then split it into an example for each batch `B`\n",
    "2. Extract the output `y` and then split it into an example for each batch `B`\n",
    "\n",
    "*Note: View can take `-1` which allows the matrix to infer the dimension so we do not need to pass in `T`, but given how many matrices we'll work with we want to make sure we're controlling the dimensions or erroring out if they do not match our expectations.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8632328a-d0c0-4036-abea-601786682dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8]),\n",
       " tensor([[14, 13, 10,  1, 12,  0,  6, 11],\n",
       "         [ 8,  4,  9, 10,  6, 11,  8,  0]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=tok_for_training[:-1].view(B_batch, T_context)\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee388227-0a8c-469d-bd51-c683b323d177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 13, 10,  1, 12,  0,  6, 11,  8,  4,  9, 10,  6, 11,  8,  0, 10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_for_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3440807c-eec6-4dfe-b671-31aaea855172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1]),\n",
       " tensor([[ 8],\n",
       "         [10]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=tok_for_training[T_context::T_context].view(B_batch, 1)\n",
    "y.size(), y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811ab64c-4cae-4095-89b1-4534085323be",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99517d1f-2b40-4db4-9f85-bfdfc5c3b392",
   "metadata": {},
   "source": [
    "<img src=\"explainer_screenshots/cnn/full_network.png\" width=\"200\">\n",
    "\n",
    "\n",
    "During training, in the CNN we've built, the forward pass takes a string of tokens in and predicts the likelihood of the next token for each batch. This is different from the other models we've used as there's only a single example in each batch. This is mainly because CNNs do best with multi-dimension data and so we're hacking our text input for this explainer by using our `text x embedding` to be our 2 dimensions, instead of an image or other 2d data. \n",
    "\n",
    "This explainer for the forward pass is focused on training where we'll pass in the input `x`, carry that input through the layers, and generate a matrix of the probability of each token being the next one, something we call `logits`. During the forward pass, since this is an CNN, we will actually pass each example through different convolution layers and even show downsampling, which reduces our matrix size. \n",
    "\n",
    "At the end of the forward pass we then compare the probability in the logits to the actual next token in `y` and calculate `loss` based on the difference. This difference is what we'll then use in the backprop/training steps.  \n",
    "\n",
    "*Note that we will do some layer initialization to simplify following along.  In reality, layers are often initialized to normal distribution with some adjustments made for parameter sizes, called Kaiming normal, to keep the weights properly noisy.  We will not cover initialization in this series*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b16235f-5aec-4268-baae-f36a2a377b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_batch, T_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d001e8-02e0-4b9c-8407-6fc7ef07838d",
   "metadata": {},
   "source": [
    "### Input Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20f3332-2ed8-4262-97a4-8d2a8de8473c",
   "metadata": {},
   "source": [
    "<img src=\"explainer_screenshots/cnn/input_layer.png\" width=\"200\">\n",
    "\n",
    "We'll first create an initial **embedding layer** for our input tokens. Recall that this is the layer that will add the second dimension to our text examples. We start with only supplying our embedding positions, though, if we wanted to add more learning capability, we could also do position.  Since CNNs generally take in multi-dimension examples and then use multi-dimension patches for learning in the convolutional layers, position is generally avoided since the goal would be to learn patterns in the data regardless of the position. We will make sure that our embedding weights are larger than 1 to visualize the convolutions well.  The output becomes `vocab_size X n_embd` so that each position can store weights that correspond with each token.  The more embedding layers added the more complex data the model can learn. \n",
    "\n",
    "After the embedding layer we'll then insert in the fourth dimension of 1 to better suit our convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308803fa-ae04-4b10-88b7-78f756b36daa",
   "metadata": {},
   "source": [
    "#### Input Layer - Embedding Projection\n",
    "\n",
    "To start we'll initialize our embeddings with an iterative weight so that we can see how it changes through our convolutions.  \n",
    "of 1.000 so that all inputs are equally weighted. We'll also set our embedding dimension to 6 to allow us to see how our convolution strides across the embedding dimension.  You'll see that because our `x` plucks our different embedding rows, we are quickly adjusting away from the nicely ordered initial embeddings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49238829-17be-4c83-aed1-51810f374b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 15)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_embd = 6 # level of embedding of input tokens\n",
    "n_embd, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9697d1c-02bd-49de-8b78-66de9eef7ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600],\n",
       "        [0.0200, 0.0300, 0.0400, 0.0500, 0.0600, 0.0700],\n",
       "        [0.0300, 0.0400, 0.0500, 0.0600, 0.0700, 0.0800],\n",
       "        [0.0400, 0.0500, 0.0600, 0.0700, 0.0800, 0.0900],\n",
       "        [0.0500, 0.0600, 0.0700, 0.0800, 0.0900, 0.1000],\n",
       "        [0.0600, 0.0700, 0.0800, 0.0900, 0.1000, 0.1100],\n",
       "        [0.0700, 0.0800, 0.0900, 0.1000, 0.1100, 0.1200],\n",
       "        [0.0800, 0.0900, 0.1000, 0.1100, 0.1200, 0.1300],\n",
       "        [0.0900, 0.1000, 0.1100, 0.1200, 0.1300, 0.1400],\n",
       "        [0.1000, 0.1100, 0.1200, 0.1300, 0.1400, 0.1500],\n",
       "        [0.1100, 0.1200, 0.1300, 0.1400, 0.1500, 0.1600],\n",
       "        [0.1200, 0.1300, 0.1400, 0.1500, 0.1600, 0.1700],\n",
       "        [0.1300, 0.1400, 0.1500, 0.1600, 0.1700, 0.1800],\n",
       "        [0.1400, 0.1500, 0.1600, 0.1700, 0.1800, 0.1900],\n",
       "        [0.1500, 0.1600, 0.1700, 0.1800, 0.1900, 0.2000]], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wte = nn.Embedding(vocab_size, n_embd)\n",
    "with torch.no_grad(): # initilize to W[i,j] = 0.001*(1+i+j) for easy following \n",
    "    vs, d = wte.num_embeddings, wte.embedding_dim\n",
    "    rows = torch.arange(vs).unsqueeze(1)  # (vs,1)\n",
    "    cols = torch.arange(d).unsqueeze(0)  # (1,d)\n",
    "    pattern = 0.01*(1 + rows + cols)  # W[i,j] = 0.001*(1+i+j)\n",
    "    wte.weight.copy_(pattern)\n",
    "wte.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f4a3355-fa6b-4396-8643-ae8602bef769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 6]),\n",
       " tensor([[[0.1500, 0.1600, 0.1700, 0.1800, 0.1900, 0.2000],\n",
       "          [0.1400, 0.1500, 0.1600, 0.1700, 0.1800, 0.1900],\n",
       "          [0.1100, 0.1200, 0.1300, 0.1400, 0.1500, 0.1600],\n",
       "          [0.0200, 0.0300, 0.0400, 0.0500, 0.0600, 0.0700],\n",
       "          [0.1300, 0.1400, 0.1500, 0.1600, 0.1700, 0.1800],\n",
       "          [0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600],\n",
       "          [0.0700, 0.0800, 0.0900, 0.1000, 0.1100, 0.1200],\n",
       "          [0.1200, 0.1300, 0.1400, 0.1500, 0.1600, 0.1700]],\n",
       " \n",
       "         [[0.0900, 0.1000, 0.1100, 0.1200, 0.1300, 0.1400],\n",
       "          [0.0500, 0.0600, 0.0700, 0.0800, 0.0900, 0.1000],\n",
       "          [0.1000, 0.1100, 0.1200, 0.1300, 0.1400, 0.1500],\n",
       "          [0.1100, 0.1200, 0.1300, 0.1400, 0.1500, 0.1600],\n",
       "          [0.0700, 0.0800, 0.0900, 0.1000, 0.1100, 0.1200],\n",
       "          [0.1200, 0.1300, 0.1400, 0.1500, 0.1600, 0.1700],\n",
       "          [0.0900, 0.1000, 0.1100, 0.1200, 0.1300, 0.1400],\n",
       "          [0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600]]],\n",
       "        grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = wte(x)\n",
    "x.shape, x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af748fae-1f4b-4cbe-a361-e3719dcf1908",
   "metadata": {},
   "source": [
    "#### Input Layer - Add Dimension\n",
    "\n",
    "We projected our input tokens `x` that was `[B×T]` into the embedding to get `[B×T×C]` so that we now have our `T×C` for each batch. To run our convolution per batch, though, we also need a spatial dimension for the kernel to slide over. PyTorch-style convolution layers expect tensors in `[B, C, H, W]` (channels-first), where the kernel slides over `H,W` while mixing across `C`. Because of this we add a singleton spatial dimension and reorder axes. With this process, the embedding dimension `C` becomes the channels and the token axis `T` becomes the width to slide across:\n",
    "\n",
    "`[B, T, C]  →  [B, C, T]  →  [B, C, 1, T]`\n",
    "\n",
    "The convolution we show is a `1×k` convolution which slides only along our tokens `T`, and aggregates over all `C` channels at each position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35617286-087d-4897-bbeb-69e6dd5ebde6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 8]),\n",
       " tensor([[[[0.1500, 0.1400, 0.1100, 0.0200, 0.1300, 0.0100, 0.0700, 0.1200]],\n",
       " \n",
       "          [[0.1600, 0.1500, 0.1200, 0.0300, 0.1400, 0.0200, 0.0800, 0.1300]],\n",
       " \n",
       "          [[0.1700, 0.1600, 0.1300, 0.0400, 0.1500, 0.0300, 0.0900, 0.1400]],\n",
       " \n",
       "          [[0.1800, 0.1700, 0.1400, 0.0500, 0.1600, 0.0400, 0.1000, 0.1500]],\n",
       " \n",
       "          [[0.1900, 0.1800, 0.1500, 0.0600, 0.1700, 0.0500, 0.1100, 0.1600]],\n",
       " \n",
       "          [[0.2000, 0.1900, 0.1600, 0.0700, 0.1800, 0.0600, 0.1200, 0.1700]]],\n",
       " \n",
       " \n",
       "         [[[0.0900, 0.0500, 0.1000, 0.1100, 0.0700, 0.1200, 0.0900, 0.0100]],\n",
       " \n",
       "          [[0.1000, 0.0600, 0.1100, 0.1200, 0.0800, 0.1300, 0.1000, 0.0200]],\n",
       " \n",
       "          [[0.1100, 0.0700, 0.1200, 0.1300, 0.0900, 0.1400, 0.1100, 0.0300]],\n",
       " \n",
       "          [[0.1200, 0.0800, 0.1300, 0.1400, 0.1000, 0.1500, 0.1200, 0.0400]],\n",
       " \n",
       "          [[0.1300, 0.0900, 0.1400, 0.1500, 0.1100, 0.1600, 0.1300, 0.0500]],\n",
       " \n",
       "          [[0.1400, 0.1000, 0.1500, 0.1600, 0.1200, 0.1700, 0.1400, 0.0600]]]],\n",
       "        grad_fn=<UnsqueezeBackward0>))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.permute(0,2,1) # [B,C,T]\n",
    "x = x.unsqueeze(2)  # [B,C,1,T]\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82c654c-a572-47d8-ae1a-db0c494193f5",
   "metadata": {},
   "source": [
    "### Convolution Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef77ecbb-ddc8-4f83-a7c0-c6f4cec083d0",
   "metadata": {},
   "source": [
    "<img src=\"explainer_screenshots/cnn/convolutional_layers.png\" width=\"400\">\n",
    "\n",
    "As is common in CNNs, we use multiple convolution layers with normalization and nonlinearity to learn increasingly expressive features from the input. Each convolution “looks” at a local patch whose size and stride we choose; stacking layers (sequentially). We also use residual skips to let the model capture richer patterns and relationships.\n",
    "\n",
    "In our model, our input to the convolution is $[B,C,1,T]$ with a $1\\times k$ kernel. The convolution runs as 2-D discrete cross-correlation along the token axis. For output channel $m$,\n",
    "$$\n",
    "y^{m}_{t}=\\sum_{c=1}^{C}\\sum_{u=0}^{k-1} W^{m}_{c,u}x_{c,ts+u-p}+b^{m}.\n",
    "$$\n",
    "\n",
    "Under the hood we:\n",
    "\n",
    "1. Build the matrix of local patches $P\\in\\mathbb{R}^{(Ck)\\times L}$ by extracting all sliding $1\\times k$ windows; $L$ is the number of output positions.\n",
    "2. Flatten the kernel bank into $W_{\\text{flat}}\\in\\mathbb{R}^{C_{\\text{out}}\\times (Ck)}$.\n",
    "3. Compute all positions at once: $Y = W_{\\text{flat}},P \\in \\mathbb{R}^{C_{\\text{out}}\\times L}$ independently for each batch element, then reshape back to $[B,C_{\\text{out}},1,T_{\\text{out}}]$.\n",
    "\n",
    "We interleave batch normalization and ReLU to stabilize activations, improve gradient flow, and add nonlinearity. \n",
    "\n",
    "The second convolution in the block downsamples with stride 2, reducing the token length $T\\to\\lceil T/2\\rceil$. This both cuts compute and expands the effective receptive field of subsequent layers, helping the model capture longer-range patterns over the sequence.\n",
    "\n",
    "\n",
    "Finally, as a nod to ResNets, the convolutional block also uses a residual path. For this path we add a projected skip $S(x)$ to the main path $F(x)$, yielding $y=F(x)+S(x)$.  Since we used downsampling on our main path, the residual path also uses a $1\\times 1$ projection with stride 2 downsample so dimensions of the residual path match that of the convolutional block output. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f3189e-d94b-4afc-aa96-63ecfff22863",
   "metadata": {},
   "source": [
    "#### Convolution Block - 1x3 Conv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a2ab84-9a4b-4850-9418-f8038ebdea5e",
   "metadata": {},
   "source": [
    "##### 1x3 Conv - Initialize weights\n",
    "Our first convolutional block uses a kernel width of `(1,3)`, a stride of `(1,1)` and padding both at the start and end of the token dimension so that we can slide across all entries. For this first convolution layer we'll go through step by step showing how the convolution is built.  \n",
    "\n",
    "To start, we will configure our weights to be based on the channel dimension, currently equal to our embedding, and our kernel. By matching the kernel we allow the layer to learn what parts of the kernel are more important for our final prediction. \n",
    "\n",
    "We'll also initialize our weights to be iterative so that we can see the impact clearly as they interact with our input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3871f2b-0d2d-492a-a920-03f43e71d618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv 1 kernel': (1, 3), 'conv 1 stride': (1, 1), 'conv 1 padding': (0, 1)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1_kernel_height = 1\n",
    "c1_kernel_width = 3\n",
    "c1_stride_height = 1\n",
    "c1_stride_width = 1\n",
    "c1_padding_height = 0\n",
    "c1_padding_width = 1\n",
    "{'conv 1 kernel': (c1_kernel_height, c1_kernel_width),\n",
    " 'conv 1 stride': (c1_stride_height, c1_stride_width),\n",
    " 'conv 1 padding': (c1_padding_height, c1_padding_width)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cdd7cda-440a-48fc-9a46-26d1b12ad441",
   "metadata": {},
   "outputs": [],
   "source": [
    "## weight layer for convolution (similar to linear, just more explicit)\n",
    "conv1 = nn.Parameter(\n",
    "    torch.empty(n_embd, n_embd, c1_kernel_height, c1_kernel_width), \n",
    "    requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ec5b799-3a2d-49bc-bb32-57d74b54c3c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 6, 1, 3]),\n",
       " Parameter containing:\n",
       " tensor([[[[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]]],\n",
       " \n",
       " \n",
       "         [[[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]]],\n",
       " \n",
       " \n",
       "         [[[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]]],\n",
       " \n",
       " \n",
       "         [[[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]]],\n",
       " \n",
       " \n",
       "         [[[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]]],\n",
       " \n",
       " \n",
       "         [[[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]]]], requires_grad=True))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iniate rows as 0.1, 0.2, and 0.3 for easier view of the weight impact\n",
    "with torch.no_grad():\n",
    "    c1_pattern = torch.tensor([0.001,0.002,0.001]).view(1,1,1,c1_kernel_width).expand(conv1.size()).clone()\n",
    "    conv1.copy_(c1_pattern)\n",
    "conv1.size(), conv1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449163be-7cf9-4313-878f-9af77d7e0838",
   "metadata": {},
   "source": [
    "##### 1x3 Conv - Run convolution\n",
    "\n",
    "Now we'll calculate the 2-D discrete cross-correlation for our weight and input `x`.  Since we know we have a residual connection we'll branch `x` and rejoin it after the convolutional block. For our convolutional layer, in our step by step guide we'll do the following: \n",
    "1. Since we have padding, pad our channel\n",
    "2. Flattens, or **unfolds** each sliding kernel_size-sized block within the spatial dimensions of input into a column (i.e., last dimension) of a 3-D output tensor of shape $(N,C*k_h*k_w,L)$\n",
    "3. Stack our weights so that it is reused across batches meaning our learning benefits from both. \n",
    "4. Take the dot product of the unstacked input and the stacked weights and reshape the result back to our batch and channels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e161d7-abb2-4116-9982-b05c66aeab44",
   "metadata": {},
   "source": [
    "**1x3 Conv - Step-by-step unfolding**\n",
    "\n",
    "In particular we'll focus on step #2, as this specifically creates a sliding view that extracts a kernel size view across our input. By converting them to columns, when we do $W_{flat} \\cdot X_{unfolded}$ the result is a sum of the row in the weight times what was previously a row in the input. Mentally, **unfold** linearizes all local receptive fields so you can do per-patch operations with a single batched matrix multiply. Convolution is exactly this with shared weights, hence the name.  After walking through step by step, we'll show you `F.unfold` a function that does the padding and unfolding for you and use it from there on out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a693e572-baa2-4306-9bac-0474bbed73b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 8]), 2, 6, 1, 8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = B_batch\n",
    "channel = n_embd\n",
    "height = 1\n",
    "width = T_context\n",
    "x.size(),batch, channel, height, width, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1602cf4e-82df-48da-a74a-6c60ef96583b",
   "metadata": {},
   "source": [
    "**Calculate expected unfolded dimensions**  \n",
    "\n",
    "To do our proper reshaping, we need to calculate the expected dimensions for our loop.  \n",
    "Recall that we expect to go from $(B,C,1,T)$ to $(B,C*k_h*k_w,L)$ where $L$ is a flattening or our output height and width as follows: \n",
    "$$\n",
    "\\begin{align}\n",
    "height_{out} &= (height + 2*pad_h - 1*(kernel_h-1) -1)\\ //\\ stride_{h}\\\\ \n",
    "width_{out} &= (width + 2*pad_w - 1*(kernel_w-1) -1)\\ //\\ stride_{w}\\\\\n",
    "L &= height_{out} * width_{out}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97f70c1c-6a3d-4277-80ec-e32d07442d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width out 8, height out 1, final dimension (2,18,8)\n"
     ]
    }
   ],
   "source": [
    "c1_khw = channel*c1_kernel_height*c1_kernel_width\n",
    "\n",
    "c1_height_out = (height + 2*c1_padding_height - 1*(c1_kernel_height-1) - 1)//c1_stride_height + 1   # = 1, \n",
    "c1_width_out = (width + 2*c1_padding_width - 1*(c1_kernel_width-1) - 1)//c1_stride_width + 1   # = 4\n",
    "c1_L = c1_height_out * c1_width_out\n",
    "\n",
    "print(f'width out {c1_width_out}, height out {c1_height_out}, final dimension ({batch},{c1_khw},{c1_L})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e830999-85d7-46b7-bddc-bcbc1c646f85",
   "metadata": {},
   "source": [
    "**Padding** \n",
    "\n",
    "We first start by padding.  Since we're using a stride of `(1,3)` we need to pad both the start and end of the tokens so that we can slide across it without losing an increment on the dimension. Padding simply adds `0` though we can add other values if we wanted.  When we pad on both sides we get output of `[2, 6, 1+0, 8+2]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "068c1f6a-6e9f-4ea6-b028-b1d028f59992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 10]),\n",
       " tensor([[[[0.0000, 0.1500, 0.1400, 0.1100, 0.0200, 0.1300, 0.0100, 0.0700,\n",
       "            0.1200, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.1600, 0.1500, 0.1200, 0.0300, 0.1400, 0.0200, 0.0800,\n",
       "            0.1300, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.1700, 0.1600, 0.1300, 0.0400, 0.1500, 0.0300, 0.0900,\n",
       "            0.1400, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.1800, 0.1700, 0.1400, 0.0500, 0.1600, 0.0400, 0.1000,\n",
       "            0.1500, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.1900, 0.1800, 0.1500, 0.0600, 0.1700, 0.0500, 0.1100,\n",
       "            0.1600, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.2000, 0.1900, 0.1600, 0.0700, 0.1800, 0.0600, 0.1200,\n",
       "            0.1700, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.0000, 0.0900, 0.0500, 0.1000, 0.1100, 0.0700, 0.1200, 0.0900,\n",
       "            0.0100, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.1000, 0.0600, 0.1100, 0.1200, 0.0800, 0.1300, 0.1000,\n",
       "            0.0200, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.1100, 0.0700, 0.1200, 0.1300, 0.0900, 0.1400, 0.1100,\n",
       "            0.0300, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.1200, 0.0800, 0.1300, 0.1400, 0.1000, 0.1500, 0.1200,\n",
       "            0.0400, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.1300, 0.0900, 0.1400, 0.1500, 0.1100, 0.1600, 0.1300,\n",
       "            0.0500, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.1400, 0.1000, 0.1500, 0.1600, 0.1200, 0.1700, 0.1400,\n",
       "            0.0600, 0.0000]]]], grad_fn=<ConstantPadNdBackward0>))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad last dim by (width, width) and 2nd to last by (height, height). width = 1, height = 0\n",
    "c1_x_pad = F.pad(x, pad=(c1_padding_width,c1_padding_width,c1_padding_height,c1_padding_height))\n",
    "\n",
    "c1_x_pad.size(), c1_x_pad #total size and show first example in batch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17938995-a7ba-4140-af02-d1698afbebd3",
   "metadata": {},
   "source": [
    "**Manual Unfolding - First Stride** \n",
    "\n",
    "Now we will manually unfold our padded input.  The process of unfolding flattens each sliding kernel-sized block within the spatial dimensions of input into a column (i.e., last dimension) of a 3-D output tensor of shape $(N,C*k_h*k_w,L)$ \n",
    "\n",
    "We'll first start by pulling out the first patch.  Since we have a kernel of `(1,3)` we pull out the first 3 tokens from the first spatial dimension for each channel in each batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f35fbee0-cb49-4eef-b000-bd6ae742024e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 3]),\n",
       " tensor([[[[0.0000, 0.1500, 0.1400]],\n",
       " \n",
       "          [[0.0000, 0.1600, 0.1500]],\n",
       " \n",
       "          [[0.0000, 0.1700, 0.1600]],\n",
       " \n",
       "          [[0.0000, 0.1800, 0.1700]],\n",
       " \n",
       "          [[0.0000, 0.1900, 0.1800]],\n",
       " \n",
       "          [[0.0000, 0.2000, 0.1900]]],\n",
       " \n",
       " \n",
       "         [[[0.0000, 0.0900, 0.0500]],\n",
       " \n",
       "          [[0.0000, 0.1000, 0.0600]],\n",
       " \n",
       "          [[0.0000, 0.1100, 0.0700]],\n",
       " \n",
       "          [[0.0000, 0.1200, 0.0800]],\n",
       " \n",
       "          [[0.0000, 0.1300, 0.0900]],\n",
       " \n",
       "          [[0.0000, 0.1400, 0.1000]]]], grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = 0\n",
    "patch = c1_x_pad[:, :, step:c1_kernel_height, step:step+c1_kernel_width]\n",
    "patch.size(), patch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2290686c-7693-4523-b5b7-188b8f294e6d",
   "metadata": {},
   "source": [
    "Now we need to stack our channels together. Since we want to make sure that eventually we can do a dot product of the weight and input where the weight column multiplies by the entry row, flattening our patches into a single entry gives us that.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb79101d-be10-4d00-856b-ffb14676ae40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 18]),\n",
       " tensor([[0.0000, 0.1500, 0.1400, 0.0000, 0.1600, 0.1500, 0.0000, 0.1700, 0.1600,\n",
       "          0.0000, 0.1800, 0.1700, 0.0000, 0.1900, 0.1800, 0.0000, 0.2000, 0.1900],\n",
       "         [0.0000, 0.0900, 0.0500, 0.0000, 0.1000, 0.0600, 0.0000, 0.1100, 0.0700,\n",
       "          0.0000, 0.1200, 0.0800, 0.0000, 0.1300, 0.0900, 0.0000, 0.1400, 0.1000]],\n",
       "        grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = patch.reshape(batch, c1_khw)\n",
    "col.size(), col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb8bb4e-5214-43c9-b44c-a554179264b4",
   "metadata": {},
   "source": [
    "Finally we want to make sure to save this since this is just the first pass of the patch. Let's create a list for now and store them. After we complete all the strides we can reshape our final output of the unfolded step to make each entry a column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5bf5e34-345b-4023-89d8-4441fb07137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_cols = []\n",
    "manual_cols.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfbff38-59a5-4ea3-8828-f58faaa972d3",
   "metadata": {},
   "source": [
    "**Manual Unfolding - Second Stride** \n",
    "\n",
    "We now need to move our patch by the stride amount, in this case `(1,1)`. Using a stride of 1 on both dimensions ensures that we continue covering every input token in the example. As you'll see in future convolutions, changing the stride can downsample an input.  Let's start by again extracting the patch. You'll see that we just shifted to the \"left\" by 1 and took the next 3 columns in our input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d8dc717-aee8-4625-b8ba-09f5eb108a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 3]),\n",
       " tensor([[[[0.1500, 0.1400, 0.1100]],\n",
       " \n",
       "          [[0.1600, 0.1500, 0.1200]],\n",
       " \n",
       "          [[0.1700, 0.1600, 0.1300]],\n",
       " \n",
       "          [[0.1800, 0.1700, 0.1400]],\n",
       " \n",
       "          [[0.1900, 0.1800, 0.1500]],\n",
       " \n",
       "          [[0.2000, 0.1900, 0.1600]]],\n",
       " \n",
       " \n",
       "         [[[0.0900, 0.0500, 0.1000]],\n",
       " \n",
       "          [[0.1000, 0.0600, 0.1100]],\n",
       " \n",
       "          [[0.1100, 0.0700, 0.1200]],\n",
       " \n",
       "          [[0.1200, 0.0800, 0.1300]],\n",
       " \n",
       "          [[0.1300, 0.0900, 0.1400]],\n",
       " \n",
       "          [[0.1400, 0.1000, 0.1500]]]], grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = 1\n",
    "patch = c1_x_pad[:, :, 0:c1_kernel_height, step:step+c1_kernel_width]\n",
    "patch.size(), patch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531b8fcf-d7f5-451f-b6cb-b2cfb26ea86e",
   "metadata": {},
   "source": [
    "we'll again flatten this the same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60c4f1ef-7b65-4b60-aade-9d96dd8cb15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 18]),\n",
       " tensor([[0.1500, 0.1400, 0.1100, 0.1600, 0.1500, 0.1200, 0.1700, 0.1600, 0.1300,\n",
       "          0.1800, 0.1700, 0.1400, 0.1900, 0.1800, 0.1500, 0.2000, 0.1900, 0.1600],\n",
       "         [0.0900, 0.0500, 0.1000, 0.1000, 0.0600, 0.1100, 0.1100, 0.0700, 0.1200,\n",
       "          0.1200, 0.0800, 0.1300, 0.1300, 0.0900, 0.1400, 0.1400, 0.1000, 0.1500]],\n",
       "        grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = patch.reshape(batch, c1_khw)\n",
    "col.size(), col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c236208-2c2f-4c14-a70b-71d72678acdc",
   "metadata": {},
   "source": [
    "and now add it to our list.  We can now see that we have entries for our first 2 steps already in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "609ab8e0-dab8-4ca5-a3cc-aacbb5962331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.0000, 0.1500, 0.1400, 0.0000, 0.1600, 0.1500, 0.0000, 0.1700, 0.1600,\n",
       "          0.0000, 0.1800, 0.1700, 0.0000, 0.1900, 0.1800, 0.0000, 0.2000, 0.1900],\n",
       "         [0.0000, 0.0900, 0.0500, 0.0000, 0.1000, 0.0600, 0.0000, 0.1100, 0.0700,\n",
       "          0.0000, 0.1200, 0.0800, 0.0000, 0.1300, 0.0900, 0.0000, 0.1400, 0.1000]],\n",
       "        grad_fn=<UnsafeViewBackward0>),\n",
       " tensor([[0.1500, 0.1400, 0.1100, 0.1600, 0.1500, 0.1200, 0.1700, 0.1600, 0.1300,\n",
       "          0.1800, 0.1700, 0.1400, 0.1900, 0.1800, 0.1500, 0.2000, 0.1900, 0.1600],\n",
       "         [0.0900, 0.0500, 0.1000, 0.1000, 0.0600, 0.1100, 0.1100, 0.0700, 0.1200,\n",
       "          0.1200, 0.0800, 0.1300, 0.1300, 0.0900, 0.1400, 0.1400, 0.1000, 0.1500]],\n",
       "        grad_fn=<UnsafeViewBackward0>)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_cols.append(col)\n",
    "manual_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55718f56-dc69-4430-9782-b0e542e4e778",
   "metadata": {},
   "source": [
    "**Manual Unfolding - Remaining Strides** \n",
    "\n",
    "We'll now loop through the remaining steps for the manual unfolding to fill in the rest of the list.  This is the same set of steps done before, just in a loop but appending to the same list.  We'll start from 2 onward since we already did steps 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4172ea99-b2de-460f-8d8d-c91854d95746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execting stride 2\n",
      "execting stride 3\n",
      "execting stride 4\n",
      "execting stride 5\n",
      "execting stride 6\n",
      "execting stride 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[0.0000, 0.1500, 0.1400, 0.0000, 0.1600, 0.1500, 0.0000, 0.1700, 0.1600,\n",
       "          0.0000, 0.1800, 0.1700, 0.0000, 0.1900, 0.1800, 0.0000, 0.2000, 0.1900],\n",
       "         [0.0000, 0.0900, 0.0500, 0.0000, 0.1000, 0.0600, 0.0000, 0.1100, 0.0700,\n",
       "          0.0000, 0.1200, 0.0800, 0.0000, 0.1300, 0.0900, 0.0000, 0.1400, 0.1000]],\n",
       "        grad_fn=<UnsafeViewBackward0>),\n",
       " tensor([[0.1500, 0.1400, 0.1100, 0.1600, 0.1500, 0.1200, 0.1700, 0.1600, 0.1300,\n",
       "          0.1800, 0.1700, 0.1400, 0.1900, 0.1800, 0.1500, 0.2000, 0.1900, 0.1600],\n",
       "         [0.0900, 0.0500, 0.1000, 0.1000, 0.0600, 0.1100, 0.1100, 0.0700, 0.1200,\n",
       "          0.1200, 0.0800, 0.1300, 0.1300, 0.0900, 0.1400, 0.1400, 0.1000, 0.1500]],\n",
       "        grad_fn=<UnsafeViewBackward0>),\n",
       " tensor([[0.1400, 0.1100, 0.0200, 0.1500, 0.1200, 0.0300, 0.1600, 0.1300, 0.0400,\n",
       "          0.1700, 0.1400, 0.0500, 0.1800, 0.1500, 0.0600, 0.1900, 0.1600, 0.0700],\n",
       "         [0.0500, 0.1000, 0.1100, 0.0600, 0.1100, 0.1200, 0.0700, 0.1200, 0.1300,\n",
       "          0.0800, 0.1300, 0.1400, 0.0900, 0.1400, 0.1500, 0.1000, 0.1500, 0.1600]],\n",
       "        grad_fn=<UnsafeViewBackward0>),\n",
       " tensor([[0.1100, 0.0200, 0.1300, 0.1200, 0.0300, 0.1400, 0.1300, 0.0400, 0.1500,\n",
       "          0.1400, 0.0500, 0.1600, 0.1500, 0.0600, 0.1700, 0.1600, 0.0700, 0.1800],\n",
       "         [0.1000, 0.1100, 0.0700, 0.1100, 0.1200, 0.0800, 0.1200, 0.1300, 0.0900,\n",
       "          0.1300, 0.1400, 0.1000, 0.1400, 0.1500, 0.1100, 0.1500, 0.1600, 0.1200]],\n",
       "        grad_fn=<UnsafeViewBackward0>),\n",
       " tensor([[0.0200, 0.1300, 0.0100, 0.0300, 0.1400, 0.0200, 0.0400, 0.1500, 0.0300,\n",
       "          0.0500, 0.1600, 0.0400, 0.0600, 0.1700, 0.0500, 0.0700, 0.1800, 0.0600],\n",
       "         [0.1100, 0.0700, 0.1200, 0.1200, 0.0800, 0.1300, 0.1300, 0.0900, 0.1400,\n",
       "          0.1400, 0.1000, 0.1500, 0.1500, 0.1100, 0.1600, 0.1600, 0.1200, 0.1700]],\n",
       "        grad_fn=<UnsafeViewBackward0>),\n",
       " tensor([[0.1300, 0.0100, 0.0700, 0.1400, 0.0200, 0.0800, 0.1500, 0.0300, 0.0900,\n",
       "          0.1600, 0.0400, 0.1000, 0.1700, 0.0500, 0.1100, 0.1800, 0.0600, 0.1200],\n",
       "         [0.0700, 0.1200, 0.0900, 0.0800, 0.1300, 0.1000, 0.0900, 0.1400, 0.1100,\n",
       "          0.1000, 0.1500, 0.1200, 0.1100, 0.1600, 0.1300, 0.1200, 0.1700, 0.1400]],\n",
       "        grad_fn=<UnsafeViewBackward0>),\n",
       " tensor([[0.0100, 0.0700, 0.1200, 0.0200, 0.0800, 0.1300, 0.0300, 0.0900, 0.1400,\n",
       "          0.0400, 0.1000, 0.1500, 0.0500, 0.1100, 0.1600, 0.0600, 0.1200, 0.1700],\n",
       "         [0.1200, 0.0900, 0.0100, 0.1300, 0.1000, 0.0200, 0.1400, 0.1100, 0.0300,\n",
       "          0.1500, 0.1200, 0.0400, 0.1600, 0.1300, 0.0500, 0.1700, 0.1400, 0.0600]],\n",
       "        grad_fn=<UnsafeViewBackward0>),\n",
       " tensor([[0.0700, 0.1200, 0.0000, 0.0800, 0.1300, 0.0000, 0.0900, 0.1400, 0.0000,\n",
       "          0.1000, 0.1500, 0.0000, 0.1100, 0.1600, 0.0000, 0.1200, 0.1700, 0.0000],\n",
       "         [0.0900, 0.0100, 0.0000, 0.1000, 0.0200, 0.0000, 0.1100, 0.0300, 0.0000,\n",
       "          0.1200, 0.0400, 0.0000, 0.1300, 0.0500, 0.0000, 0.1400, 0.0600, 0.0000]],\n",
       "        grad_fn=<UnsafeViewBackward0>)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for step in range(2,c1_width_out): \n",
    "    print(f'execting stride {step}')\n",
    "    # extract step\n",
    "    patch = c1_x_pad[:, :, 0:c1_kernel_height, step:step+c1_kernel_width]        # (2,6,1,3)\n",
    "    \n",
    "    # stack the entries in each batch together into a row\n",
    "    col = patch.reshape(batch, c1_khw) # shape to [2,18]\n",
    "\n",
    "    manual_cols.append(col)\n",
    "\n",
    "manual_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b165195b-0714-4208-8e41-9cb80b475e3b",
   "metadata": {},
   "source": [
    "**Manual Unfolding - Flatten List** \n",
    "\n",
    "Now that we've completed the patch extraction we have a list of tensors. We want to create a new tensor where we maintain the batch of 2 but convert our row length of 18 into the column dimension. We'll use stack to complete this and result in a `(2,18,8)` tensor, just like we calculated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92f1c1b0-b56d-4745-b0b4-a0df400d35a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 18, 8]),\n",
       " tensor([[[0.0000, 0.1500, 0.1400, 0.1100, 0.0200, 0.1300, 0.0100, 0.0700],\n",
       "          [0.1500, 0.1400, 0.1100, 0.0200, 0.1300, 0.0100, 0.0700, 0.1200],\n",
       "          [0.1400, 0.1100, 0.0200, 0.1300, 0.0100, 0.0700, 0.1200, 0.0000],\n",
       "          [0.0000, 0.1600, 0.1500, 0.1200, 0.0300, 0.1400, 0.0200, 0.0800],\n",
       "          [0.1600, 0.1500, 0.1200, 0.0300, 0.1400, 0.0200, 0.0800, 0.1300],\n",
       "          [0.1500, 0.1200, 0.0300, 0.1400, 0.0200, 0.0800, 0.1300, 0.0000],\n",
       "          [0.0000, 0.1700, 0.1600, 0.1300, 0.0400, 0.1500, 0.0300, 0.0900],\n",
       "          [0.1700, 0.1600, 0.1300, 0.0400, 0.1500, 0.0300, 0.0900, 0.1400],\n",
       "          [0.1600, 0.1300, 0.0400, 0.1500, 0.0300, 0.0900, 0.1400, 0.0000],\n",
       "          [0.0000, 0.1800, 0.1700, 0.1400, 0.0500, 0.1600, 0.0400, 0.1000],\n",
       "          [0.1800, 0.1700, 0.1400, 0.0500, 0.1600, 0.0400, 0.1000, 0.1500],\n",
       "          [0.1700, 0.1400, 0.0500, 0.1600, 0.0400, 0.1000, 0.1500, 0.0000],\n",
       "          [0.0000, 0.1900, 0.1800, 0.1500, 0.0600, 0.1700, 0.0500, 0.1100],\n",
       "          [0.1900, 0.1800, 0.1500, 0.0600, 0.1700, 0.0500, 0.1100, 0.1600],\n",
       "          [0.1800, 0.1500, 0.0600, 0.1700, 0.0500, 0.1100, 0.1600, 0.0000],\n",
       "          [0.0000, 0.2000, 0.1900, 0.1600, 0.0700, 0.1800, 0.0600, 0.1200],\n",
       "          [0.2000, 0.1900, 0.1600, 0.0700, 0.1800, 0.0600, 0.1200, 0.1700],\n",
       "          [0.1900, 0.1600, 0.0700, 0.1800, 0.0600, 0.1200, 0.1700, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0900, 0.0500, 0.1000, 0.1100, 0.0700, 0.1200, 0.0900],\n",
       "          [0.0900, 0.0500, 0.1000, 0.1100, 0.0700, 0.1200, 0.0900, 0.0100],\n",
       "          [0.0500, 0.1000, 0.1100, 0.0700, 0.1200, 0.0900, 0.0100, 0.0000],\n",
       "          [0.0000, 0.1000, 0.0600, 0.1100, 0.1200, 0.0800, 0.1300, 0.1000],\n",
       "          [0.1000, 0.0600, 0.1100, 0.1200, 0.0800, 0.1300, 0.1000, 0.0200],\n",
       "          [0.0600, 0.1100, 0.1200, 0.0800, 0.1300, 0.1000, 0.0200, 0.0000],\n",
       "          [0.0000, 0.1100, 0.0700, 0.1200, 0.1300, 0.0900, 0.1400, 0.1100],\n",
       "          [0.1100, 0.0700, 0.1200, 0.1300, 0.0900, 0.1400, 0.1100, 0.0300],\n",
       "          [0.0700, 0.1200, 0.1300, 0.0900, 0.1400, 0.1100, 0.0300, 0.0000],\n",
       "          [0.0000, 0.1200, 0.0800, 0.1300, 0.1400, 0.1000, 0.1500, 0.1200],\n",
       "          [0.1200, 0.0800, 0.1300, 0.1400, 0.1000, 0.1500, 0.1200, 0.0400],\n",
       "          [0.0800, 0.1300, 0.1400, 0.1000, 0.1500, 0.1200, 0.0400, 0.0000],\n",
       "          [0.0000, 0.1300, 0.0900, 0.1400, 0.1500, 0.1100, 0.1600, 0.1300],\n",
       "          [0.1300, 0.0900, 0.1400, 0.1500, 0.1100, 0.1600, 0.1300, 0.0500],\n",
       "          [0.0900, 0.1400, 0.1500, 0.1100, 0.1600, 0.1300, 0.0500, 0.0000],\n",
       "          [0.0000, 0.1400, 0.1000, 0.1500, 0.1600, 0.1200, 0.1700, 0.1400],\n",
       "          [0.1400, 0.1000, 0.1500, 0.1600, 0.1200, 0.1700, 0.1400, 0.0600],\n",
       "          [0.1000, 0.1500, 0.1600, 0.1200, 0.1700, 0.1400, 0.0600, 0.0000]]],\n",
       "        grad_fn=<StackBackward0>))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn all the rows in the list into columns while maintaining the batch\n",
    "manual_unfold = torch.stack(manual_cols, dim=2)  # (N, 18, 8)\n",
    "manual_unfold.size(), manual_unfold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade75289-413e-4597-ab98-fa3c91d97333",
   "metadata": {},
   "source": [
    "**Unfolding - Efficiently**\n",
    "\n",
    "While the above is great for demonstration purposes, it eats up a lot of time and code space.  Let's switch to the help of a pytorch function `F.unfold`.  This unfold function does the same steps as above: padding, patch extraction, reshaping, stacking. \n",
    "\n",
    "Let's setup our unfold of the original input `x`.  We'll also do a comparison of the previous output `manual_unfold` with this functions output to demonstrate that it is in fact equal and we can use it going forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5fc5670d-b3ad-4f9f-ab1d-6d1609733662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manual equals unfold: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 18, 8]),\n",
       " tensor([[[0.0000, 0.1500, 0.1400, 0.1100, 0.0200, 0.1300, 0.0100, 0.0700],\n",
       "          [0.1500, 0.1400, 0.1100, 0.0200, 0.1300, 0.0100, 0.0700, 0.1200],\n",
       "          [0.1400, 0.1100, 0.0200, 0.1300, 0.0100, 0.0700, 0.1200, 0.0000],\n",
       "          [0.0000, 0.1600, 0.1500, 0.1200, 0.0300, 0.1400, 0.0200, 0.0800],\n",
       "          [0.1600, 0.1500, 0.1200, 0.0300, 0.1400, 0.0200, 0.0800, 0.1300],\n",
       "          [0.1500, 0.1200, 0.0300, 0.1400, 0.0200, 0.0800, 0.1300, 0.0000],\n",
       "          [0.0000, 0.1700, 0.1600, 0.1300, 0.0400, 0.1500, 0.0300, 0.0900],\n",
       "          [0.1700, 0.1600, 0.1300, 0.0400, 0.1500, 0.0300, 0.0900, 0.1400],\n",
       "          [0.1600, 0.1300, 0.0400, 0.1500, 0.0300, 0.0900, 0.1400, 0.0000],\n",
       "          [0.0000, 0.1800, 0.1700, 0.1400, 0.0500, 0.1600, 0.0400, 0.1000],\n",
       "          [0.1800, 0.1700, 0.1400, 0.0500, 0.1600, 0.0400, 0.1000, 0.1500],\n",
       "          [0.1700, 0.1400, 0.0500, 0.1600, 0.0400, 0.1000, 0.1500, 0.0000],\n",
       "          [0.0000, 0.1900, 0.1800, 0.1500, 0.0600, 0.1700, 0.0500, 0.1100],\n",
       "          [0.1900, 0.1800, 0.1500, 0.0600, 0.1700, 0.0500, 0.1100, 0.1600],\n",
       "          [0.1800, 0.1500, 0.0600, 0.1700, 0.0500, 0.1100, 0.1600, 0.0000],\n",
       "          [0.0000, 0.2000, 0.1900, 0.1600, 0.0700, 0.1800, 0.0600, 0.1200],\n",
       "          [0.2000, 0.1900, 0.1600, 0.0700, 0.1800, 0.0600, 0.1200, 0.1700],\n",
       "          [0.1900, 0.1600, 0.0700, 0.1800, 0.0600, 0.1200, 0.1700, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0900, 0.0500, 0.1000, 0.1100, 0.0700, 0.1200, 0.0900],\n",
       "          [0.0900, 0.0500, 0.1000, 0.1100, 0.0700, 0.1200, 0.0900, 0.0100],\n",
       "          [0.0500, 0.1000, 0.1100, 0.0700, 0.1200, 0.0900, 0.0100, 0.0000],\n",
       "          [0.0000, 0.1000, 0.0600, 0.1100, 0.1200, 0.0800, 0.1300, 0.1000],\n",
       "          [0.1000, 0.0600, 0.1100, 0.1200, 0.0800, 0.1300, 0.1000, 0.0200],\n",
       "          [0.0600, 0.1100, 0.1200, 0.0800, 0.1300, 0.1000, 0.0200, 0.0000],\n",
       "          [0.0000, 0.1100, 0.0700, 0.1200, 0.1300, 0.0900, 0.1400, 0.1100],\n",
       "          [0.1100, 0.0700, 0.1200, 0.1300, 0.0900, 0.1400, 0.1100, 0.0300],\n",
       "          [0.0700, 0.1200, 0.1300, 0.0900, 0.1400, 0.1100, 0.0300, 0.0000],\n",
       "          [0.0000, 0.1200, 0.0800, 0.1300, 0.1400, 0.1000, 0.1500, 0.1200],\n",
       "          [0.1200, 0.0800, 0.1300, 0.1400, 0.1000, 0.1500, 0.1200, 0.0400],\n",
       "          [0.0800, 0.1300, 0.1400, 0.1000, 0.1500, 0.1200, 0.0400, 0.0000],\n",
       "          [0.0000, 0.1300, 0.0900, 0.1400, 0.1500, 0.1100, 0.1600, 0.1300],\n",
       "          [0.1300, 0.0900, 0.1400, 0.1500, 0.1100, 0.1600, 0.1300, 0.0500],\n",
       "          [0.0900, 0.1400, 0.1500, 0.1100, 0.1600, 0.1300, 0.0500, 0.0000],\n",
       "          [0.0000, 0.1400, 0.1000, 0.1500, 0.1600, 0.1200, 0.1700, 0.1400],\n",
       "          [0.1400, 0.1000, 0.1500, 0.1600, 0.1200, 0.1700, 0.1400, 0.0600],\n",
       "          [0.1000, 0.1500, 0.1600, 0.1200, 0.1700, 0.1400, 0.0600, 0.0000]]],\n",
       "        grad_fn=<Im2ColBackward0>))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1_unfolded = F.unfold(x, \n",
    "\t\tkernel_size=(c1_kernel_height, c1_kernel_width),  # (1,3)\n",
    "\t\tpadding=(c1_padding_height, c1_padding_width), #(0,1)\n",
    "\t\tstride=(c1_stride_height, c1_stride_width))#(1,1)\n",
    "\n",
    "print(\"manual equals unfold:\", torch.allclose(c1_unfolded, manual_unfold))\n",
    "c1_unfolded.size() , c1_unfolded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec055fe-103a-4ea4-ad03-5095ab0e4a0a",
   "metadata": {},
   "source": [
    "##### 1x3 Conv - $W\\cdot X_{unfolded}$\n",
    "\n",
    "Now that we have our unstacked patches, we can then let our network decide how much of the patch, and which part of the patch, influences our output.  To do this we take the dot product of the weight with the unfolded input.  We do have an issue though since our weight is `[6,6,1,3]` but our input is `[2x18x8]`. We will solve this simply by squeezing the last two dimensions of our Weights together to result in a `[6,18]` tensor that we can multiply.  \n",
    "\n",
    "You might be now asking \"what about the batch dimensions of 2\".  We do want to make sure the 2 different batches actually share the same weight so we don't actually want to increase our weight dimension.  Instead we rely on the pytorch which broadcasts the same matrix to each of the batches in the input automatically.  This allows the two batches to share the weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "41e453ff-f282-44ed-a85b-b4990499fcfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 18]),\n",
       " tensor([[0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010,\n",
       "          0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010],\n",
       "         [0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010,\n",
       "          0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010],\n",
       "         [0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010,\n",
       "          0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010],\n",
       "         [0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010,\n",
       "          0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010],\n",
       "         [0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010,\n",
       "          0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010],\n",
       "         [0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010,\n",
       "          0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010]],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1_weigth = conv1.view(n_embd, -1) # [6,6,1,3] > [6,18]\n",
    "conv1_weigth.size(), conv1_weigth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258056eb-8544-4bd0-bbd6-6a0a0a7ba561",
   "metadata": {},
   "source": [
    "Now that we have the weights in the dimension we want them we're ready to multiply them with the unfolded input.  Because in our weight matrix each \"row\" is the same, each of our column entries in the result will be equal.  We'll also get a final output of `[2,6,8]` compressing the 18 down. Also note that the batch dimension is maintained as the weight is broadcast across the batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5328b95a-6d03-4afb-b060-145c3a77c43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 8]),\n",
       " tensor([[[0.0031, 0.0038, 0.0029, 0.0023, 0.0023, 0.0019, 0.0022, 0.0023],\n",
       "          [0.0031, 0.0038, 0.0029, 0.0023, 0.0023, 0.0019, 0.0022, 0.0023],\n",
       "          [0.0031, 0.0038, 0.0029, 0.0023, 0.0023, 0.0019, 0.0022, 0.0023],\n",
       "          [0.0031, 0.0038, 0.0029, 0.0023, 0.0023, 0.0019, 0.0022, 0.0023],\n",
       "          [0.0031, 0.0038, 0.0029, 0.0023, 0.0023, 0.0019, 0.0022, 0.0023],\n",
       "          [0.0031, 0.0038, 0.0029, 0.0023, 0.0023, 0.0019, 0.0022, 0.0023]],\n",
       " \n",
       "         [[0.0018, 0.0023, 0.0028, 0.0029, 0.0028, 0.0030, 0.0025, 0.0011],\n",
       "          [0.0018, 0.0023, 0.0028, 0.0029, 0.0028, 0.0030, 0.0025, 0.0011],\n",
       "          [0.0018, 0.0023, 0.0028, 0.0029, 0.0028, 0.0030, 0.0025, 0.0011],\n",
       "          [0.0018, 0.0023, 0.0028, 0.0029, 0.0028, 0.0030, 0.0025, 0.0011],\n",
       "          [0.0018, 0.0023, 0.0028, 0.0029, 0.0028, 0.0030, 0.0025, 0.0011],\n",
       "          [0.0018, 0.0023, 0.0028, 0.0029, 0.0028, 0.0030, 0.0025, 0.0011]]],\n",
       "        grad_fn=<CloneBackward0>))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = conv1_weigth @ c1_unfolded\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e458f84d-f6a3-47c8-b005-d4c92911a317",
   "metadata": {},
   "source": [
    "Finally we need to resize our last dimension back to our target channel height and width.  Since our height is 1 it will just insert in another dimension of 1 without looking significantly different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d782195-9352-4fdf-b26d-cdd71cdb145f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 8]),\n",
       " tensor([[[[0.0031, 0.0038, 0.0029, 0.0023, 0.0023, 0.0019, 0.0022, 0.0023]],\n",
       " \n",
       "          [[0.0031, 0.0038, 0.0029, 0.0023, 0.0023, 0.0019, 0.0022, 0.0023]],\n",
       " \n",
       "          [[0.0031, 0.0038, 0.0029, 0.0023, 0.0023, 0.0019, 0.0022, 0.0023]],\n",
       " \n",
       "          [[0.0031, 0.0038, 0.0029, 0.0023, 0.0023, 0.0019, 0.0022, 0.0023]],\n",
       " \n",
       "          [[0.0031, 0.0038, 0.0029, 0.0023, 0.0023, 0.0019, 0.0022, 0.0023]],\n",
       " \n",
       "          [[0.0031, 0.0038, 0.0029, 0.0023, 0.0023, 0.0019, 0.0022, 0.0023]]],\n",
       " \n",
       " \n",
       "         [[[0.0018, 0.0023, 0.0028, 0.0029, 0.0028, 0.0030, 0.0025, 0.0011]],\n",
       " \n",
       "          [[0.0018, 0.0023, 0.0028, 0.0029, 0.0028, 0.0030, 0.0025, 0.0011]],\n",
       " \n",
       "          [[0.0018, 0.0023, 0.0028, 0.0029, 0.0028, 0.0030, 0.0025, 0.0011]],\n",
       " \n",
       "          [[0.0018, 0.0023, 0.0028, 0.0029, 0.0028, 0.0030, 0.0025, 0.0011]],\n",
       " \n",
       "          [[0.0018, 0.0023, 0.0028, 0.0029, 0.0028, 0.0030, 0.0025, 0.0011]],\n",
       " \n",
       "          [[0.0018, 0.0023, 0.0028, 0.0029, 0.0028, 0.0030, 0.0025, 0.0011]]]],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out.view(batch,n_embd, c1_height_out, c1_width_out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1926f12-bb37-426e-985f-f3f0c502152e",
   "metadata": {},
   "source": [
    "#### Convolution Block - First Batch Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c32835-5f45-4f42-9b88-3716083d75c1",
   "metadata": {},
   "source": [
    "With batch normalization (2D), we normalize each channel using statistics computed over the current mini-batch and spatial positions. Concretely, for inputs of shape $[N,C,H,W]$, batch normalization computes a per-channel mean and variance across ${B,C,1}$ and standardizes that channel. This helps to stabilize activations, act as a mild regularizer, and speed up training. If an entire channel is scaled uniformly across the batch and spatial locations, that scale is largely removed by the normalization. This means an array of `[1,2,3,4]` and `[2,4,6,8]` will have the same normalized entries after batch normalization.\n",
    "\n",
    "Batch normalization applies the following:\n",
    "$$\n",
    "y_{b,c,h,t}=\\gamma_c \\frac{x_{b,c,h,t}-\\mu_c}{\\sqrt{\\sigma_c^2+\\epsilon}}+\\beta_c,\n",
    "\\quad\n",
    "\\mu_c=\\mathbb{E}_{b,h,t}[x_{b,c,h,t}],\\quad\n",
    "\\sigma_c^2=\\operatorname{Var}_{b,h,t}[x_{b,c,h,t}].\n",
    "$$\n",
    "\n",
    "Batch normalization is applied per channel on the feature map. Initially, batch normalization creates affine parameters with $\\gamma_c=1$ and $\\beta_c=0$ so that all channels are scaled and shifted equally. We’ll keep this initialization unchanged. With training though, they adjust based on the gradient. Since initiation is still 1, we'll see that the values are repeated across the columns but the values themselves now span both positive and negative given they represent scaled distance from the mean of the batch. \n",
    "\n",
    "*Note that even though we may apply batch normalization in multiple places, we keep it as a separate layer so its effect can be tuned independently of other normalization layers.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20809e41-d391-4b13-abc4-f7b4bdf7b258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0.], requires_grad=True))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_a = nn.BatchNorm2d(n_embd)\n",
    "bn_a.weight, bn_a.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a29c64c5-7447-4c12-8594-a96be21188b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 8]),\n",
       " tensor([[[[ 0.1805,  0.4135,  0.1153, -0.0711, -0.0524, -0.1829, -0.0897,\n",
       "            -0.0617]],\n",
       " \n",
       "          [[ 0.1805,  0.4135,  0.1153, -0.0711, -0.0524, -0.1829, -0.0897,\n",
       "            -0.0617]],\n",
       " \n",
       "          [[ 0.1805,  0.4135,  0.1153, -0.0711, -0.0524, -0.1829, -0.0897,\n",
       "            -0.0617]],\n",
       " \n",
       "          [[ 0.1805,  0.4135,  0.1153, -0.0711, -0.0524, -0.1829, -0.0897,\n",
       "            -0.0617]],\n",
       " \n",
       "          [[ 0.1805,  0.4135,  0.1153, -0.0711, -0.0524, -0.1829, -0.0897,\n",
       "            -0.0617]],\n",
       " \n",
       "          [[ 0.1805,  0.4135,  0.1153, -0.0711, -0.0524, -0.1829, -0.0897,\n",
       "            -0.0617]]],\n",
       " \n",
       " \n",
       "         [[[-0.2108, -0.0524,  0.0780,  0.1340,  0.0967,  0.1526, -0.0151,\n",
       "            -0.4345]],\n",
       " \n",
       "          [[-0.2108, -0.0524,  0.0780,  0.1340,  0.0967,  0.1526, -0.0151,\n",
       "            -0.4345]],\n",
       " \n",
       "          [[-0.2108, -0.0524,  0.0780,  0.1340,  0.0967,  0.1526, -0.0151,\n",
       "            -0.4345]],\n",
       " \n",
       "          [[-0.2108, -0.0524,  0.0780,  0.1340,  0.0967,  0.1526, -0.0151,\n",
       "            -0.4345]],\n",
       " \n",
       "          [[-0.2108, -0.0524,  0.0780,  0.1340,  0.0967,  0.1526, -0.0151,\n",
       "            -0.4345]],\n",
       " \n",
       "          [[-0.2108, -0.0524,  0.0780,  0.1340,  0.0967,  0.1526, -0.0151,\n",
       "            -0.4345]]]], grad_fn=<NativeBatchNormBackward0>))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = bn_a(out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8d4c38-4d8b-40e7-9acc-30a49b4a5d93",
   "metadata": {},
   "source": [
    "#### Convolution Block - ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef500f06-1662-4b00-a50f-e2af0677c808",
   "metadata": {},
   "source": [
    "Next we apply, rectified linear units (**ReLU**), an element-wise nonlinearity that keeps only positive activations and zeroes out negatives. This operates on any input shape (e.g., $[B,C,H,T]$ and is commonly used after convolution and normalization to introduce sparsity and enable the network to model nonlinear relationships while maintaining strong gradient flow compared to saturating activations. Activation sparsity zeroes out uninformative responses (weight < 0), which acts as an implicit regularizer reducing co-adaptation and overfitting while making features more selective and improving generalization. It can also lower the effective compute/memory footprint since there are fewer nonzeros to propagate and accumulate. ReLU does not have any learnable parameters so you won't see us create a layer for it.\n",
    "\n",
    "The formula applied is:\n",
    "$$\n",
    "y = \\max(0, x),\n",
    "\\quad\n",
    "\\frac{\\partial y}{\\partial x}=\n",
    "\\begin{cases}\n",
    "1,& x>0\\\\\n",
    "0,& x\\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In CNN blocks, ReLU is typically placed after BatchNorm (e.g., Conv $\\rightarrow$ BN $\\rightarrow$ ReLU). In pre-activation designs, you may see BN $\\rightarrow$ ReLU $\\rightarrow$ Conv. A practical caveat is the “dying ReLU” phenomenon (units stuck at zero); if this becomes an issue, consider alternatives like LeakyReLU, ELU, or GELU.\n",
    "\n",
    "*We keep ReLU as a separate layer so it can be swapped or configured independently (e.g., in-place vs. out-of-place) without changing other components.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3477c9ed-0cfa-42de-9903-47b602578df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 8]),\n",
       " tensor([[[[0.1805, 0.4135, 0.1153, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.1805, 0.4135, 0.1153, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.1805, 0.4135, 0.1153, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.1805, 0.4135, 0.1153, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.1805, 0.4135, 0.1153, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.1805, 0.4135, 0.1153, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.0000, 0.0000, 0.0780, 0.1340, 0.0967, 0.1526, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0780, 0.1340, 0.0967, 0.1526, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0780, 0.1340, 0.0967, 0.1526, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0780, 0.1340, 0.0967, 0.1526, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0780, 0.1340, 0.0967, 0.1526, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0780, 0.1340, 0.0967, 0.1526, 0.0000, 0.0000]]]],\n",
       "        grad_fn=<ReluBackward0>))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = F.relu(out) \n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2170f11-7281-4422-9398-7734d9602d46",
   "metadata": {},
   "source": [
    "#### Convolution Block - 1x3 Conv 1x2 Stride downsample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24598a33-71d7-4a60-af33-8e50694e6f04",
   "metadata": {},
   "source": [
    "##### 1x3 Conv 2 Stride - Initialize weights\n",
    "Our second convolutional block uses a kernel width of `(1,3)`, a stride of `(1,2)` and padding both at the start and end of the token dimension so that we can slide across all entries. The `(1,2)` stride will downsample our channel from 8 to 4 reducing the size of our output.  While our stride is `(1,2)` the kernel width still ensures that all input cells are seen during the downsampling and the learned weights will determine which portion of the patch should impact the layer output the most. \n",
    "\n",
    "To start, we will configure our weights to be based on the channel dimension, currently equal to our embedding, and our kernel. By matching the kernel we allow the layer to learn what parts of the kernel are more important for our final prediction. \n",
    "\n",
    "We'll also initialize our weights to be iterative so that we can see the impact clearly as they interact with our input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "203d10a8-2a14-4105-8115-0c9af2f8643b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernel': (1, 3), 'stride': (1, 2), 'padding': (0, 1)}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2_kernel_height = 1\n",
    "c2_kernel_width = 3\n",
    "c2_stride_height = 1\n",
    "c2_stride_width = 2\n",
    "c2_padding_height = 0\n",
    "c2_padding_width = 1\n",
    "{'kernel': (c2_kernel_height, c2_kernel_width),\n",
    " 'stride': (c2_stride_height, c2_stride_width),\n",
    " 'padding': (c2_padding_height, c2_padding_width)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9fef676c-3cbd-4c86-a53c-1dca20938567",
   "metadata": {},
   "outputs": [],
   "source": [
    "## weight layer for convolution (similar to linear, just more explicit)\n",
    "conv2 = nn.Parameter(\n",
    "    torch.empty(n_embd, n_embd, c2_kernel_height, c2_kernel_width), \n",
    "    requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e2e8415-6f2a-444b-9ead-d332a7309590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 6, 1, 3]),\n",
       " Parameter containing:\n",
       " tensor([[[[0.0020, 0.0010, 0.0010]],\n",
       " \n",
       "          [[0.0020, 0.0010, 0.0010]],\n",
       " \n",
       "          [[0.0020, 0.0010, 0.0010]],\n",
       " \n",
       "          [[0.0020, 0.0010, 0.0010]],\n",
       " \n",
       "          [[0.0020, 0.0010, 0.0010]],\n",
       " \n",
       "          [[0.0020, 0.0010, 0.0010]]],\n",
       " \n",
       " \n",
       "         [[[0.0020, 0.0010, 0.0010]],\n",
       " \n",
       "          [[0.0020, 0.0010, 0.0010]],\n",
       " \n",
       "          [[0.0020, 0.0010, 0.0010]],\n",
       " \n",
       "          [[0.0020, 0.0010, 0.0010]],\n",
       " \n",
       "          [[0.0020, 0.0010, 0.0010]],\n",
       " \n",
       "          [[0.0020, 0.0010, 0.0010]]],\n",
       " \n",
       " \n",
       "         [[[0.0020, 0.0010, 0.0010]],\n",
       " \n",
       "          [[0.0020, 0.0010, 0.0010]],\n",
       " \n",
       "          [[0.0020, 0.0010, 0.0010]],\n",
       " \n",
       "          [[0.0020, 0.0010, 0.0010]],\n",
       " \n",
       "          [[0.0020, 0.0010, 0.0010]],\n",
       " \n",
       "          [[0.0020, 0.0010, 0.0010]]],\n",
       " \n",
       " \n",
       "         [[[0.0020, 0.0010, 0.0010]],\n",
       " \n",
       "          [[0.0020, 0.0010, 0.0010]],\n",
       " \n",
       "          [[0.0020, 0.0010, 0.0010]],\n",
       " \n",
       "          [[0.0020, 0.0010, 0.0010]],\n",
       " \n",
       "          [[0.0020, 0.0010, 0.0010]],\n",
       " \n",
       "          [[0.0020, 0.0010, 0.0010]]],\n",
       " \n",
       " \n",
       "         [[[0.0020, 0.0010, 0.0010]],\n",
       " \n",
       "          [[0.0020, 0.0010, 0.0010]],\n",
       " \n",
       "          [[0.0020, 0.0010, 0.0010]],\n",
       " \n",
       "          [[0.0020, 0.0010, 0.0010]],\n",
       " \n",
       "          [[0.0020, 0.0010, 0.0010]],\n",
       " \n",
       "          [[0.0020, 0.0010, 0.0010]]],\n",
       " \n",
       " \n",
       "         [[[0.0020, 0.0010, 0.0010]],\n",
       " \n",
       "          [[0.0020, 0.0010, 0.0010]],\n",
       " \n",
       "          [[0.0020, 0.0010, 0.0010]],\n",
       " \n",
       "          [[0.0020, 0.0010, 0.0010]],\n",
       " \n",
       "          [[0.0020, 0.0010, 0.0010]],\n",
       " \n",
       "          [[0.0020, 0.0010, 0.0010]]]], requires_grad=True))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iniate rows as 0.002, 0.001, and 0.001 for easier view of the weight impact\n",
    "with torch.no_grad():\n",
    "    c2_pattern = torch.tensor([0.002,0.001,0.001]).view(1,1,1,c2_kernel_width).expand(conv2.size()).clone()\n",
    "    conv2.copy_(c2_pattern)\n",
    "conv2.size(), conv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef61c77d-3200-482f-8ca8-5e94a66d0099",
   "metadata": {},
   "source": [
    "##### 1x3 Conv 2 Stride - Run convolution\n",
    "\n",
    "Now we'll calculate the 2-D discrete cross-correlation for our weight and ReLU output `out`. For our convolutional layer, we won't go through step by step and instead do the following: \n",
    "1. Unfold each sliding kernel_size-sized block within the spatial dimensions of input into a column (i.e., last dimension) of a 3-D output tensor of shape $(B,C*k_h*k_w,L)$.  This is the step that does our downsampling by using the `(1,2)` stride.\n",
    "2. Reshape our weights to match our downsampled input and to allow it to be reused across batches. \n",
    "3. Take the dot product of the unstacked input and the stacked weights and reshape the result back to our batch and channels.\n",
    "\n",
    "\n",
    "*Let's start by highlighting the dimensions of our input. We won't reinitialize as we want this step to error if there are dimensions not aligned with expectations.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c3197dd-e7e7-4ce1-ae4e-3fe27409ea50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 8]), 2, 6, 1, 8)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size(), batch, channel, height, width, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2204b8a-d26b-4f61-ac3b-daeeb7ae6ead",
   "metadata": {},
   "source": [
    "**Calculate expected unfolded dimensions**  \n",
    "\n",
    "To do our reshaping in this layer, we need to calculate the expected dimensions for our loop.  \n",
    "Recall that we expect to go from $(B,C,1,T)$ to $(B,C*k_h*k_w,L)$ where $L$ is a flattening or our output height and width as follows: \n",
    "$$\n",
    "\\begin{align}\n",
    "height_{out} &= (height + 2*pad_h - 1*(kernel_h-1) -1)\\ //\\ stride_{h}\\\\ \n",
    "width_{out} &= (width + 2*pad_w - 1*(kernel_w-1) -1)\\ //\\ stride_{w}\\\\\n",
    "L &= height_{out} * width_{out}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We'll see here that the main dimension that changes is $L$.  Review how L is determined above, you can see that the $width_{out}$ is a result of the $width //\\ stride_{w}$ meaning that for this convolution where $stride_{w} = 2$, we cut the final width in half. This is our **downsampling** in action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b33e2e65-d7eb-47e8-a757-b6a181d7596f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Conv: idth out 8, height out 1, final dimension (2,18,8)\n",
      "This  Conv: width out 4, height out 1, final dimension (2,18,4)\n"
     ]
    }
   ],
   "source": [
    "c2_khw = channel*c2_kernel_height*c2_kernel_width\n",
    "\n",
    "c2_height_out = (height + 2*c2_padding_height - 1*(c2_kernel_height-1) - 1)//c2_stride_height + 1   # = 1, \n",
    "c2_width_out = (width + 2*c2_padding_width - 1*(c2_kernel_width-1) - 1)//c2_stride_width + 1   # = 4\n",
    "c2_L = c2_height_out * c2_width_out\n",
    "\n",
    "print(f'First Conv: idth out {c1_width_out}, height out {c1_height_out}, final dimension ({batch},{c1_khw},{c1_L})')\n",
    "print(f'This  Conv: width out {c2_width_out}, height out {c2_height_out}, final dimension ({batch},{c2_khw},{c2_L})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8d0c21f0-b930-4d43-ad0e-72ffbfa82925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.1805, 0.4135, 0.1153, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.1805, 0.4135, 0.1153, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.1805, 0.4135, 0.1153, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.1805, 0.4135, 0.1153, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.1805, 0.4135, 0.1153, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.1805, 0.4135, 0.1153, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0780, 0.1340, 0.0967, 0.1526, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0780, 0.1340, 0.0967, 0.1526, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0780, 0.1340, 0.0967, 0.1526, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0780, 0.1340, 0.0967, 0.1526, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0780, 0.1340, 0.0967, 0.1526, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0780, 0.1340, 0.0967, 0.1526, 0.0000, 0.0000]]]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bedfee-3955-412f-affb-1fad79138e42",
   "metadata": {},
   "source": [
    "**Unfold**  Now we'll do the actual unfolding.  In this case we'll run the single function and you can see that we get a \"narrower\" output. An interesting observation here is that we saw in the ReLU step we had multiple columns of zero since they were negative.  Even with our stride of $(1,2)$ those zeros are persisting as a full column. We may wonder: why is there a full column, after unfolding, of zero.  Let's think about our kernel, it has a width of 3.  This means that we would have to have 3 columns next to each other that are 0, and, if we check, that's exactly what happened.  Looking forward then we can expect that this channel will be 0 once we multiply with our weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c75c07f-688c-4405-abd1-977123fd3ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 18, 4]),\n",
       " tensor([[[0.0000, 0.4135, 0.0000, 0.0000],\n",
       "          [0.1805, 0.1153, 0.0000, 0.0000],\n",
       "          [0.4135, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.4135, 0.0000, 0.0000],\n",
       "          [0.1805, 0.1153, 0.0000, 0.0000],\n",
       "          [0.4135, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.4135, 0.0000, 0.0000],\n",
       "          [0.1805, 0.1153, 0.0000, 0.0000],\n",
       "          [0.4135, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.4135, 0.0000, 0.0000],\n",
       "          [0.1805, 0.1153, 0.0000, 0.0000],\n",
       "          [0.4135, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.4135, 0.0000, 0.0000],\n",
       "          [0.1805, 0.1153, 0.0000, 0.0000],\n",
       "          [0.4135, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.4135, 0.0000, 0.0000],\n",
       "          [0.1805, 0.1153, 0.0000, 0.0000],\n",
       "          [0.4135, 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.1340, 0.1526],\n",
       "          [0.0000, 0.0780, 0.0967, 0.0000],\n",
       "          [0.0000, 0.1340, 0.1526, 0.0000],\n",
       "          [0.0000, 0.0000, 0.1340, 0.1526],\n",
       "          [0.0000, 0.0780, 0.0967, 0.0000],\n",
       "          [0.0000, 0.1340, 0.1526, 0.0000],\n",
       "          [0.0000, 0.0000, 0.1340, 0.1526],\n",
       "          [0.0000, 0.0780, 0.0967, 0.0000],\n",
       "          [0.0000, 0.1340, 0.1526, 0.0000],\n",
       "          [0.0000, 0.0000, 0.1340, 0.1526],\n",
       "          [0.0000, 0.0780, 0.0967, 0.0000],\n",
       "          [0.0000, 0.1340, 0.1526, 0.0000],\n",
       "          [0.0000, 0.0000, 0.1340, 0.1526],\n",
       "          [0.0000, 0.0780, 0.0967, 0.0000],\n",
       "          [0.0000, 0.1340, 0.1526, 0.0000],\n",
       "          [0.0000, 0.0000, 0.1340, 0.1526],\n",
       "          [0.0000, 0.0780, 0.0967, 0.0000],\n",
       "          [0.0000, 0.1340, 0.1526, 0.0000]]], grad_fn=<Im2ColBackward0>))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2_unfolded = F.unfold(out, \n",
    "\t\tkernel_size=(c2_kernel_height, c2_kernel_width),  # (1,3)\n",
    "\t\tpadding=(c2_padding_height, c2_padding_width), #(0,1)\n",
    "\t\tstride=(c2_stride_height, c2_stride_width))#(1,2)\n",
    "c2_unfolded.size() , c2_unfolded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fee877f-b612-4dcf-9c62-d7b78539414b",
   "metadata": {},
   "source": [
    "##### 1x3 Conv 2 Stride - $W\\cdot X_{unfolded}$\n",
    "\n",
    "Now that we have our unstacked patches that are downsampled, we can then let our network decide how much of the patch, and which part of the patch, influences our output.  To do this we take the dot product of the weight with the unfolded input.  We do have an issue though since our weight is `[6,6,1,3]` but our input is `[2x18x4]`. We will solve this simply by squeezing the last two dimensions of our Weights together to result in a `[6,18]` tensor that we can multiply.  \n",
    "\n",
    "You might be now asking \"what about the batch dimensions of 2\".  Just like in our first convolution, we do want to make sure the 2 different batches actually share the same weight so we don't actually want to increase our weight dimension.  Instead we rely on the pytorch which broadcasts the same matrix to each of the batches in the input automatically.  This allows the two batches to share the weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ba5c5c52-4640-484c-b72f-a3c366f1f359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 18]),\n",
       " tensor([[0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010,\n",
       "          0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010],\n",
       "         [0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010,\n",
       "          0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010],\n",
       "         [0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010,\n",
       "          0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010],\n",
       "         [0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010,\n",
       "          0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010],\n",
       "         [0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010,\n",
       "          0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010],\n",
       "         [0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010,\n",
       "          0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010]],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stacks creates a 2-d matrix of `out_channelX rest` so `6*18` by stacking the weights we match the shape of \n",
    "conv2_weigth = conv2.view(n_embd, -1) # [6,6,1,3] > [6,18]\n",
    "conv2_weigth.size(), conv2_weigth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cf6a9f-8521-44b0-aa35-afd3eff8fe14",
   "metadata": {},
   "source": [
    "Now that we have the weights in the dimension we want them we're ready to multiply them with the unfolded input.  Because in our weight matrix each \"row\" is the same, each of our column entries in the result will be equal. We can also see that our zero columns persist.  We'll also get a final output of `[2,6,4]` compressing the 18 down. Also note that the batch dimension is maintained as the weight is broadcast across the batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44222855-5c5d-4d01-a9f9-a6cda20965bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 4]),\n",
       " tensor([[[0.0036, 0.0057, 0.0000, 0.0000],\n",
       "          [0.0036, 0.0057, 0.0000, 0.0000],\n",
       "          [0.0036, 0.0057, 0.0000, 0.0000],\n",
       "          [0.0036, 0.0057, 0.0000, 0.0000],\n",
       "          [0.0036, 0.0057, 0.0000, 0.0000],\n",
       "          [0.0036, 0.0057, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0013, 0.0031, 0.0018],\n",
       "          [0.0000, 0.0013, 0.0031, 0.0018],\n",
       "          [0.0000, 0.0013, 0.0031, 0.0018],\n",
       "          [0.0000, 0.0013, 0.0031, 0.0018],\n",
       "          [0.0000, 0.0013, 0.0031, 0.0018],\n",
       "          [0.0000, 0.0013, 0.0031, 0.0018]]], grad_fn=<CloneBackward0>))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = conv2_weigth @ c2_unfolded\n",
    "out.size(), out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6194edf2-caca-4e6a-8b5d-0c17cf636351",
   "metadata": {},
   "source": [
    "Finally, like in the previous convolution, we need to resize our last dimension back to our target channel height and width. Since our height is 1 it will just insert in another dimension of 1 without looking significantly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "126da870-19af-41b7-aa8e-f0fcb0d2c635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 4]),\n",
       " tensor([[[[0.0036, 0.0057, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0036, 0.0057, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0036, 0.0057, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0036, 0.0057, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0036, 0.0057, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0036, 0.0057, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.0000, 0.0013, 0.0031, 0.0018]],\n",
       " \n",
       "          [[0.0000, 0.0013, 0.0031, 0.0018]],\n",
       " \n",
       "          [[0.0000, 0.0013, 0.0031, 0.0018]],\n",
       " \n",
       "          [[0.0000, 0.0013, 0.0031, 0.0018]],\n",
       " \n",
       "          [[0.0000, 0.0013, 0.0031, 0.0018]],\n",
       " \n",
       "          [[0.0000, 0.0013, 0.0031, 0.0018]]]], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert in the channel dimension to go back to 1/2 of [B,C,1,T] since we took a stride of 2\n",
    "out = out.view(batch,n_embd, c2_height_out, c2_width_out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c911dd3-47b6-4534-a7e2-f0af6ea7f7d7",
   "metadata": {},
   "source": [
    "#### Convolution Block - Second Batch Norm\n",
    "\n",
    "Again, on the output of the convolutional layer we apply another 2D batch normalization. Batch normalization (2D), normalize each channel using statistics computed over the current mini-batch and spatial positions. Concretely, for inputs of shape $[B,C,H,T]$, batch normalization computes a per-channel mean and variance across ${B,C,1}$ and standardizes that channel. This helps to stabilize activations, act as a mild regularizer, and speed up training. If an entire channel is scaled uniformly across the batch and spatial locations, that scale is largely removed by the normalization. This means an array of `[1,2,3,4]` and `[2,4,6,8]` will have the same normalized entries after batch normalization.\n",
    "\n",
    "Batch normalization applies the following:\n",
    "$$\n",
    "y_{b,c,h,t}=\\gamma_c \\frac{x_{b,c,h,t}-\\mu_c}{\\sqrt{\\sigma_c^2+\\epsilon}}+\\beta_c,\n",
    "\\quad\n",
    "\\mu_c=\\mathbb{E}_{b,h,t}[x_{b,c,h,t}],\\quad\n",
    "\\sigma_c^2=\\operatorname{Var}_{b,h,t}[x_{b,c,h,t}].\n",
    "$$\n",
    "\n",
    "Batch normalization is again applied per channel on the feature map. Initially, batch normalization creates affine parameters with $\\gamma_c=1$ and $\\beta_c=0$ so that all channels are scaled and shifted equally. We’ll keep this initialization unchanged. With training though, they adjust based on the gradient. Since initiation is still 1, we'll see that the values are repeated across the columns but the values themselves now span both positive and negative given they represent scaled distance from the mean of the batch. \n",
    "\n",
    "*Note that even though we may apply batch normalization in multiple places, we keep it as a separate layer so its effect can be tuned independently of other normalization layers.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "85a762c9-fd66-44ed-83e2-50838ecdac6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0.], requires_grad=True))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_b = nn.BatchNorm2d(n_embd)   \n",
    "bn_b.weight, bn_b.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7ba14628-8817-48ed-87d2-2c580c995461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 4]),\n",
       " tensor([[[[ 0.4422,  1.0070, -0.5211, -0.5211]],\n",
       " \n",
       "          [[ 0.4422,  1.0070, -0.5211, -0.5211]],\n",
       " \n",
       "          [[ 0.4422,  1.0070, -0.5211, -0.5211]],\n",
       " \n",
       "          [[ 0.4422,  1.0070, -0.5211, -0.5211]],\n",
       " \n",
       "          [[ 0.4422,  1.0070, -0.5211, -0.5211]],\n",
       " \n",
       "          [[ 0.4422,  1.0070, -0.5211, -0.5211]]],\n",
       " \n",
       " \n",
       "         [[[-0.5211, -0.1773,  0.3175, -0.0262]],\n",
       " \n",
       "          [[-0.5211, -0.1773,  0.3175, -0.0262]],\n",
       " \n",
       "          [[-0.5211, -0.1773,  0.3175, -0.0262]],\n",
       " \n",
       "          [[-0.5211, -0.1773,  0.3175, -0.0262]],\n",
       " \n",
       "          [[-0.5211, -0.1773,  0.3175, -0.0262]],\n",
       " \n",
       "          [[-0.5211, -0.1773,  0.3175, -0.0262]]]],\n",
       "        grad_fn=<NativeBatchNormBackward0>))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = bn_b(out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e654c487-4eee-4750-bec0-b52d17a75be1",
   "metadata": {},
   "source": [
    "### Residual Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84da273-76fb-498a-a127-30181a99f0d4",
   "metadata": {},
   "source": [
    "<img src=\"explainer_screenshots/cnn/resdiual_connection.png\" width=\"200\">\n",
    "\n",
    "ResNets are a popular flavor of CNN and ustilize a key component called a \"residual connection\".  To showcase it's value we'll also include it here. Residual connections, often also called \"skip connections\" allow for pathways to bypass around other layers, passing through gradients during the backward pass.  This attribute ensures that the impact of each layer is normalized against the embeddings that are passed around. Functionally this creates a connection represented by:\n",
    "\n",
    "$$\n",
    "y = f(x) + x\n",
    "$$\n",
    "\n",
    "In our case though, we have a problem simply summing the two layers together. Our current token embedding is `[2,6,1,8]` but because of our downsampling, the output from our batch normalization is `[2,6,1,4]`.   We do have a couple options: we could find a way to upsample our convolutional output through *dilation* or we could downsample our residual connection to match the dimensions.  We'll choose to do a downsampled projection of our residual connection. This approach preserves the intended downsampling in the main, convolution block, branch, doesn't distort its receptive field or batch-norm statistics, and keeps the main branch’s computation and optimization behavior unchanged. We are not choosing dilation as it would alter the kernel’s geometry and typically increase compute without clear benefits. \n",
    "\n",
    "To do our *downsampling*, it's actually similar to our convolution down-sampling, but we just just a simple `(1,1)` patch and a stride of `(1,2)`.  Unlike the convolution layers this far, this connection will prevent  every other entry in `x` from being able to impact our output through the residual connection.  Overall though this is not a major concern as we still maintain the benefit of the residual connection's ability to connect gradients around the convolution.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519e6f1d-16d3-4739-ad51-7d3d51f21bf7",
   "metadata": {},
   "source": [
    "#### Residual Connection - Downsampling 1x1 Convolution 2 Stride"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf63d83-b803-4efa-b06c-a778b73466de",
   "metadata": {},
   "source": [
    "##### 1x1 Conv 2 Stride - Initialize weights\n",
    "Our residual connection's convolutional block uses a kernel width of `(1,1)`, a stride of `(1,2)` without padding. Padding is not needed because the kernel is small enough that with our stride we can still cover our input edge to edge. The `(1,2)` stride will downsample our channel from 8 to 4 reducing the size of our output to our desired target.  Because our stride is `(1,2)` and the kernel is `(1,1)` we will skip every other input cell during the downsampling.\n",
    "\n",
    "To start, we will setup our weights to be based on the channel dimension, currently equal to our embedding, and our kernel. We'll also initialize our weights to be 1.0 as we want to demonstrate the benefit of the residual connection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fe985974-0c06-4f6a-998f-28bad44946d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernel': (1, 1), 'stride': (1, 2), 'padding': (0, 0)}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_kernel_height = 1\n",
    "res_kernel_width = 1\n",
    "res_stride_height = 1\n",
    "res_stride_width = 2\n",
    "res_padding_height = 0\n",
    "res_padding_width = 0\n",
    "{'kernel': (res_kernel_height, res_kernel_width),\n",
    " 'stride': (res_stride_height, res_stride_width),\n",
    " 'padding': (res_padding_height, res_padding_width)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dbbcf6b0-4bd5-4dd5-aa42-ea00864be1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "convRes = nn.Parameter(\n",
    "    torch.empty(n_embd, n_embd, res_kernel_height, res_kernel_width), \n",
    "    requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "db6d368b-0930-425a-acb0-33a02bce26b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 6, 1, 1]),\n",
       " Parameter containing:\n",
       " tensor([[[[1.]],\n",
       " \n",
       "          [[1.]],\n",
       " \n",
       "          [[1.]],\n",
       " \n",
       "          [[1.]],\n",
       " \n",
       "          [[1.]],\n",
       " \n",
       "          [[1.]]],\n",
       " \n",
       " \n",
       "         [[[1.]],\n",
       " \n",
       "          [[1.]],\n",
       " \n",
       "          [[1.]],\n",
       " \n",
       "          [[1.]],\n",
       " \n",
       "          [[1.]],\n",
       " \n",
       "          [[1.]]],\n",
       " \n",
       " \n",
       "         [[[1.]],\n",
       " \n",
       "          [[1.]],\n",
       " \n",
       "          [[1.]],\n",
       " \n",
       "          [[1.]],\n",
       " \n",
       "          [[1.]],\n",
       " \n",
       "          [[1.]]],\n",
       " \n",
       " \n",
       "         [[[1.]],\n",
       " \n",
       "          [[1.]],\n",
       " \n",
       "          [[1.]],\n",
       " \n",
       "          [[1.]],\n",
       " \n",
       "          [[1.]],\n",
       " \n",
       "          [[1.]]],\n",
       " \n",
       " \n",
       "         [[[1.]],\n",
       " \n",
       "          [[1.]],\n",
       " \n",
       "          [[1.]],\n",
       " \n",
       "          [[1.]],\n",
       " \n",
       "          [[1.]],\n",
       " \n",
       "          [[1.]]],\n",
       " \n",
       " \n",
       "         [[[1.]],\n",
       " \n",
       "          [[1.]],\n",
       " \n",
       "          [[1.]],\n",
       " \n",
       "          [[1.]],\n",
       " \n",
       "          [[1.]],\n",
       " \n",
       "          [[1.]]]], requires_grad=True))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    torch.nn.init.ones_(convRes)\n",
    "convRes.size(), convRes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab74a0cc-cf1e-4444-82ba-7d5be9a00e88",
   "metadata": {},
   "source": [
    "##### 1x1 Conv 2 Stride - Run convolution\n",
    "\n",
    "Now we'll calculate the 2-D discrete cross-correlation for our weight and residual connection input `x`. For our convolutional layer, we won't go through step by step and instead do the following: \n",
    "1. Unfold each sliding kernel_size-sized block within the spatial dimensions of input into a column (i.e., last dimension) of a 3-D output tensor of shape $(B,C*k_h*k_w,L)$.  This is the step that does our downsampling by using the `(1,2)` stride.\n",
    "2. Reshape our weights to match our downsampled input and to allow it to be reused across batches. \n",
    "3. Take the dot product of the unstacked input and the stacked weights and reshape the result back to our batch and channels.\n",
    "\n",
    "\n",
    "*Let's start by highlighting our input `x` and the dimensions of our input. We won't reinitialize as we want this step to error if there are dimensions not aligned with expectations.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bf631813-1501-4ea8-9349-57c3930b645a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.1500, 0.1400, 0.1100, 0.0200, 0.1300, 0.0100, 0.0700, 0.1200]],\n",
       "\n",
       "         [[0.1600, 0.1500, 0.1200, 0.0300, 0.1400, 0.0200, 0.0800, 0.1300]],\n",
       "\n",
       "         [[0.1700, 0.1600, 0.1300, 0.0400, 0.1500, 0.0300, 0.0900, 0.1400]],\n",
       "\n",
       "         [[0.1800, 0.1700, 0.1400, 0.0500, 0.1600, 0.0400, 0.1000, 0.1500]],\n",
       "\n",
       "         [[0.1900, 0.1800, 0.1500, 0.0600, 0.1700, 0.0500, 0.1100, 0.1600]],\n",
       "\n",
       "         [[0.2000, 0.1900, 0.1600, 0.0700, 0.1800, 0.0600, 0.1200, 0.1700]]],\n",
       "\n",
       "\n",
       "        [[[0.0900, 0.0500, 0.1000, 0.1100, 0.0700, 0.1200, 0.0900, 0.0100]],\n",
       "\n",
       "         [[0.1000, 0.0600, 0.1100, 0.1200, 0.0800, 0.1300, 0.1000, 0.0200]],\n",
       "\n",
       "         [[0.1100, 0.0700, 0.1200, 0.1300, 0.0900, 0.1400, 0.1100, 0.0300]],\n",
       "\n",
       "         [[0.1200, 0.0800, 0.1300, 0.1400, 0.1000, 0.1500, 0.1200, 0.0400]],\n",
       "\n",
       "         [[0.1300, 0.0900, 0.1400, 0.1500, 0.1100, 0.1600, 0.1300, 0.0500]],\n",
       "\n",
       "         [[0.1400, 0.1000, 0.1500, 0.1600, 0.1200, 0.1700, 0.1400, 0.0600]]]],\n",
       "       grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d3493684-8e7e-4b75-a56b-45dc6e8e2b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 8]), 2, 6, 1, 8)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size(), batch, channel, height, width, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0645c0-1eb9-44a2-8c37-7687a0fe4628",
   "metadata": {},
   "source": [
    "**Calculate expected unfolded dimensions**  \n",
    "\n",
    "To do our reshaping in this layer, we need to calculate the expected dimensions for our loop.  \n",
    "Recall that we expect to go from $(B,C,1,T)$ to $(B,C*k_h*k_w,L)$ where $L$ is a flattening or our output height and width as follows: \n",
    "$$\n",
    "\\begin{align}\n",
    "height_{out} &= (height + 2*pad_h - 1*(kernel_h-1) -1)\\ //\\ stride_{h}\\\\ \n",
    "width_{out} &= (width + 2*pad_w - 1*(kernel_w-1) -1)\\ //\\ stride_{w}\\\\\n",
    "L &= height_{out} * width_{out}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "One big difference  you'll see here compared to previous convolution layers is that, because our stride is `(1,1)`, we do not actually change our channel dimension. Additionally, we expect that our output will be half of our `height*width` since we have a stride of `(1,2)` because we're downsampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4a1a069d-3c1e-4dda-9c67-0208b00c341d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second  Conv: width out 4, height out 1, final dimension (2,18,4)\n",
      "This  Conv: width out 4, height out 1, final dimension (2,6,4)\n"
     ]
    }
   ],
   "source": [
    "res_khw = channel*res_kernel_height*res_kernel_width\n",
    "\n",
    "res_height_out = (height + 2*res_padding_height - 1*(res_kernel_height-1) - 1)//res_stride_height + 1   # = 1, \n",
    "res_width_out = (width + 2*res_padding_width - 1*(res_kernel_width-1) - 1)//res_stride_width + 1   # = 4\n",
    "res_L = res_height_out * res_width_out\n",
    "\n",
    "print(f'Second  Conv: width out {c2_width_out}, height out {c2_height_out}, final dimension ({batch},{c2_khw},{c2_L})')\n",
    "print(f'This  Conv: width out {res_width_out}, height out {res_height_out}, final dimension ({batch},{res_khw},{res_L})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8438064-18c3-4af0-aae8-bbfdf293f118",
   "metadata": {},
   "source": [
    "**Unfold**  Now we'll do the actual unfolding. This unfolding, contrary to previous unfolding, is just selecting every other \"column\" due to our kernel size and stride. If we go back to how we setup `x` it was a rearrangement that lead to `(B,C,1,T)` so by selecting every other \"column\" we're selecting the channels corresponding to every other token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "673dfef5-e401-4ab9-bf95-2bd5c6ecac34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 4]),\n",
       " tensor([[[0.1500, 0.1100, 0.1300, 0.0700],\n",
       "          [0.1600, 0.1200, 0.1400, 0.0800],\n",
       "          [0.1700, 0.1300, 0.1500, 0.0900],\n",
       "          [0.1800, 0.1400, 0.1600, 0.1000],\n",
       "          [0.1900, 0.1500, 0.1700, 0.1100],\n",
       "          [0.2000, 0.1600, 0.1800, 0.1200]],\n",
       " \n",
       "         [[0.0900, 0.1000, 0.0700, 0.0900],\n",
       "          [0.1000, 0.1100, 0.0800, 0.1000],\n",
       "          [0.1100, 0.1200, 0.0900, 0.1100],\n",
       "          [0.1200, 0.1300, 0.1000, 0.1200],\n",
       "          [0.1300, 0.1400, 0.1100, 0.1300],\n",
       "          [0.1400, 0.1500, 0.1200, 0.1400]]], grad_fn=<Im2ColBackward0>))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_unfolded = F.unfold(x, \n",
    "\t\tkernel_size=(res_kernel_height, res_kernel_width),  # (1,1)\n",
    "\t\tpadding=(res_padding_height, res_padding_width), #(0,0)\n",
    "\t\tstride=(res_stride_height, res_stride_width))#(1,2)\n",
    "x_unfolded.size() , x_unfolded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42574e2-69cc-43c4-b3f3-78fde2be506e",
   "metadata": {},
   "source": [
    "##### 1x1 Conv 2 Stride - $W\\cdot X_{unfolded}$\n",
    "\n",
    "Now that we have unfolded the patches that are downsampled, we can then let our network decide which patch influences our output through the residual connection.  To do this we take the dot product of the weight with the unfolded input.  We do have an issue though since our weight is `[6,6,1,1]` but our input is `[2x6x4]`. We will solve this simply by squeezing the last two dimensions of our weights together to result in a `[6,6]` tensor that we can multiply.  \n",
    "\n",
    "By now you also hopefully know that this tensor will be shared across the two batches through broadcasting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a604be43-82a3-40e2-a5bc-939af199fa2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 6]),\n",
       " tensor([[1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.]], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convRes_weigth = convRes.view(n_embd, -1) # [6,6,1,1] > [6,6]\n",
    "convRes_weigth.size(), convRes_weigth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dab6d60-7c13-4392-8d09-8a86224f8aa8",
   "metadata": {},
   "source": [
    "Now that we have the weights in the dimension we want them we're ready to multiply them with the unfolded input.  Because in our weight matrix each entry is 1 currently, each of our column entries in the result will be the sum of the column, or equal. We'll also get a final output of `[2,6,4]` matching our target dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dca05ebd-aa90-445b-9110-45782c2fd100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 4]),\n",
       " tensor([[[1.0500, 0.8100, 0.9300, 0.5700],\n",
       "          [1.0500, 0.8100, 0.9300, 0.5700],\n",
       "          [1.0500, 0.8100, 0.9300, 0.5700],\n",
       "          [1.0500, 0.8100, 0.9300, 0.5700],\n",
       "          [1.0500, 0.8100, 0.9300, 0.5700],\n",
       "          [1.0500, 0.8100, 0.9300, 0.5700]],\n",
       " \n",
       "         [[0.6900, 0.7500, 0.5700, 0.6900],\n",
       "          [0.6900, 0.7500, 0.5700, 0.6900],\n",
       "          [0.6900, 0.7500, 0.5700, 0.6900],\n",
       "          [0.6900, 0.7500, 0.5700, 0.6900],\n",
       "          [0.6900, 0.7500, 0.5700, 0.6900],\n",
       "          [0.6900, 0.7500, 0.5700, 0.6900]]], grad_fn=<CloneBackward0>))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identity = convRes_weigth @ x_unfolded\n",
    "identity.size(), identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cfff31-3e6d-40a3-8ff1-98eb00e24b16",
   "metadata": {},
   "source": [
    "Finally, like in the previous convolution, we need to resize our last dimension back to our target channel height and width. Since our height is 1 it will just insert in another dimension of 1 without looking significantly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d70de44a-41be-4941-ac9d-21c8ff8316f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 4]),\n",
       " tensor([[[[1.0500, 0.8100, 0.9300, 0.5700]],\n",
       " \n",
       "          [[1.0500, 0.8100, 0.9300, 0.5700]],\n",
       " \n",
       "          [[1.0500, 0.8100, 0.9300, 0.5700]],\n",
       " \n",
       "          [[1.0500, 0.8100, 0.9300, 0.5700]],\n",
       " \n",
       "          [[1.0500, 0.8100, 0.9300, 0.5700]],\n",
       " \n",
       "          [[1.0500, 0.8100, 0.9300, 0.5700]]],\n",
       " \n",
       " \n",
       "         [[[0.6900, 0.7500, 0.5700, 0.6900]],\n",
       " \n",
       "          [[0.6900, 0.7500, 0.5700, 0.6900]],\n",
       " \n",
       "          [[0.6900, 0.7500, 0.5700, 0.6900]],\n",
       " \n",
       "          [[0.6900, 0.7500, 0.5700, 0.6900]],\n",
       " \n",
       "          [[0.6900, 0.7500, 0.5700, 0.6900]],\n",
       " \n",
       "          [[0.6900, 0.7500, 0.5700, 0.6900]]]], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identity = identity.view(batch,n_embd, res_height_out, res_width_out)\n",
    "identity.size(), identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c55f19-e328-40ab-a9b5-c833c66c53e4",
   "metadata": {},
   "source": [
    "#### Residual Connection - Sum \n",
    "\n",
    "Now that we have our token projection downsampled to match our convolution block output dimension we're ready to connect the two weights together.  Recall that the residual connection performs:  \n",
    "\n",
    "$y = f(x) + x$\n",
    "\n",
    "Because of this join, during the backward pass we're now able to let gradients flow through the convolution layers and around them.  We'll start by reprinting the output of the convolution block and then showing the sum.  Notice that because of how we initialized our weights across the inputs, in both our residual chain and main chain the current calculated weights have uniformity per channel (visually column). Because of this, we then expect that our connection will maintain this behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bb117fa6-99dc-4b09-9d62-6a69c5affe80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 4]),\n",
       " tensor([[[[ 0.4422,  1.0070, -0.5211, -0.5211]],\n",
       " \n",
       "          [[ 0.4422,  1.0070, -0.5211, -0.5211]],\n",
       " \n",
       "          [[ 0.4422,  1.0070, -0.5211, -0.5211]],\n",
       " \n",
       "          [[ 0.4422,  1.0070, -0.5211, -0.5211]],\n",
       " \n",
       "          [[ 0.4422,  1.0070, -0.5211, -0.5211]],\n",
       " \n",
       "          [[ 0.4422,  1.0070, -0.5211, -0.5211]]],\n",
       " \n",
       " \n",
       "         [[[-0.5211, -0.1773,  0.3175, -0.0262]],\n",
       " \n",
       "          [[-0.5211, -0.1773,  0.3175, -0.0262]],\n",
       " \n",
       "          [[-0.5211, -0.1773,  0.3175, -0.0262]],\n",
       " \n",
       "          [[-0.5211, -0.1773,  0.3175, -0.0262]],\n",
       " \n",
       "          [[-0.5211, -0.1773,  0.3175, -0.0262]],\n",
       " \n",
       "          [[-0.5211, -0.1773,  0.3175, -0.0262]]]],\n",
       "        grad_fn=<NativeBatchNormBackward0>))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size(), out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9b339043-c914-46fe-8cac-e9a7201329a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 4]),\n",
       " tensor([[[[1.4922, 1.8170, 0.4089, 0.0489]],\n",
       " \n",
       "          [[1.4922, 1.8170, 0.4089, 0.0489]],\n",
       " \n",
       "          [[1.4922, 1.8170, 0.4089, 0.0489]],\n",
       " \n",
       "          [[1.4922, 1.8170, 0.4089, 0.0489]],\n",
       " \n",
       "          [[1.4922, 1.8170, 0.4089, 0.0489]],\n",
       " \n",
       "          [[1.4922, 1.8170, 0.4089, 0.0489]]],\n",
       " \n",
       " \n",
       "         [[[0.1689, 0.5727, 0.8875, 0.6638]],\n",
       " \n",
       "          [[0.1689, 0.5727, 0.8875, 0.6638]],\n",
       " \n",
       "          [[0.1689, 0.5727, 0.8875, 0.6638]],\n",
       " \n",
       "          [[0.1689, 0.5727, 0.8875, 0.6638]],\n",
       " \n",
       "          [[0.1689, 0.5727, 0.8875, 0.6638]],\n",
       " \n",
       "          [[0.1689, 0.5727, 0.8875, 0.6638]]]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = out + identity\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c7b12e-5352-429b-82e1-92b9f34b9d16",
   "metadata": {},
   "source": [
    "### Output Layers AKA Model Head."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438b2c7e-44c0-4f7f-b7e9-15df4d9f7123",
   "metadata": {},
   "source": [
    "We've now shown a common pattern for a convolutional block  inside a downsampling  along with a residual connection that's common inside of ResNets. Once those layers are complete during the forward pass we then start the output process that results in `logits` which is a representation of the probability of each token being the next token given the input.  \n",
    "\n",
    "<img src=\"explainer_screenshots/cnn/output_layer.png\" width=\"200\">\n",
    "\n",
    "This layer is also known as the model **head** and the weights learned are more task specific than the general model itself.  Once a model is trained, there are processes to swap out the \"head\" and learn new tasks, e.g. go from next token prediction to classification.  In our example case, this is a linear layer mapping the backbone to vocab logits.\n",
    "\n",
    "Recall in the beginning that we discussed that each batch had 1 example in it and our goal was to predict the next token for that 1 specific example. To predict the tokens we'll generate `logits`, or a pseudo-probability linked to each vocab entry to express how likely it is to be the next token. \n",
    "\n",
    "We do have an issue though.  Currently we still have an expanded T dimension: `[2,6,1,4]`.  If we did our usual route of linear projection, we'd end up with 4 sets of logits per example.  Luckily, this is a common occurrence in CNNs and one that does NOT need downsampling to resolve.  Instead we use a technique called pooling that reduces the dimension for us. After pooling we'll then undo our dimension adjustments we made at the start going from `[B,C,1,T]` $\\rightarrow$ `[B,T,C]`. And then we'll do our final linear projection to create our logits. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a0b7e7-59fa-4810-9697-fa594006ed3f",
   "metadata": {},
   "source": [
    "#### Output Layer - Adaptive Average Pooling\n",
    "\n",
    "Pooling is a downsampling operation common in CNNs that reduces the spatial dimensions of a feature map by aggregating values within a sliding window. The difference compared to a convolution is that there's typically no overlapping of the sliding window like there may be with kernels and there are no weights to be learned. We'll use a common version of pulling called adaptive average pooling.  \n",
    "\n",
    "Adaptive average pooling down-samples the input feature maps by dividing them into a grid of rectangular regions (sliding window) and replacing each region with its average value. The *adaptive* part is due to the fact that the function dynamically adjusts the sliding window size to allow for a fixed-size output regardless of the input feature map's dimensions. In CNNs this is nice since a traditional modality like images may have different resolutions, meaning different sized inputs, but the adaptive average pooling can ensure that they all output this step at the same size. \n",
    "\n",
    "In our example, each batch should include just 1 example, so we need to squeeze the output `x` from `[2,6,1,4]` to `[2,6,1,1]`. As a reminder, at this point while the column values are different, each \"row\" on the T dimension contains the same values, so we expect that the output of the average will be the same.  This is now a precursor to showing us that our output at this point won't be great. \n",
    "\n",
    "*Note that this function is the same as running x.mean(dim=(2,3), keepdim=True)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8c0d304f-11ed-4de2-9f36-7e0c6ccdcb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgPool = nn.AdaptiveAvgPool2d((1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d90b0895-5f15-4d63-9f47-0f44bef12942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 1]),\n",
       " tensor([[[[0.9418]],\n",
       " \n",
       "          [[0.9418]],\n",
       " \n",
       "          [[0.9418]],\n",
       " \n",
       "          [[0.9418]],\n",
       " \n",
       "          [[0.9418]],\n",
       " \n",
       "          [[0.9418]]],\n",
       " \n",
       " \n",
       "         [[[0.5732]],\n",
       " \n",
       "          [[0.5732]],\n",
       " \n",
       "          [[0.5732]],\n",
       " \n",
       "          [[0.5732]],\n",
       " \n",
       "          [[0.5732]],\n",
       " \n",
       "          [[0.5732]]]], grad_fn=<MeanBackward1>))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = avgPool(x)\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d232b03-eead-4665-b17c-0912c5d47228",
   "metadata": {},
   "source": [
    "#### Output Layer - Remove Dimension\n",
    "\n",
    "Recall that during our pre-convolution steps we took our input embedding `[B, T, C]` and converted it to `[B, C, H, T]`.  Now that we're running our output we need to undo this dimension rearrangement so that we can properly project our logits as `[B,T,vocab_size]`. \n",
    "\n",
    "To do this we'll first remove the height dimension and then flip our channels back. \n",
    "\n",
    "**Remove our third dimension**\n",
    "\n",
    "First we'll remove our height dimension, currently our third size.  Since this dimension is still 1 we won't see any shifts in the data, simply a reduction in size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "43be7c21-86d8-4f7a-9293-f78e700c759f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1]),\n",
       " tensor([[[0.9418],\n",
       "          [0.9418],\n",
       "          [0.9418],\n",
       "          [0.9418],\n",
       "          [0.9418],\n",
       "          [0.9418]],\n",
       " \n",
       "         [[0.5732],\n",
       "          [0.5732],\n",
       "          [0.5732],\n",
       "          [0.5732],\n",
       "          [0.5732],\n",
       "          [0.5732]]], grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.squeeze(2)\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dec617-264f-4cae-b899-af0f05336899",
   "metadata": {},
   "source": [
    "**Permute Channel and Token**\n",
    "\n",
    "Now we need to shuffle our channel and token token dimensions back so that we have our output ready for the final linear projection. In this case we'll see that our columns become a single row by batch, matching our expectation to have a single example, projected over the input channels, for each batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a7967d7d-de6b-456e-8234-b96748df6b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 6]),\n",
       " tensor([[[0.9418, 0.9418, 0.9418, 0.9418, 0.9418, 0.9418]],\n",
       " \n",
       "         [[0.5732, 0.5732, 0.5732, 0.5732, 0.5732, 0.5732]]],\n",
       "        grad_fn=<PermuteBackward0>))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.permute(0,2,1)\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9259a8-2457-442f-9137-0f8e18aebd3a",
   "metadata": {},
   "source": [
    "#### Output layer - LM Head aka logits\n",
    "\n",
    "We can now project `x` onto the vocabulary resulting in a `[B,T,vocab_size]` final array `logits`. This output correlates with the probability of each output token given the input context.  The best way to read  this is:\n",
    "\n",
    "(dimension 0) we have 2 batches B, \n",
    "(dimension 1) each batch has an example. \n",
    "(dimension 2) for each example we see the probability of each token in our vocabulary\n",
    "\n",
    "\n",
    "**LM Head - Weight initialization**\n",
    "We'll first initialize our weights.  We'll again initialize the weights as a consistent value that can be read as each token has an equal probability based on the weight. This should rapidly change through our back propagation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bceb13c6-b1c0-4e2b-80c4-e118affbfaf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([15, 6]),\n",
       " Parameter containing:\n",
       " tensor([[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],\n",
       "         [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100]], requires_grad=True))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "torch.nn.init.constant_(lm_head.weight,0.01)\n",
    "lm_head.weight.size(), lm_head.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3544b98-ce88-4b1a-9043-bc1c324a67ac",
   "metadata": {},
   "source": [
    "**LM Head - Output Projection**\n",
    "\n",
    "Now we're ready for our final projection. Since our `x` at this point actually has different values but the LM_Head weights are equal, we do expect that our final output has different values for each example, but the value for each logit in that example is the same.  This is best interpreted as the model having equal probability across all logits, or a random \"next token\", meaning it's shit. Luckily backpropagation has a way of updating this so that with enough data and time the probabilities change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "34c78a02-212c-423a-b956-19ffa41ca820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 15]),\n",
       " tensor([[[0.0565, 0.0565, 0.0565, 0.0565, 0.0565, 0.0565, 0.0565, 0.0565,\n",
       "           0.0565, 0.0565, 0.0565, 0.0565, 0.0565, 0.0565, 0.0565]],\n",
       " \n",
       "         [[0.0344, 0.0344, 0.0344, 0.0344, 0.0344, 0.0344, 0.0344, 0.0344,\n",
       "           0.0344, 0.0344, 0.0344, 0.0344, 0.0344, 0.0344, 0.0344]]],\n",
       "        grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = lm_head(x)\n",
    "\n",
    "logits.shape, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc4f9b1-fde1-48fc-9fa9-f22809343bc6",
   "metadata": {},
   "source": [
    "## Loss calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a430c6-92dd-469c-bc9e-608fd8b2aec7",
   "metadata": {},
   "source": [
    "Now we have to see how good our ~shit~ prediction is.  Since we haven't done training and we saw that regardless of example we had the same exact logit values across the vocab, we can expect it's bad, basically random. That said, we need to know how bad. For this example we'll use cross entropy, also known as the negative log likelihood of the softmax.  Our loss calculates\n",
    "\n",
    "$$\n",
    "\\ell_i=-\\log\\big(\\mathrm{softmax}(z_i)\\_{y_i}\\big)\n",
    "= -z_{i,y_i}+\\log\\!\\sum_{c=1}^C e^{z_{i,c}},\n",
    "$$\n",
    "\n",
    "\n",
    "To calculate loss we'll pass in the calculated `logits` and our next tokens stored in `y`. The cross entropy function does not respect batches so we'll flatten the `B` dimension for both `logits` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2476d669-b37e-4517-ac9a-095db4d9fae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2]), tensor([ 8, 10]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_flat = y.view(-1)\n",
    "y_flat.shape, y_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bff319dd-ae68-4660-84f1-9ee11e417eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 15]),\n",
       " tensor([[0.0565, 0.0565, 0.0565, 0.0565, 0.0565, 0.0565, 0.0565, 0.0565, 0.0565,\n",
       "          0.0565, 0.0565, 0.0565, 0.0565, 0.0565, 0.0565],\n",
       "         [0.0344, 0.0344, 0.0344, 0.0344, 0.0344, 0.0344, 0.0344, 0.0344, 0.0344,\n",
       "          0.0344, 0.0344, 0.0344, 0.0344, 0.0344, 0.0344]],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_flat = logits.view(-1, logits.size(-1))\n",
    "logits_flat.shape, logits_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b93007fd-b635-4066-9fb9-86d783f1abfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([]), tensor(2.7081, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.cross_entropy(logits_flat, y_flat)\n",
    "loss.shape, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ca3c18-8761-4d25-9c4d-3da7c50e85b6",
   "metadata": {},
   "source": [
    "## Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ad82cb-e927-45d1-8d74-c1d5793470c2",
   "metadata": {},
   "source": [
    "We now know just how ~well~ terribly the current model, with its weights and biases, predicts the next token given the input context. We now need to know how to change the different weights and biases to improve the formula.  We could do this by guessing through making minor changes and seeing what improves, or we can think through this more critically.\n",
    "\n",
    "If you review the chain of layers above, you can see that it's a series of formulas.  We can think of this as $f(g(x))$, except with many many more layers and complexities.  Since this is a formula, we can dig into our math toolbox and find a better way to determine what parts need to update.  Recall that in our calculus we learned that differentiation tells us the rate of change in a graph.  So if we treat the loss function $\\mathcal{L}$ as $\\mathcal{L}(f(g(x)))$ taking the partial differential \n",
    "\n",
    "$\\delta=\\partial \\mathcal{L}/\\partial h$\n",
    "\n",
    "at each layer will give us the impact of each weight/bias on our final out (albeit the inverse since our loss function is the negative log likelihood). \n",
    "\n",
    "Lucky for us, each layer of our model already has a placeholder for the partial differential called the **Gradient**. We'll use this field to store it.  We'll start by first zeroing out the gradients. We do this because of the nature of handling partial differentials for multiple dependencies. Recall that in multiple places we had a formula structure of \n",
    "\n",
    "$a+b=c ; a+c= d$\n",
    "\n",
    "In this case $a$ has 2 dependencies and determining the partial derivative of $\\partial d / \\partial a$ requires understanding both the path from $d$ and $c$.  To determine the true impact of a we would sum both partial derivatives together.  Because of this property, the tool we use, the built in `.backwards()` automatically sums gradients, `+=`, so if we do not set the gradient to `0` we then end up with erroneous gradients. \n",
    "\n",
    "Finally, we start `.backwards` from the `loss`, not `logits` as our goal is to minimize loss, we need to ensure we are looking at the calculations that impact loss which requires the whole forward pass to be able to generate the prediction `logits_flat`.  If we think of it as $\\mathcal{L}(f(x))$ where $f(x)$ is the forward pass to generate logits, then a simple chain rule is applied:\n",
    "\n",
    "${\\partial}/{\\partial x} =  \\mathcal{L}'(f(x)) f'(x)$\n",
    "\n",
    "Lets start by zeroing the gradients and leaning on pytorch to calculate the gradients for us. We'll also validate the gradients were `none`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dac7549-9e3b-4a8f-a6bf-963a630cddb7",
   "metadata": {},
   "source": [
    "### Back Propagation - Zero out gradients\n",
    "\n",
    "Before we start we have to zero our our gradients to make sure there's nothing in them.  Recall that when we run  `.backwards()` it automatically sums gradients if there are multiple paths so zeroing out ensure no erroneous measures.  \n",
    "\n",
    "Two things to notice: \n",
    "1. Our pooling step is not included.  While pooling does pass through gradients, it is stateless, meaning there are no learnable parameters, so the layer does not have a gradient buffers to clear. The step itself is differentiable though so gradients flow through it back to upstream parameters.\n",
    "2. Our convolutional layers use a special reset.  Since we manually built the layers using `nn.Parameter` they do not have the typical gradient and weight functions that layers have, but the layers are learnable.  Because of this they do have gradient buffers but no magic function to clear them so we have to clear them manually.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4ba144b8-b078-4a0a-9600-81ad8caf0293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_head.zero_grad()\n",
    "convRes.grad = None  # slightly different given it's manually built and not a torch layer. \n",
    "#conv block\n",
    "bn_b.zero_grad()\n",
    "conv2.weight = None\n",
    "bn_a.zero_grad()\n",
    "conv1.weight = None\n",
    "\n",
    "wte.zero_grad()\n",
    "\n",
    "\n",
    "# validate gradients\n",
    "lm_head.weight.grad,convRes.grad, wte.weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f3cc26-0e69-4b23-84d1-0bdd565d6a4b",
   "metadata": {},
   "source": [
    "### Back Propagation - Auto Diff\n",
    "\n",
    "Now let's see the magic of the gradients populate.  This magic is called auto-differentiation, or auto-diff for short. This allows us to not have to write many layers of nasty code to do the differentiation for us, but, if you're a sadist, you can surely find people who have written out that code (it's not too bad since you just do one layer at a time). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "47b05bb4-3760-4275-af44-cbf3da4684c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6fdc60-78ea-44fd-8ade-608f658fd1af",
   "metadata": {},
   "source": [
    "Now we will revisualize our gradients and see that they contain values, just like we wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "49171c35-8934-47f0-a788-cfad94457b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0505,  0.0505,  0.0505,  0.0505,  0.0505,  0.0505],\n",
       "         [ 0.0505,  0.0505,  0.0505,  0.0505,  0.0505,  0.0505],\n",
       "         [ 0.0505,  0.0505,  0.0505,  0.0505,  0.0505,  0.0505],\n",
       "         [ 0.0505,  0.0505,  0.0505,  0.0505,  0.0505,  0.0505],\n",
       "         [ 0.0505,  0.0505,  0.0505,  0.0505,  0.0505,  0.0505],\n",
       "         [ 0.0505,  0.0505,  0.0505,  0.0505,  0.0505,  0.0505],\n",
       "         [ 0.0505,  0.0505,  0.0505,  0.0505,  0.0505,  0.0505],\n",
       "         [ 0.0505,  0.0505,  0.0505,  0.0505,  0.0505,  0.0505],\n",
       "         [-0.4204, -0.4204, -0.4204, -0.4204, -0.4204, -0.4204],\n",
       "         [ 0.0505,  0.0505,  0.0505,  0.0505,  0.0505,  0.0505],\n",
       "         [-0.2361, -0.2361, -0.2361, -0.2361, -0.2361, -0.2361],\n",
       "         [ 0.0505,  0.0505,  0.0505,  0.0505,  0.0505,  0.0505],\n",
       "         [ 0.0505,  0.0505,  0.0505,  0.0505,  0.0505,  0.0505],\n",
       "         [ 0.0505,  0.0505,  0.0505,  0.0505,  0.0505,  0.0505],\n",
       "         [ 0.0505,  0.0505,  0.0505,  0.0505,  0.0505,  0.0505]]),\n",
       " tensor([[[[-1.2303e-11]],\n",
       " \n",
       "          [[-1.3360e-11]],\n",
       " \n",
       "          [[-1.4417e-11]],\n",
       " \n",
       "          [[-1.5475e-11]],\n",
       " \n",
       "          [[-1.6532e-11]],\n",
       " \n",
       "          [[-1.7589e-11]]],\n",
       " \n",
       " \n",
       "         [[[-1.2303e-11]],\n",
       " \n",
       "          [[-1.3360e-11]],\n",
       " \n",
       "          [[-1.4417e-11]],\n",
       " \n",
       "          [[-1.5475e-11]],\n",
       " \n",
       "          [[-1.6532e-11]],\n",
       " \n",
       "          [[-1.7589e-11]]],\n",
       " \n",
       " \n",
       "         [[[-1.2303e-11]],\n",
       " \n",
       "          [[-1.3360e-11]],\n",
       " \n",
       "          [[-1.4417e-11]],\n",
       " \n",
       "          [[-1.5475e-11]],\n",
       " \n",
       "          [[-1.6532e-11]],\n",
       " \n",
       "          [[-1.7589e-11]]],\n",
       " \n",
       " \n",
       "         [[[-1.2303e-11]],\n",
       " \n",
       "          [[-1.3360e-11]],\n",
       " \n",
       "          [[-1.4417e-11]],\n",
       " \n",
       "          [[-1.5475e-11]],\n",
       " \n",
       "          [[-1.6532e-11]],\n",
       " \n",
       "          [[-1.7589e-11]]],\n",
       " \n",
       " \n",
       "         [[[ 2.6776e-11]],\n",
       " \n",
       "          [[ 2.9104e-11]],\n",
       " \n",
       "          [[ 3.1432e-11]],\n",
       " \n",
       "          [[ 3.3760e-11]],\n",
       " \n",
       "          [[ 3.6089e-11]],\n",
       " \n",
       "          [[ 3.8417e-11]]],\n",
       " \n",
       " \n",
       "         [[[ 2.6776e-11]],\n",
       " \n",
       "          [[ 2.9104e-11]],\n",
       " \n",
       "          [[ 3.1432e-11]],\n",
       " \n",
       "          [[ 3.3760e-11]],\n",
       " \n",
       "          [[ 3.6089e-11]],\n",
       " \n",
       "          [[ 3.8417e-11]]]]),\n",
       " tensor([[ 4.8445e-19,  4.8445e-19,  4.8445e-19,  4.8445e-19,  4.8445e-19,\n",
       "           4.8445e-19],\n",
       "         [-7.5690e-20, -7.5690e-20, -7.5690e-20, -7.5690e-20, -7.5690e-20,\n",
       "          -7.5690e-20],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00],\n",
       "         [ 7.5046e-19,  7.5046e-19,  7.5046e-19,  7.5046e-19,  7.5046e-19,\n",
       "           7.5046e-19],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00],\n",
       "         [ 1.0710e-11,  1.0710e-11,  1.0710e-11,  1.0710e-11,  1.0710e-11,\n",
       "           1.0710e-11],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00],\n",
       "         [ 1.0710e-11,  1.0710e-11,  1.0710e-11,  1.0710e-11,  1.0710e-11,\n",
       "           1.0710e-11],\n",
       "         [ 5.3551e-12,  5.3551e-12,  5.3551e-12,  5.3551e-12,  5.3551e-12,\n",
       "           5.3551e-12],\n",
       "         [ 5.3551e-12,  5.3551e-12,  5.3551e-12,  5.3551e-12,  5.3551e-12,\n",
       "           5.3551e-12],\n",
       "         [ 2.1225e-18,  2.1225e-18,  2.1225e-18,  2.1225e-18,  2.1225e-18,\n",
       "           2.1225e-18],\n",
       "         [ 5.3551e-12,  5.3551e-12,  5.3551e-12,  5.3551e-12,  5.3551e-12,\n",
       "           5.3551e-12],\n",
       "         [-3.2730e-18, -3.2730e-18, -3.2730e-18, -3.2730e-18, -3.2730e-18,\n",
       "          -3.2730e-18],\n",
       "         [ 5.3551e-12,  5.3551e-12,  5.3551e-12,  5.3551e-12,  5.3551e-12,\n",
       "           5.3551e-12]]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_head.weight.grad,convRes.grad, wte.weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203067ba-ec52-4b19-b1a4-7f9c9bb960af",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d817699-a13c-4333-8fdd-d9e2148d6a58",
   "metadata": {},
   "source": [
    "The process of learning now requires us to update our weights based on this gradient. To really feel the \"back propagation\" we'll start with the last layer and work backwards, though, since we have all of the gradients calculated already, the order does not matter. Recall that our loss function is the negative log likelihood ratio so our gradient signs are flipped.  If a parameter is important, the gradient will be more negative, and vice versa. The gradients are a ratio of importance of each parameter and we need to know how much of that gradient to apply to our weights. This \"how much\" is referred to as the *learning rate*. In modern training learning rate schedulers and optimizers are used to vary the rate and application by layer and by training round with learning rates that are small (e.g. 1e-3) and decaying. \n",
    "\n",
    "We however are trying to learn and if you look at the gradient above in most layers it's tiny (~1e-10).  If we used a typical learning rate scheduler, with our context, batch size, and just 1 pass, the second pass would just have the same values and we wouldn't learn anything new.  Because of this we'll do something super unorthodox.  Our LM_head will use a learning rate of `1.000` but for the rest of the layers we'll use a learning rate of `1e8`. This INSANE learning rate would absolutely be too noisy for real training as taking that \"large\" of steps would mean the model could never find a good fit.  But since we're doing 1 step we don't care. Also, we're rerunning on the same example we used in the first loop so even less of a care. But as a warning, DO NOT DO THIS IN REAL TRAINING. If you did your model would most likely not converge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "76fb574c-69d6-49f7-8aec-a2599c71f6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Huge learning rate to emphasize\n",
    "head_learning_rate = 1.000\n",
    "learning_rate = 1e8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e81e5aa-953a-4a68-a795-d45c65cb3931",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Let's start with our output layer.  Recall that we initialized the weights to `0.0100` so we can quickly see the impact of the gradient update on the weights. Most notable that we'll see is that the entries corresponding with the token that are present in `y`, our target, are up weighted and others are downweighted. This should already intuitively give you a sense as to \n",
    "1. The model will be improved\n",
    "2. This is WAY too high of a learning rate if we wanted to generalize.\n",
    "\n",
    "Additionally you'll notice that the gradient is consistent across all of the `n_embd` dimensions given our light learning. Finally, since this is the output layer, we did not include any bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7893d3cc-4f81-4354-94ae-a4f7624a13a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0405, -0.0405, -0.0405, -0.0405, -0.0405, -0.0405],\n",
       "        [-0.0405, -0.0405, -0.0405, -0.0405, -0.0405, -0.0405],\n",
       "        [-0.0405, -0.0405, -0.0405, -0.0405, -0.0405, -0.0405],\n",
       "        [-0.0405, -0.0405, -0.0405, -0.0405, -0.0405, -0.0405],\n",
       "        [-0.0405, -0.0405, -0.0405, -0.0405, -0.0405, -0.0405],\n",
       "        [-0.0405, -0.0405, -0.0405, -0.0405, -0.0405, -0.0405],\n",
       "        [-0.0405, -0.0405, -0.0405, -0.0405, -0.0405, -0.0405],\n",
       "        [-0.0405, -0.0405, -0.0405, -0.0405, -0.0405, -0.0405],\n",
       "        [ 0.4304,  0.4304,  0.4304,  0.4304,  0.4304,  0.4304],\n",
       "        [-0.0405, -0.0405, -0.0405, -0.0405, -0.0405, -0.0405],\n",
       "        [ 0.2461,  0.2461,  0.2461,  0.2461,  0.2461,  0.2461],\n",
       "        [-0.0405, -0.0405, -0.0405, -0.0405, -0.0405, -0.0405],\n",
       "        [-0.0405, -0.0405, -0.0405, -0.0405, -0.0405, -0.0405],\n",
       "        [-0.0405, -0.0405, -0.0405, -0.0405, -0.0405, -0.0405],\n",
       "        [-0.0405, -0.0405, -0.0405, -0.0405, -0.0405, -0.0405]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    lm_head.weight -= head_learning_rate * lm_head.weight.grad\n",
    "lm_head.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9660c80e-feff-4404-8bb7-cceab2f9c9b7",
   "metadata": {},
   "source": [
    "### Residual Connection\n",
    "Next we'll update the convolutional layer inside of our residual connection.  Recall that we initialized the weights to `1.000` and did not include bias. Our gradients on this layer are extremely small (~1e-12) but some are positive and some negative across the channels.  This helps intuitively show that different channels impact our prediction.  \n",
    "\n",
    "With this layer you can start seeing the beauty of our insanely high learning rate.  Even though our gradient impact was tiny, we can see the weight shift away from the initialization suggesting some level of adaptation. Even with our high rate though, we only see the negative impact though, if we expanded the decimal level, we'd see some positive impact also "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6d9c8d80-ee4e-41e3-8ff7-67e722a99ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[1.0012]],\n",
       "\n",
       "         [[1.0013]],\n",
       "\n",
       "         [[1.0014]],\n",
       "\n",
       "         [[1.0015]],\n",
       "\n",
       "         [[1.0017]],\n",
       "\n",
       "         [[1.0018]]],\n",
       "\n",
       "\n",
       "        [[[1.0012]],\n",
       "\n",
       "         [[1.0013]],\n",
       "\n",
       "         [[1.0014]],\n",
       "\n",
       "         [[1.0015]],\n",
       "\n",
       "         [[1.0017]],\n",
       "\n",
       "         [[1.0018]]],\n",
       "\n",
       "\n",
       "        [[[1.0012]],\n",
       "\n",
       "         [[1.0013]],\n",
       "\n",
       "         [[1.0014]],\n",
       "\n",
       "         [[1.0015]],\n",
       "\n",
       "         [[1.0017]],\n",
       "\n",
       "         [[1.0018]]],\n",
       "\n",
       "\n",
       "        [[[1.0012]],\n",
       "\n",
       "         [[1.0013]],\n",
       "\n",
       "         [[1.0014]],\n",
       "\n",
       "         [[1.0015]],\n",
       "\n",
       "         [[1.0017]],\n",
       "\n",
       "         [[1.0018]]],\n",
       "\n",
       "\n",
       "        [[[0.9973]],\n",
       "\n",
       "         [[0.9971]],\n",
       "\n",
       "         [[0.9969]],\n",
       "\n",
       "         [[0.9966]],\n",
       "\n",
       "         [[0.9964]],\n",
       "\n",
       "         [[0.9962]]],\n",
       "\n",
       "\n",
       "        [[[0.9973]],\n",
       "\n",
       "         [[0.9971]],\n",
       "\n",
       "         [[0.9969]],\n",
       "\n",
       "         [[0.9966]],\n",
       "\n",
       "         [[0.9964]],\n",
       "\n",
       "         [[0.9962]]]], requires_grad=True)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    convRes -= learning_rate * convRes.grad\n",
    "convRes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff4b6c7-efa4-4547-b7cd-99d7c57aa85f",
   "metadata": {},
   "source": [
    "### Convolutional Block\n",
    "Next we'll update the layers in our convolutional block.  This includes our batch normalization layers and our two convolutional layers. Our ReLU layer is stateless so it does not have learnable parameters to update. Our batch normal layers include bias while our convolutional layers do not.\n",
    "\n",
    "Recall that our batch norm layers initiate with weights of `1` and bias of `0`.  For our convolution layers we initiated with iterations of `conv2 = [0.0020, 0.0010, 0.0010]` and `conv1 = [0.001,0.002,0.001]`.  We can see that the gradients, thanks again to our large learning rate, slightly shifted our weights away from their initial values. Interestingly, in conv2 within a patch we see that we only have started learning different weights on a single dimension and have not yet seen different shifts based on token position like we see in conv1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "03ba14c1-7d05-4822-be2b-5322e63dd15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([1.0012, 1.0012, 1.0012, 1.0012, 0.9976, 0.9976], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0106,  0.0106,  0.0106,  0.0106, -0.0233, -0.0233],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    bn_b.weight -= learning_rate * bn_b.weight.grad\n",
    "    bn_b.bias -= learning_rate * bn_b.bias.grad\n",
    "bn_b.weight, bn_b.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0f663c54-e271-4fc8-a1ed-9cb136a391c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.0337,  0.0401,  0.0426]],\n",
       "\n",
       "         [[ 0.0337,  0.0401,  0.0426]],\n",
       "\n",
       "         [[ 0.0337,  0.0401,  0.0426]],\n",
       "\n",
       "         [[ 0.0337,  0.0401,  0.0426]],\n",
       "\n",
       "         [[ 0.0337,  0.0401,  0.0426]],\n",
       "\n",
       "         [[ 0.0337,  0.0401,  0.0426]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0337,  0.0401,  0.0426]],\n",
       "\n",
       "         [[ 0.0337,  0.0401,  0.0426]],\n",
       "\n",
       "         [[ 0.0337,  0.0401,  0.0426]],\n",
       "\n",
       "         [[ 0.0337,  0.0401,  0.0426]],\n",
       "\n",
       "         [[ 0.0337,  0.0401,  0.0426]],\n",
       "\n",
       "         [[ 0.0337,  0.0401,  0.0426]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0337,  0.0401,  0.0426]],\n",
       "\n",
       "         [[ 0.0337,  0.0401,  0.0426]],\n",
       "\n",
       "         [[ 0.0337,  0.0401,  0.0426]],\n",
       "\n",
       "         [[ 0.0337,  0.0401,  0.0426]],\n",
       "\n",
       "         [[ 0.0337,  0.0401,  0.0426]],\n",
       "\n",
       "         [[ 0.0337,  0.0401,  0.0426]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0337,  0.0401,  0.0426]],\n",
       "\n",
       "         [[ 0.0337,  0.0401,  0.0426]],\n",
       "\n",
       "         [[ 0.0337,  0.0401,  0.0426]],\n",
       "\n",
       "         [[ 0.0337,  0.0401,  0.0426]],\n",
       "\n",
       "         [[ 0.0337,  0.0401,  0.0426]],\n",
       "\n",
       "         [[ 0.0337,  0.0401,  0.0426]]],\n",
       "\n",
       "\n",
       "        [[[-0.0615, -0.0772, -0.0823]],\n",
       "\n",
       "         [[-0.0615, -0.0772, -0.0823]],\n",
       "\n",
       "         [[-0.0615, -0.0772, -0.0823]],\n",
       "\n",
       "         [[-0.0615, -0.0772, -0.0823]],\n",
       "\n",
       "         [[-0.0615, -0.0772, -0.0823]],\n",
       "\n",
       "         [[-0.0615, -0.0772, -0.0823]]],\n",
       "\n",
       "\n",
       "        [[[-0.0615, -0.0772, -0.0823]],\n",
       "\n",
       "         [[-0.0615, -0.0772, -0.0823]],\n",
       "\n",
       "         [[-0.0615, -0.0772, -0.0823]],\n",
       "\n",
       "         [[-0.0615, -0.0772, -0.0823]],\n",
       "\n",
       "         [[-0.0615, -0.0772, -0.0823]],\n",
       "\n",
       "         [[-0.0615, -0.0772, -0.0823]]]], requires_grad=True)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    conv2 -= learning_rate * conv2.grad\n",
    "conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "df1269b2-7458-4d32-bb3d-6e1f1167170d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([6.5325e-11, 6.5325e-11, 6.5325e-11, 6.5325e-11, 6.5325e-11, 6.5325e-11],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    bn_a.weight -= learning_rate * bn_a.weight.grad\n",
    "    bn_a.bias -= learning_rate * bn_a.bias.grad\n",
    "bn_a.weight, bn_a.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a6ef9da5-33f5-4400-bb96-94a0113a5374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[0.0010, 0.0020, 0.0010]],\n",
       "\n",
       "         [[0.0010, 0.0020, 0.0010]],\n",
       "\n",
       "         [[0.0010, 0.0020, 0.0010]],\n",
       "\n",
       "         [[0.0010, 0.0020, 0.0010]],\n",
       "\n",
       "         [[0.0010, 0.0020, 0.0010]],\n",
       "\n",
       "         [[0.0010, 0.0020, 0.0010]]],\n",
       "\n",
       "\n",
       "        [[[0.0010, 0.0020, 0.0010]],\n",
       "\n",
       "         [[0.0010, 0.0020, 0.0010]],\n",
       "\n",
       "         [[0.0010, 0.0020, 0.0010]],\n",
       "\n",
       "         [[0.0010, 0.0020, 0.0010]],\n",
       "\n",
       "         [[0.0010, 0.0020, 0.0010]],\n",
       "\n",
       "         [[0.0010, 0.0020, 0.0010]]],\n",
       "\n",
       "\n",
       "        [[[0.0010, 0.0020, 0.0010]],\n",
       "\n",
       "         [[0.0010, 0.0020, 0.0010]],\n",
       "\n",
       "         [[0.0010, 0.0020, 0.0010]],\n",
       "\n",
       "         [[0.0010, 0.0020, 0.0010]],\n",
       "\n",
       "         [[0.0010, 0.0020, 0.0010]],\n",
       "\n",
       "         [[0.0010, 0.0020, 0.0010]]],\n",
       "\n",
       "\n",
       "        [[[0.0010, 0.0020, 0.0010]],\n",
       "\n",
       "         [[0.0010, 0.0020, 0.0010]],\n",
       "\n",
       "         [[0.0010, 0.0020, 0.0010]],\n",
       "\n",
       "         [[0.0010, 0.0020, 0.0010]],\n",
       "\n",
       "         [[0.0010, 0.0020, 0.0010]],\n",
       "\n",
       "         [[0.0010, 0.0020, 0.0010]]],\n",
       "\n",
       "\n",
       "        [[[0.0010, 0.0020, 0.0010]],\n",
       "\n",
       "         [[0.0010, 0.0020, 0.0010]],\n",
       "\n",
       "         [[0.0010, 0.0020, 0.0010]],\n",
       "\n",
       "         [[0.0010, 0.0020, 0.0010]],\n",
       "\n",
       "         [[0.0010, 0.0020, 0.0010]],\n",
       "\n",
       "         [[0.0010, 0.0020, 0.0010]]],\n",
       "\n",
       "\n",
       "        [[[0.0010, 0.0020, 0.0010]],\n",
       "\n",
       "         [[0.0010, 0.0020, 0.0010]],\n",
       "\n",
       "         [[0.0010, 0.0020, 0.0010]],\n",
       "\n",
       "         [[0.0010, 0.0020, 0.0010]],\n",
       "\n",
       "         [[0.0010, 0.0020, 0.0010]],\n",
       "\n",
       "         [[0.0010, 0.0020, 0.0010]]]], requires_grad=True)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    conv1 -= learning_rate * conv1.grad\n",
    "conv1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51eec87-ffb3-406e-99c6-7f540130b039",
   "metadata": {},
   "source": [
    "### Input Layer\n",
    "Finally we'll update our input layer. Recall that we initialized it to increments of `0.0100`. Given our high learning rate, a keen eye can observe an interesting phenomenon, gradient updates only impacted the tokens that were used in our example. This differs from our other layers because our embedding layer projects the example into the embedding space only uses vectors for tokens present so those are the only ones that the gradient can trace back to, versus on other layers because of dot products and other operations there's impact across the whole tensor.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c8363ddc-440a-425d-b8af-f0bd9a45449e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600],\n",
       "        [0.0200, 0.0300, 0.0400, 0.0500, 0.0600, 0.0700],\n",
       "        [0.0300, 0.0400, 0.0500, 0.0600, 0.0700, 0.0800],\n",
       "        [0.0400, 0.0500, 0.0600, 0.0700, 0.0800, 0.0900],\n",
       "        [0.0500, 0.0600, 0.0700, 0.0800, 0.0900, 0.1000],\n",
       "        [0.0600, 0.0700, 0.0800, 0.0900, 0.1000, 0.1100],\n",
       "        [0.0689, 0.0789, 0.0889, 0.0989, 0.1089, 0.1189],\n",
       "        [0.0800, 0.0900, 0.1000, 0.1100, 0.1200, 0.1300],\n",
       "        [0.0889, 0.0989, 0.1089, 0.1189, 0.1289, 0.1389],\n",
       "        [0.0995, 0.1095, 0.1195, 0.1295, 0.1395, 0.1495],\n",
       "        [0.1095, 0.1195, 0.1295, 0.1395, 0.1495, 0.1595],\n",
       "        [0.1200, 0.1300, 0.1400, 0.1500, 0.1600, 0.1700],\n",
       "        [0.1295, 0.1395, 0.1495, 0.1595, 0.1695, 0.1795],\n",
       "        [0.1400, 0.1500, 0.1600, 0.1700, 0.1800, 0.1900],\n",
       "        [0.1495, 0.1595, 0.1695, 0.1795, 0.1895, 0.1995]], requires_grad=True)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    wte.weight -= learning_rate * wte.weight.grad\n",
    "wte.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc95715-d8e8-4ae9-a46f-1a918dd74169",
   "metadata": {},
   "source": [
    "## Forward Pass with Updated Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2948df6e-afa8-4be2-abcc-5f4c88f03ccc",
   "metadata": {},
   "source": [
    "Now that we have the updated weights for each layer, let's do another forward pass and compare the loss. Since each layer was previously explained we will instead focus on just showing the outputs of the different layers and the final loss. If you want, you can check the previous outputs in the cached cell outputs above and compare them to see how the weight changes impacted the values at each layer. \n",
    "\n",
    "You'll notice that because our high learning rate we're able to see how each layer now shifts the embedding values as the input passes through them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d60652-ab5c-4efe-9d51-d78e419917c7",
   "metadata": {},
   "source": [
    "### Data Re-loading\n",
    "Repulling to a new `x_2`. We'll keep `y` to emphasize the same examples are being used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "eda5fdc7-5585-4e7f-8fd2-2370b7451ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[14, 13, 10,  1, 12,  0,  6, 11],\n",
       "         [ 8,  4,  9, 10,  6, 11,  8,  0]]),\n",
       " tensor([[ 8],\n",
       "         [10]]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2 = tok_for_training[:-1].view(B_batch, T_context)\n",
    "x_2, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff34638-049b-41c2-b5f6-01a0c34e44ba",
   "metadata": {},
   "source": [
    "### Input Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e536b787-40b7-4ebb-8fa6-38f90ed05ff8",
   "metadata": {},
   "source": [
    "Note that in `wte` since the gradient impact was only on the embeddings that mapped to our tokens. Lucky for us, we're passing the same example through so we'll pull those same weights.\n",
    "\n",
    "In a typical learning rate, with our vocab size and context length, along with the many layers we added to our CNN, we would need an extended training length to see changes in this initiation layer. Alternatively, a common step to help improve the input embedding is to do weight-tying between the input and output layers. This would be tricky in our case given the dimensions of the input and output layers do not align and is less common in CNNs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00b178a-39cd-4905-8d78-43e142f71eca",
   "metadata": {},
   "source": [
    "#### Input Layer - Embedding Projection\n",
    "\n",
    "We can quickly see that instead of our nice iterative layers we're getting weights that are adjusted.  We can see that we haven't yet learned much of a distinction across embedding layers yet as the increments between columns are consistent still. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "373b5725-63a4-4824-8912-7e0978d069bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 6]),\n",
       " tensor([[[0.1495, 0.1595, 0.1695, 0.1795, 0.1895, 0.1995],\n",
       "          [0.1400, 0.1500, 0.1600, 0.1700, 0.1800, 0.1900],\n",
       "          [0.1095, 0.1195, 0.1295, 0.1395, 0.1495, 0.1595],\n",
       "          [0.0200, 0.0300, 0.0400, 0.0500, 0.0600, 0.0700],\n",
       "          [0.1295, 0.1395, 0.1495, 0.1595, 0.1695, 0.1795],\n",
       "          [0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600],\n",
       "          [0.0689, 0.0789, 0.0889, 0.0989, 0.1089, 0.1189],\n",
       "          [0.1200, 0.1300, 0.1400, 0.1500, 0.1600, 0.1700]],\n",
       " \n",
       "         [[0.0889, 0.0989, 0.1089, 0.1189, 0.1289, 0.1389],\n",
       "          [0.0500, 0.0600, 0.0700, 0.0800, 0.0900, 0.1000],\n",
       "          [0.0995, 0.1095, 0.1195, 0.1295, 0.1395, 0.1495],\n",
       "          [0.1095, 0.1195, 0.1295, 0.1395, 0.1495, 0.1595],\n",
       "          [0.0689, 0.0789, 0.0889, 0.0989, 0.1089, 0.1189],\n",
       "          [0.1200, 0.1300, 0.1400, 0.1500, 0.1600, 0.1700],\n",
       "          [0.0889, 0.0989, 0.1089, 0.1189, 0.1289, 0.1389],\n",
       "          [0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600]]],\n",
       "        grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = wte(x_2)\n",
    "x.shape, x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f66382-c92d-4b18-89bb-4e10ae4f960f",
   "metadata": {},
   "source": [
    "#### Input Layer - Add Dimension\n",
    "\n",
    "We'll now do the permutation and add the height dimension into our matrix. This step does not change our values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fdfa6af8-0799-4b9d-9fce-b59d75fa7117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 8]),\n",
       " tensor([[[[0.1495, 0.1400, 0.1095, 0.0200, 0.1295, 0.0100, 0.0689, 0.1200]],\n",
       " \n",
       "          [[0.1595, 0.1500, 0.1195, 0.0300, 0.1395, 0.0200, 0.0789, 0.1300]],\n",
       " \n",
       "          [[0.1695, 0.1600, 0.1295, 0.0400, 0.1495, 0.0300, 0.0889, 0.1400]],\n",
       " \n",
       "          [[0.1795, 0.1700, 0.1395, 0.0500, 0.1595, 0.0400, 0.0989, 0.1500]],\n",
       " \n",
       "          [[0.1895, 0.1800, 0.1495, 0.0600, 0.1695, 0.0500, 0.1089, 0.1600]],\n",
       " \n",
       "          [[0.1995, 0.1900, 0.1595, 0.0700, 0.1795, 0.0600, 0.1189, 0.1700]]],\n",
       " \n",
       " \n",
       "         [[[0.0889, 0.0500, 0.0995, 0.1095, 0.0689, 0.1200, 0.0889, 0.0100]],\n",
       " \n",
       "          [[0.0989, 0.0600, 0.1095, 0.1195, 0.0789, 0.1300, 0.0989, 0.0200]],\n",
       " \n",
       "          [[0.1089, 0.0700, 0.1195, 0.1295, 0.0889, 0.1400, 0.1089, 0.0300]],\n",
       " \n",
       "          [[0.1189, 0.0800, 0.1295, 0.1395, 0.0989, 0.1500, 0.1189, 0.0400]],\n",
       " \n",
       "          [[0.1289, 0.0900, 0.1395, 0.1495, 0.1089, 0.1600, 0.1289, 0.0500]],\n",
       " \n",
       "          [[0.1389, 0.1000, 0.1495, 0.1595, 0.1189, 0.1700, 0.1389, 0.0600]]]],\n",
       "        grad_fn=<UnsqueezeBackward0>))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.permute(0,2,1) # [B,C,T]\n",
    "x = x.unsqueeze(2)  # [B,C,1,T]\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2b4f8d-2451-4a39-a2c5-45c6e04bba8f",
   "metadata": {},
   "source": [
    "### Convolution Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3078c933-c563-4a38-9f74-3e82d000b684",
   "metadata": {},
   "source": [
    "We'll go through the convolution layers again. Given the high learning rate we will now see how different weights impact our convolution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cfef8e-c925-4bd7-85e4-254b1ba2a1d9",
   "metadata": {},
   "source": [
    "#### Convolution Block - 1x3 Conv\n",
    "\n",
    "This time through we'll do all the steps sequentially and just print the output.  Even with the weight updates you can see that because the gradient was so small it did not impact our convolution output.  While our weights were updated, because our weights are still so repetitive, we can see that the values we have across the channel dimension remain consistent yet our token position differs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4e256b16-c6d2-4988-87f5-55342d3dea1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010,\n",
       "         0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010],\n",
       "        [0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010,\n",
       "         0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010],\n",
       "        [0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010,\n",
       "         0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010],\n",
       "        [0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010,\n",
       "         0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010],\n",
       "        [0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010,\n",
       "         0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010],\n",
       "        [0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010,\n",
       "         0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1_unfolded = F.unfold(x, \n",
    "\t\tkernel_size=(c1_kernel_height, c1_kernel_width),  # (1,3)\n",
    "\t\tpadding=(c1_padding_height, c1_padding_width), #(0,1)\n",
    "\t\tstride=(c1_stride_height, c1_stride_width))#(1,1)\n",
    "conv1_weigth = conv1.view(n_embd, -1) # [6,6,1,3] > [6,18]\n",
    "conv1_weigth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "039a15de-a76e-4c1a-b3f8-c136863f52cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 8]),\n",
       " tensor([[[[0.0031, 0.0038, 0.0029, 0.0023, 0.0023, 0.0019, 0.0022, 0.0023]],\n",
       " \n",
       "          [[0.0031, 0.0038, 0.0029, 0.0023, 0.0023, 0.0019, 0.0022, 0.0023]],\n",
       " \n",
       "          [[0.0031, 0.0038, 0.0029, 0.0023, 0.0023, 0.0019, 0.0022, 0.0023]],\n",
       " \n",
       "          [[0.0031, 0.0038, 0.0029, 0.0023, 0.0023, 0.0019, 0.0022, 0.0023]],\n",
       " \n",
       "          [[0.0031, 0.0038, 0.0029, 0.0023, 0.0023, 0.0019, 0.0022, 0.0023]],\n",
       " \n",
       "          [[0.0031, 0.0038, 0.0029, 0.0023, 0.0023, 0.0019, 0.0022, 0.0023]]],\n",
       " \n",
       " \n",
       "         [[[0.0018, 0.0023, 0.0028, 0.0029, 0.0028, 0.0030, 0.0024, 0.0011]],\n",
       " \n",
       "          [[0.0018, 0.0023, 0.0028, 0.0029, 0.0028, 0.0030, 0.0024, 0.0011]],\n",
       " \n",
       "          [[0.0018, 0.0023, 0.0028, 0.0029, 0.0028, 0.0030, 0.0024, 0.0011]],\n",
       " \n",
       "          [[0.0018, 0.0023, 0.0028, 0.0029, 0.0028, 0.0030, 0.0024, 0.0011]],\n",
       " \n",
       "          [[0.0018, 0.0023, 0.0028, 0.0029, 0.0028, 0.0030, 0.0024, 0.0011]],\n",
       " \n",
       "          [[0.0018, 0.0023, 0.0028, 0.0029, 0.0028, 0.0030, 0.0024, 0.0011]]]],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = conv1_weigth @ c1_unfolded\n",
    "out = out.view(batch,n_embd, c1_height_out, c1_width_out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020fba06-bb21-439d-87c2-0daef1c4d7d0",
   "metadata": {},
   "source": [
    "#### Convolution Block - First Batch Norm\n",
    "\n",
    "Both the weight and bias was updated in the batch normalization but the updates were consistent across the channels for normalization so we don't expect too much of a deviation from an initialized batch norm. Interestingly though, we can see that we have a set of channels that becomes negative before our ReLU step so we'll again have a set of channels get clipped during dilation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6990bfdc-9e68-4d37-97de-62cc22b2bb0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 8]),\n",
       " tensor([[[[ 0.1816,  0.4146,  0.1164, -0.0700, -0.0514, -0.1828, -0.0906,\n",
       "            -0.0607]],\n",
       " \n",
       "          [[ 0.1816,  0.4146,  0.1164, -0.0700, -0.0514, -0.1828, -0.0906,\n",
       "            -0.0607]],\n",
       " \n",
       "          [[ 0.1816,  0.4146,  0.1164, -0.0700, -0.0514, -0.1828, -0.0906,\n",
       "            -0.0607]],\n",
       " \n",
       "          [[ 0.1816,  0.4146,  0.1164, -0.0700, -0.0514, -0.1828, -0.0906,\n",
       "            -0.0607]],\n",
       " \n",
       "          [[ 0.1816,  0.4146,  0.1164, -0.0700, -0.0514, -0.1828, -0.0906,\n",
       "            -0.0607]],\n",
       " \n",
       "          [[ 0.1816,  0.4146,  0.1164, -0.0700, -0.0514, -0.1828, -0.0906,\n",
       "            -0.0607]]],\n",
       " \n",
       " \n",
       "         [[[-0.2118, -0.0524,  0.0781,  0.1320,  0.0947,  0.1517, -0.0161,\n",
       "            -0.4334]],\n",
       " \n",
       "          [[-0.2118, -0.0524,  0.0781,  0.1320,  0.0947,  0.1517, -0.0161,\n",
       "            -0.4334]],\n",
       " \n",
       "          [[-0.2118, -0.0524,  0.0781,  0.1320,  0.0947,  0.1517, -0.0161,\n",
       "            -0.4334]],\n",
       " \n",
       "          [[-0.2118, -0.0524,  0.0781,  0.1320,  0.0947,  0.1517, -0.0161,\n",
       "            -0.4334]],\n",
       " \n",
       "          [[-0.2118, -0.0524,  0.0781,  0.1320,  0.0947,  0.1517, -0.0161,\n",
       "            -0.4334]],\n",
       " \n",
       "          [[-0.2118, -0.0524,  0.0781,  0.1320,  0.0947,  0.1517, -0.0161,\n",
       "            -0.4334]]]], grad_fn=<NativeBatchNormBackward0>))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = bn_a(out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdf9799-718d-4382-8643-b6369392053c",
   "metadata": {},
   "source": [
    "#### Convolution Block - ReLU\n",
    "\n",
    "We can see a lot more exaggeration now within our Relu step.  We start pushing some values very high and still see a set of channels zero-out.  If we were training for a long time we'd need to keep an eye on this to make sure we don't end up with dead channels (this is also why different types of normalized initiation are used). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7f43e9bc-b037-4546-8b9f-869badfd69ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 8]),\n",
       " tensor([[[[0.1816, 0.4146, 0.1164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.1816, 0.4146, 0.1164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.1816, 0.4146, 0.1164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.1816, 0.4146, 0.1164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.1816, 0.4146, 0.1164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.1816, 0.4146, 0.1164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.0000, 0.0000, 0.0781, 0.1320, 0.0947, 0.1517, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0781, 0.1320, 0.0947, 0.1517, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0781, 0.1320, 0.0947, 0.1517, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0781, 0.1320, 0.0947, 0.1517, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0781, 0.1320, 0.0947, 0.1517, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0781, 0.1320, 0.0947, 0.1517, 0.0000, 0.0000]]]],\n",
       "        grad_fn=<ReluBackward0>))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = F.relu(out) \n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04703b62-e900-4e2e-b7e4-1ced9c0a563d",
   "metadata": {},
   "source": [
    "#### Convolution Block - 1x3 Conv 1x2 Stride downsample\n",
    "\n",
    "With our second convolution, you can see that we now have 2 channels that are no longer consistant.  While the last two channels are equal to each other, they differ from the first 4.  Because the channels and the token positions differ, we can expect a similar pattern to the weight in our convolution output where the first 4 channels have a consistent value per token but our last two channels have a different value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ab71c245-268b-4163-9a1e-368005141e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0337,  0.0401,  0.0426,  0.0337,  0.0401,  0.0426,  0.0337,  0.0401,\n",
       "          0.0426,  0.0337,  0.0401,  0.0426,  0.0337,  0.0401,  0.0426,  0.0337,\n",
       "          0.0401,  0.0426],\n",
       "        [ 0.0337,  0.0401,  0.0426,  0.0337,  0.0401,  0.0426,  0.0337,  0.0401,\n",
       "          0.0426,  0.0337,  0.0401,  0.0426,  0.0337,  0.0401,  0.0426,  0.0337,\n",
       "          0.0401,  0.0426],\n",
       "        [ 0.0337,  0.0401,  0.0426,  0.0337,  0.0401,  0.0426,  0.0337,  0.0401,\n",
       "          0.0426,  0.0337,  0.0401,  0.0426,  0.0337,  0.0401,  0.0426,  0.0337,\n",
       "          0.0401,  0.0426],\n",
       "        [ 0.0337,  0.0401,  0.0426,  0.0337,  0.0401,  0.0426,  0.0337,  0.0401,\n",
       "          0.0426,  0.0337,  0.0401,  0.0426,  0.0337,  0.0401,  0.0426,  0.0337,\n",
       "          0.0401,  0.0426],\n",
       "        [-0.0615, -0.0772, -0.0823, -0.0615, -0.0772, -0.0823, -0.0615, -0.0772,\n",
       "         -0.0823, -0.0615, -0.0772, -0.0823, -0.0615, -0.0772, -0.0823, -0.0615,\n",
       "         -0.0772, -0.0823],\n",
       "        [-0.0615, -0.0772, -0.0823, -0.0615, -0.0772, -0.0823, -0.0615, -0.0772,\n",
       "         -0.0823, -0.0615, -0.0772, -0.0823, -0.0615, -0.0772, -0.0823, -0.0615,\n",
       "         -0.0772, -0.0823]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2_unfolded = F.unfold(out, \n",
    "\t\tkernel_size=(c2_kernel_height, c2_kernel_width),  # (1,3)\n",
    "\t\tpadding=(c2_padding_height, c2_padding_width), #(0,1)\n",
    "\t\tstride=(c2_stride_height, c2_stride_width))#(1,2)\n",
    "\n",
    "conv2_weigth = conv2.view(n_embd, -1) # [6,6,1,3] > [6,18]\n",
    "conv2_weigth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2daf35c2-ad89-4976-abc9-c1f721cbdbbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 4]),\n",
       " tensor([[[[ 0.1497,  0.1119,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.1497,  0.1119,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.1497,  0.1119,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.1497,  0.1119,  0.0000,  0.0000]],\n",
       " \n",
       "          [[-0.2888, -0.2068,  0.0000,  0.0000]],\n",
       " \n",
       "          [[-0.2888, -0.2068,  0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0000,  0.0526,  0.0883,  0.0307]],\n",
       " \n",
       "          [[ 0.0000,  0.0526,  0.0883,  0.0307]],\n",
       " \n",
       "          [[ 0.0000,  0.0526,  0.0883,  0.0307]],\n",
       " \n",
       "          [[ 0.0000,  0.0526,  0.0883,  0.0307]],\n",
       " \n",
       "          [[ 0.0000, -0.1014, -0.1674, -0.0559]],\n",
       " \n",
       "          [[ 0.0000, -0.1014, -0.1674, -0.0559]]]], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = conv2_weigth @ c2_unfolded\n",
    "out = out.view(batch,n_embd, c2_height_out, c2_width_out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec67e6b-c865-40cb-9246-5b9fd0471847",
   "metadata": {},
   "source": [
    "#### Convolution Block - Second Batch Norm\n",
    "\n",
    "Our last batch normalization actually had a similar patter to our second convolution where we saw that it had values > 1.000 for the first 4 channels and values < 1.000 for the last two. These weights change how the normalization is done so that it's not consistently using all values for the mean.  That said we can see an interesting pattern start to emerge in our pass as to how the downsampled token position and the channel start to impact our final prediction. Notice though that despite these weights, we can still see a repeated pattern of consistency across the first 4 channels as well as the last two. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "530dd3b6-8698-4a25-b70e-4f4cf35b8a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([1.0012, 1.0012, 1.0012, 1.0012, 0.9976, 0.9976], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0106,  0.0106,  0.0106,  0.0106, -0.0233, -0.0233],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_b.weight, bn_b.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7ca8e40a-207d-45cd-9a3a-41dfbab7eb9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 4]),\n",
       " tensor([[[[ 1.7928,  1.0873, -0.9990, -0.9990]],\n",
       " \n",
       "          [[ 1.7928,  1.0873, -0.9990, -0.9990]],\n",
       " \n",
       "          [[ 1.7928,  1.0873, -0.9990, -0.9990]],\n",
       " \n",
       "          [[ 1.7928,  1.0873, -0.9990, -0.9990]],\n",
       " \n",
       "          [[-1.8412, -1.0411,  0.9776,  0.9776]],\n",
       " \n",
       "          [[-1.8412, -1.0411,  0.9776,  0.9776]]],\n",
       " \n",
       " \n",
       "         [[[-0.9990, -0.0190,  0.6474, -0.4269]],\n",
       " \n",
       "          [[-0.9990, -0.0190,  0.6474, -0.4269]],\n",
       " \n",
       "          [[-0.9990, -0.0190,  0.6474, -0.4269]],\n",
       " \n",
       "          [[-0.9990, -0.0190,  0.6474, -0.4269]],\n",
       " \n",
       "          [[ 0.9776, -0.0118, -0.6568,  0.4318]],\n",
       " \n",
       "          [[ 0.9776, -0.0118, -0.6568,  0.4318]]]],\n",
       "        grad_fn=<NativeBatchNormBackward0>))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = bn_b(out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d62d46-5728-456d-bcbd-cf34bd4b77f9",
   "metadata": {},
   "source": [
    "### Residual Connection\n",
    "Now that we have updated our convolution block, we have to travel our residual connection.  The residual connection includes the learnable downsampling 1x1 convolution.  Interestingly we saw that this layer learned weights across the channel showing an emergence of the model learning higher level patterns, not just token frequencies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0c9021-ab99-429b-88e3-15c4c1c5288a",
   "metadata": {},
   "source": [
    "#### Residual Connection - Downsampling 1x1 Convolution 2 Stride\n",
    "We'll first start with the downsampling convolution. We see that both channel dimensions on the weights are adjusted hinting at some interesting data patterns in our examples.  Notice though that despite these weights, we can still see a repeated pattern of consistency across the first 4 channels as well as the last two.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "08992843-b6fc-4343-96ad-22e07896f4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0012, 1.0013, 1.0014, 1.0015, 1.0017, 1.0018],\n",
       "        [1.0012, 1.0013, 1.0014, 1.0015, 1.0017, 1.0018],\n",
       "        [1.0012, 1.0013, 1.0014, 1.0015, 1.0017, 1.0018],\n",
       "        [1.0012, 1.0013, 1.0014, 1.0015, 1.0017, 1.0018],\n",
       "        [0.9973, 0.9971, 0.9969, 0.9966, 0.9964, 0.9962],\n",
       "        [0.9973, 0.9971, 0.9969, 0.9966, 0.9964, 0.9962]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_unfolded = F.unfold(x, \n",
    "\t\tkernel_size=(res_kernel_height, res_kernel_width),  # (1,1)\n",
    "\t\tpadding=(res_padding_height, res_padding_width), #(0,0)\n",
    "\t\tstride=(res_stride_height, res_stride_width))#(1,2)\n",
    "\n",
    "convRes_weigth = convRes.view(n_embd, -1) # [6,6,1,1] > [6,6]\n",
    "convRes_weigth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c9f1277b-79ef-4009-a431-f2d2003ae165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 4]),\n",
       " tensor([[[[1.0484, 0.8080, 0.9282, 0.5644]],\n",
       " \n",
       "          [[1.0484, 0.8080, 0.9282, 0.5644]],\n",
       " \n",
       "          [[1.0484, 0.8080, 0.9282, 0.5644]],\n",
       " \n",
       "          [[1.0484, 0.8080, 0.9282, 0.5644]],\n",
       " \n",
       "          [[1.0433, 0.8041, 0.9237, 0.5617]],\n",
       " \n",
       "          [[1.0433, 0.8041, 0.9237, 0.5617]]],\n",
       " \n",
       " \n",
       "         [[[0.6846, 0.7479, 0.5644, 0.6846]],\n",
       " \n",
       "          [[0.6846, 0.7479, 0.5644, 0.6846]],\n",
       " \n",
       "          [[0.6846, 0.7479, 0.5644, 0.6846]],\n",
       " \n",
       "          [[0.6846, 0.7479, 0.5644, 0.6846]],\n",
       " \n",
       "          [[0.6813, 0.7443, 0.5617, 0.6813]],\n",
       " \n",
       "          [[0.6813, 0.7443, 0.5617, 0.6813]]]], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identity = convRes_weigth @ x_unfolded\n",
    "identity = identity.view(batch,n_embd, res_height_out, res_width_out)\n",
    "identity.size(), identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee46d612-b354-4b7e-be25-589152a817ff",
   "metadata": {},
   "source": [
    "#### Residual Connection - Sum\n",
    "\n",
    "The residual connection output and our convolutional block outputs both had very different values and so when we combine them  we can even see certain entries becoming more muted, some flipping signs or becoming amplified in the direction one layer suggested. All together this shows the value of how the residual connections balances out the convolution layers and how the network can learn to use each for the type of outputs it's improving on.  Now since both the residual and convolution layers had the same patter: consistency across the first 4 channels as well as the last 2, we maintain that after the residual connection and convolution path merge.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5fe291b4-8a48-4271-b16b-e4d75a8c35fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 4]),\n",
       " tensor([[[[ 2.8411,  1.8953, -0.0708, -0.4346]],\n",
       " \n",
       "          [[ 2.8411,  1.8953, -0.0708, -0.4346]],\n",
       " \n",
       "          [[ 2.8411,  1.8953, -0.0708, -0.4346]],\n",
       " \n",
       "          [[ 2.8411,  1.8953, -0.0708, -0.4346]],\n",
       " \n",
       "          [[-0.7979, -0.2370,  1.9013,  1.5393]],\n",
       " \n",
       "          [[-0.7979, -0.2370,  1.9013,  1.5393]]],\n",
       " \n",
       " \n",
       "         [[[-0.3144,  0.7289,  1.2118,  0.2577]],\n",
       " \n",
       "          [[-0.3144,  0.7289,  1.2118,  0.2577]],\n",
       " \n",
       "          [[-0.3144,  0.7289,  1.2118,  0.2577]],\n",
       " \n",
       "          [[-0.3144,  0.7289,  1.2118,  0.2577]],\n",
       " \n",
       "          [[ 1.6589,  0.7325, -0.0951,  1.1131]],\n",
       " \n",
       "          [[ 1.6589,  0.7325, -0.0951,  1.1131]]]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = out + identity\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be08691f-e201-4d11-9239-3f020fe58b45",
   "metadata": {},
   "source": [
    "### Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda1922d-639f-491c-a8a8-d05bf39da369",
   "metadata": {},
   "source": [
    "The output layer saw the largest gradients and so we used a more normal, still large, learning rate. You'll notice that despite the inputs this layer will still dominate our logit prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021ee41e-4f30-4f1d-b336-4bb7858dddbd",
   "metadata": {},
   "source": [
    "#### Output Layers - Adaptive Average Pooling\n",
    "Before we do the final Logit calculation let's do our pooling.  You'll see that we have the same patter maintained across the channels of consistency on the first 4 and the last 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "155671a3-cec3-4948-845f-aed948acca12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 1]),\n",
       " tensor([[[[1.0578]],\n",
       " \n",
       "          [[1.0578]],\n",
       " \n",
       "          [[1.0578]],\n",
       " \n",
       "          [[1.0578]],\n",
       " \n",
       "          [[0.6015]],\n",
       " \n",
       "          [[0.6015]]],\n",
       " \n",
       " \n",
       "         [[[0.4710]],\n",
       " \n",
       "          [[0.4710]],\n",
       " \n",
       "          [[0.4710]],\n",
       " \n",
       "          [[0.4710]],\n",
       " \n",
       "          [[0.8524]],\n",
       " \n",
       "          [[0.8524]]]], grad_fn=<MeanBackward1>))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = avgPool(x)\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d207ed-491b-4864-b80c-313adbd37cec",
   "metadata": {},
   "source": [
    "#### Output Layers - Remove Dimension\n",
    "We'll again reshape without changing our values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d1011110-da91-4cfb-89af-45e28cb2d04d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 6]),\n",
       " tensor([[[1.0578, 1.0578, 1.0578, 1.0578, 0.6015, 0.6015]],\n",
       " \n",
       "         [[0.4710, 0.4710, 0.4710, 0.4710, 0.8524, 0.8524]]],\n",
       "        grad_fn=<PermuteBackward0>))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.squeeze(2)\n",
    "x = x.permute(0,2,1)\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5f3c99-2aea-4462-885a-e7344f82b0ba",
   "metadata": {},
   "source": [
    "#### Output Layers - LM Head aka logits\n",
    "Now we do our final projection against the LM head.  We know that our positions in Y are `[4,12]`.  We can see that the logit for the first example did very well and has by far the highest value for that position. At the same time though we can see that in the second example we did not do so well. A couple values got pulled negative to show they were not candidates but the remaining values are indistinguishable.  This might be because of our weird learning rates, but this is fine as we'd typically just run more training to fix this over time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9bf6ff86-22b5-4be1-b830-bee94e1f98a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 15]),\n",
       " tensor([[[-0.2201, -0.2201, -0.2201, -0.2201, -0.2201, -0.2201, -0.2201,\n",
       "           -0.2201,  2.3387, -0.2201,  1.3374, -0.2201, -0.2201, -0.2201,\n",
       "           -0.2201]],\n",
       " \n",
       "         [[-0.1453, -0.1453, -0.1453, -0.1453, -0.1453, -0.1453, -0.1453,\n",
       "           -0.1453,  1.5445, -0.1453,  0.8833, -0.1453, -0.1453, -0.1453,\n",
       "           -0.1453]]], grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = lm_head(x)\n",
    "logits.shape, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36457af-4bb9-4bb1-9b7e-5290c4a2185b",
   "metadata": {},
   "source": [
    "### Updated Loss calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75872203-00cd-44d8-a01d-2813d68cb843",
   "metadata": {},
   "source": [
    "Now we'll calculate the updated loss.  Our first pass's loss was 2.7081, on par with random. Since we're passing through the same example and used a fairly high learning rate we should see a significant improvement with just 1 learning pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d5bc7fbd-9284-4891-9f76-cb56fd55c94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.7081, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "873d5bfd-b8f2-40ae-9e59-5adf2e216ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([]) tensor(1.4453, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y_flat = y.view(-1)\n",
    "logits_flat = logits.view(-1, logits.size(-1))\n",
    "updated_loss = F.cross_entropy(logits_flat, y_flat)\n",
    "print(updated_loss.shape, updated_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "dece48ab-ebd4-4c64-bd62-c5f69786bb32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 round of training resulted in an loss improvment of 1.2628'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'1 round of training resulted in an loss improvment of {loss.item() - updated_loss.item():.4f}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd0d5b0-1d01-4328-b362-11c0a5a33b9e",
   "metadata": {},
   "source": [
    "## Training SUCCESS!\n",
    "Our training improved the loss by about **~41%** (amount may vary since we didn't set a seed). There are flaws with this, mainly passing the same example through a second time and a high learning rate, but this helps show the fundamentals of what learning does inside a ResNet style CNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db6ab87-87d9-4480-a19d-cde99ad27a42",
   "metadata": {},
   "source": [
    "## Logit to Token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530db314-a3a8-4e72-8760-5c59831dce80",
   "metadata": {},
   "source": [
    "For our last piece of code on this notebook, we'll actually now convert our logits into actual \"next tokens\", the goal of this head.  To predict the next tokens we convert:\n",
    "1. Convert our logits into probabilities\n",
    "2. Sample from the logits based on the probabilities\n",
    "3. Convert the token ids into tokens.\n",
    "\n",
    "As you can see the sampling is non-deterministics. This allows the next token to flow in a more realistic  conversational manner since, if we sample many tokens, over time we won't always pick the highest probability id. Some people read this as creativity, or consciousness, others point to this being one of a few key reasons for hallucinations (which are also caused by the fact that models don't fully memorize every piece of training data), but in reality it's just statistics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc904e4-278b-439e-b325-ff801fb0240d",
   "metadata": {},
   "source": [
    "### Logit and Input shaping\n",
    "Similar to other steps, we first have to get our tensors into the right dimensions. We'll start by compressing out our batch dimension so we just have a a tensor that shows our `[B*T,logits]`, or otherwise, an entry for each example ignoring our batch.  We'll also reinitialize our input tokens and do the same transformation so that we have a tensor ready to append to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2e0365f8-97a3-4e52-8bba-d90bc79a3c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 15]),\n",
       " tensor([[-0.2201, -0.2201, -0.2201, -0.2201, -0.2201, -0.2201, -0.2201, -0.2201,\n",
       "           2.3387, -0.2201,  1.3374, -0.2201, -0.2201, -0.2201, -0.2201],\n",
       "         [-0.1453, -0.1453, -0.1453, -0.1453, -0.1453, -0.1453, -0.1453, -0.1453,\n",
       "           1.5445, -0.1453,  0.8833, -0.1453, -0.1453, -0.1453, -0.1453]],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_logits = logits.flatten(0, 1) \n",
    "pred_logits.size(), pred_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bc434ada-0459-4206-87ec-6131d9ae8654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8]),\n",
       " tensor([[14, 13, 10,  1, 12,  0,  6, 11],\n",
       "         [ 8,  4,  9, 10,  6, 11,  8,  0]]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgen = tok_for_training[:-1].view(B_batch, T_context)\n",
    "xgen.size(), xgen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da03cf0-c2d1-4192-8433-a8f7eabdc526",
   "metadata": {},
   "source": [
    "### Logit into probabilities. \n",
    "If you inspect the logits, you'll notice that they don't sum up to `1.000`. Our goal is to understand the probability of each token in our vocab as being the next one.  Because of this we need to convert our weight to probabilities.  The most common approach is to use Softmax, which rescales a tensor so that the elements of the n-dimensional output Tensor lie in the range `(0,1)` and sum to 1. This is done applying the following formula based on the dimension specified:\n",
    "\n",
    "$$\n",
    "\\mathrm{Softmax}(x)_i = \\frac{\\exp(x_i)}{\\sum_{j} \\exp(x_j)}\n",
    "$$\n",
    "\n",
    "*Note that when the input Tensor is a sparse tensor then the unspecified values are treated as $-\\infty$. This is handy in steps like attention masking.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "45a95ec9-b9c0-47cc-ad39-50d9edc1269b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.0326, 0.4213,\n",
       "         0.0326, 0.1548, 0.0326, 0.0326, 0.0326, 0.0326],\n",
       "        [0.0471, 0.0471, 0.0471, 0.0471, 0.0471, 0.0471, 0.0471, 0.0471, 0.2554,\n",
       "         0.0471, 0.1318, 0.0471, 0.0471, 0.0471, 0.0471]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = F.softmax(pred_logits, dim=-1)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0fe889-5ba2-41d5-bcdd-dd1abf533f08",
   "metadata": {},
   "source": [
    "### Token Sampling \n",
    "Now that we have probabilities, we can now perform our sampling. Because next-token sampling is a categorical draw from the model’s softmax, `torch.multinomial` is the go-to choice for sampling from discrete distributions.  Multinomial takes non-negative weights (e.g., $\\mathrm{softmax}(\\ell/T)$ or $\\exp(\\ell/T))$ and returns indices without having to write complex loops. You can also tune its behavior with different temperatures and using top-k/top-p slicing. We'll keep things simple and just draw a single token. \n",
    "\n",
    "In cases where you'd want to generate a lot of text (e.g. chatbot use cases), instead of doing multi-sampling, you'd run the forward pass, sample, then using that sample, run another forward pass, iteratively. \n",
    "\n",
    "Once we sample the tokens we'll then append them to our input X `xgen` to generate `T_context+1` long examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6ce0de32-a161-4439-b861-8037eb22a3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10],\n",
       "        [11]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xcol = torch.multinomial(probs, 1) \n",
    "xcol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e87a5679-81bf-499d-8146-5b52fcf5cf65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14, 13, 10,  1, 12,  0,  6, 11, 10],\n",
       "        [ 8,  4,  9, 10,  6, 11,  8,  0, 11]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgen = torch.cat((xgen, xcol), dim=1)\n",
    "xgen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639643e2-b725-43f5-ab77-a78de64a9290",
   "metadata": {},
   "source": [
    "### Converting into tokens. \n",
    "Now that we have our tokens generated and appended we just have to convert them back into the original text using our tokenizer. We'll also print out our original text to see how close it gets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dd6d85-fc16-4dd6-9a26-ec3550d1fe9f",
   "metadata": {},
   "source": [
    "**Original Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ab4e133e-a821-49cc-a809-2e517f547097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>ccggaag,ffeed\n",
      "eddc,ggffeed,gg\n"
     ]
    }
   ],
   "source": [
    "#expected:\n",
    "print(tok.decode(tok_for_training[:9].tolist()))\n",
    "print(tok.decode(tok_for_training[8:].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b527ee-f159-4fdc-87d9-6532e385b0f5",
   "metadata": {},
   "source": [
    "**Predicted text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7c8e9183-ec6e-45f9-bec1-b2efaf90ae11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: <|endoftext|>ccggaag,ffegg\n",
      "batch 1: eddc,ggffeed,fe\n"
     ]
    }
   ],
   "source": [
    "for i in range(xgen.size()[0]):\n",
    "    tokens = xgen[i,:].tolist()\n",
    "    decoded = tok.decode(tokens)\n",
    "    print(f'batch {i}: {decoded}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266177bf-5e98-4d36-885f-ec25e9169ac1",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "So you might look at this now and say \"wait, the predicted text is different than our correct text, yet our loss decreased, what's going on?\".  To this I'd point to our loss function, it compares all logits, so as long as the probability for the right logit increases and the wrong logits decrease, the loss will decrease, yet, unless it's 0, it's not perfect.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422e0e32-6536-4014-b659-18b728775871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
