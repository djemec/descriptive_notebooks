{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbfe8b6e-ca81-4bf0-8cbc-e7503784f06a",
   "metadata": {},
   "source": [
    "# CNN ResNet Explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f393843-8889-4c95-9c49-759a8e264095",
   "metadata": {},
   "source": [
    "!!!!!!some info about resnets\n",
    "\n",
    "To help display how the CNNs works, we'll use the first sentence from the [linear algebra wiki page](https://en.wikipedia.org/wiki/Linear_algebra) and [lu decomposition wiki page](https://en.wikipedia.org/wiki/LU_decomposition) as the topic is fitting and it shows us some non-standard patterns.  We'll take on a more simple task to many other notebooks in this repository where we have a few sentences of text and want to predict some other text, in this case the next token. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5968de25-053a-45ce-9028-c03e90bdfa7c",
   "metadata": {},
   "source": [
    "## Text Prep/Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2685be2-e06d-4009-97b5-f26a3f61bac4",
   "metadata": {},
   "source": [
    "We'll start with a common preprocessing step of tokenizing the data.  This converts the string text into an array of numbers that can be used during the training loop.  I've built a very subtle byte-pair encoding that has each unique character that appears and the top 5 merges. This keeps our vocab size small and manageable for this example. Typically the vocab size is in the 100K+ range. A great library for this is `tiktoken`. Tokenization simply finds the longest pattern of characters that's in common with what was trained and replaces it with an integer that represents it.  This way we turn the text into a numeric array to simplify computing. import torch\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a6ac3c8-73c9-4b54-b6eb-233b5c405223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4919114e-83f0-4d2a-8a97-c4648da21f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBPETokenizer:\n",
    "    def __init__(self, num_merges=5, eot_token='<|endoftext|>'):\n",
    "        self.num_merges = num_merges\n",
    "        self.eot_token = eot_token\n",
    "        self.eot_id = None\n",
    "        self.merges = []\n",
    "        self.pair_ranks = {}\n",
    "        self.vocab = {}\n",
    "        self.id_to_token = {}\n",
    "\n",
    "    def _add_token(self, tok):\n",
    "        if tok in self.vocab:\n",
    "            return self.vocab[tok]\n",
    "        i = len(self.vocab)\n",
    "        self.vocab[tok] = i\n",
    "        self.id_to_token[i] = tok\n",
    "        return i\n",
    "\n",
    "    def _get_bigrams(self, seq):\n",
    "        for i in range(len(seq) - 1):\n",
    "            yield (seq[i], seq[i + 1])\n",
    "\n",
    "    def _merge_once(self, seq, pair):\n",
    "        a, b = pair\n",
    "        out = []\n",
    "        i = 0\n",
    "        while i < len(seq):\n",
    "            if i < len(seq) - 1 and seq[i] == a and seq[i + 1] == b:\n",
    "                out.append(a + b)\n",
    "                i += 2\n",
    "            else:\n",
    "                out.append(seq[i])\n",
    "                i += 1\n",
    "        return out\n",
    "\n",
    "    def train(self, corpus):\n",
    "        # corpus: list[str]\n",
    "        text = ''.join(corpus).lower()\n",
    "        seq = list(text)\n",
    "        merges = []\n",
    "        for _ in range(self.num_merges):\n",
    "            counts = Counter(self._get_bigrams(seq))\n",
    "            if not counts: break\n",
    "            best_pair, _ = counts.most_common(1)[0]\n",
    "            merges.append(best_pair)\n",
    "            seq = self._merge_once(seq, best_pair)\n",
    "        self.merges = merges\n",
    "        self.pair_ranks = {p: i for i, p in enumerate(self.merges)}\n",
    "\n",
    "        self.vocab = {}\n",
    "        self.id_to_token = {}\n",
    "        for ch in sorted(set(text)):\n",
    "            self._add_token(ch)\n",
    "        for a, b in self.merges:\n",
    "            self._add_token(a + b)\n",
    "        self.eot_id = self._add_token(self.eot_token)\n",
    "\n",
    "    def encode(self, text, force_last_eot=True):\n",
    "        # treat literal eot marker as special; remove it from content\n",
    "        if self.eot_token in text:\n",
    "            text = text.replace(self.eot_token, '')\n",
    "        seq = list(text)\n",
    "\n",
    "        # make sure all seen base chars exist\n",
    "        for ch in set(seq):\n",
    "            if ch not in self.vocab:\n",
    "                self._add_token(ch)\n",
    "\n",
    "        # greedy BPE using learned pair ranks\n",
    "        if self.merges:\n",
    "            while True:\n",
    "                best_pair, best_rank = None, None\n",
    "                for p in self._get_bigrams(seq):\n",
    "                    r = self.pair_ranks.get(p)\n",
    "                    if r is not None and (best_rank is None or r < best_rank):\n",
    "                        best_pair, best_rank = p, r\n",
    "                if best_pair is None:\n",
    "                    break\n",
    "                seq = self._merge_once(seq, best_pair)\n",
    "\n",
    "        # ensure all tokens in seq exist in vocab (e.g., if new chars appeared)\n",
    "        for tok in seq:\n",
    "            if tok not in self.vocab:\n",
    "                self._add_token(tok)\n",
    "\n",
    "        ids = [self.vocab[tok] for tok in seq]\n",
    "\n",
    "        # FORCE: append EOT id if not already last\n",
    "        if force_last_eot:\n",
    "            if not ids or ids[-1] != self.eot_id:\n",
    "                ids.append(self.eot_id)\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # drop trailing EOT if present\n",
    "        if ids and self.eot_id is not None and ids[-1] == self.eot_id:\n",
    "            ids = ids[:-1]\n",
    "        toks = [self.id_to_token[i] for i in ids]\n",
    "        return ''.join(toks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a42b8a5b-552e-4e15-abae-3fac8c3602e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_example_1 = r'''Linear algebra is central to almost all areas of mathematics. For instance, linear algebra is fundamental in modern presentations of geometry, including for defining basic objects such as lines, planes and rotations. Also, functional analysis, a branch of mathematical analysis, may be viewed as the application of linear algebra to function spaces.'''\n",
    "raw_example_2 = r'''In numerical analysis and linear algebra, lower–upper (LU) decomposition or factorization factors a matrix as the product of a lower triangular matrix and an upper triangular matrix (see matrix multiplication and matrix decomposition).'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb02db67-40d1-4a88-95a1-c497e3bfdaad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 'a'), ('a', 't'), ('i', 'n'), (' ', 'm'), ('i', 'o')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = SimpleBPETokenizer(num_merges=5)\n",
    "tok.train([raw_example_1,raw_example_2])\n",
    "tok.merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94546825-52f3-4668-bc9c-80bed6d0a444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '(': 1,\n",
       " ')': 2,\n",
       " ',': 3,\n",
       " '.': 4,\n",
       " 'a': 5,\n",
       " 'b': 6,\n",
       " 'c': 7,\n",
       " 'd': 8,\n",
       " 'e': 9,\n",
       " 'f': 10,\n",
       " 'g': 11,\n",
       " 'h': 12,\n",
       " 'i': 13,\n",
       " 'j': 14,\n",
       " 'l': 15,\n",
       " 'm': 16,\n",
       " 'n': 17,\n",
       " 'o': 18,\n",
       " 'p': 19,\n",
       " 'r': 20,\n",
       " 's': 21,\n",
       " 't': 22,\n",
       " 'u': 23,\n",
       " 'v': 24,\n",
       " 'w': 25,\n",
       " 'x': 26,\n",
       " 'y': 27,\n",
       " 'z': 28,\n",
       " '–': 29,\n",
       " ' a': 30,\n",
       " 'at': 31,\n",
       " 'in': 32,\n",
       " ' m': 33,\n",
       " 'io': 34,\n",
       " '<|endoftext|>': 35}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a47bae6f-3a84-4677-8b9a-5851a27ccff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tok.vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "210d0bdb-2e94-4788-8feb-9a445dfaa12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35, 15, 32,  9,  5, 20, 30, 15, 11,  9,  6, 20,  5,  0, 13, 21,  0,  7,\n",
       "         9, 17, 22, 20,  5, 15,  0, 22, 18, 30, 15, 16, 18, 21, 22, 30, 15, 15,\n",
       "        30, 20,  9,  5, 21,  0, 18, 10, 33, 31, 12,  9, 16, 31, 13,  7, 21,  4,\n",
       "         0, 10, 18, 20,  0, 32, 21, 22,  5, 17,  7,  9,  3,  0, 15, 32,  9,  5,\n",
       "        20, 30, 15, 11,  9,  6, 20,  5,  0, 13, 21,  0, 10, 23, 17,  8,  5, 16,\n",
       "         9, 17, 22,  5, 15,  0, 32, 33, 18,  8,  9, 20, 17,  0, 19, 20,  9, 21,\n",
       "         9, 17, 22, 31, 34, 17, 21,  0, 18, 10,  0, 11,  9, 18, 16,  9, 22, 20,\n",
       "        27,  3,  0, 32,  7, 15, 23,  8, 32, 11,  0, 10, 18, 20,  0,  8,  9, 10,\n",
       "        32, 32, 11,  0,  6,  5, 21, 13,  7,  0, 18,  6, 14,  9,  7, 22, 21,  0,\n",
       "        21, 23,  7, 12, 30, 21,  0, 15, 32,  9, 21,  3,  0, 19, 15,  5, 17,  9,\n",
       "        21, 30, 17,  8,  0, 20, 18, 22, 31, 34, 17, 21,  4, 30, 15, 21, 18,  3,\n",
       "         0, 10, 23, 17,  7, 22, 34, 17,  5, 15, 30, 17,  5, 15, 27, 21, 13, 21,\n",
       "         3, 30,  0,  6, 20,  5, 17,  7, 12,  0, 18, 10, 33, 31, 12,  9, 16, 31,\n",
       "        13,  7,  5, 15, 30, 17,  5, 15, 27, 21, 13, 21,  3, 33,  5, 27,  0,  6,\n",
       "         9,  0, 24, 13,  9, 25,  9,  8, 30, 21,  0, 22, 12,  9, 30, 19, 19, 15,\n",
       "        13,  7, 31, 34, 17,  0, 18, 10,  0, 15, 32,  9,  5, 20, 30, 15, 11,  9,\n",
       "         6, 20,  5,  0, 22, 18,  0, 10, 23, 17,  7, 22, 34, 17,  0, 21, 19,  5,\n",
       "         7,  9, 21,  4, 35, 35, 32,  0, 17, 23, 16,  9, 20, 13,  7,  5, 15, 30,\n",
       "        17,  5, 15, 27, 21, 13, 21, 30, 17,  8,  0, 15, 32,  9,  5, 20, 30, 15,\n",
       "        11,  9,  6, 20,  5,  3,  0, 15, 18, 25,  9, 20, 29, 23, 19, 19,  9, 20,\n",
       "         0,  1, 15, 23,  2,  0,  8,  9,  7, 18, 16, 19, 18, 21, 13, 22, 34, 17,\n",
       "         0, 18, 20,  0, 10,  5,  7, 22, 18, 20, 13, 28, 31, 34, 17,  0, 10,  5,\n",
       "         7, 22, 18, 20, 21, 30, 33, 31, 20, 13, 26, 30, 21,  0, 22, 12,  9,  0,\n",
       "        19, 20, 18,  8, 23,  7, 22,  0, 18, 10, 30,  0, 15, 18, 25,  9, 20,  0,\n",
       "        22, 20, 13,  5, 17, 11, 23, 15,  5, 20, 33, 31, 20, 13, 26, 30, 17,  8,\n",
       "        30, 17,  0, 23, 19, 19,  9, 20,  0, 22, 20, 13,  5, 17, 11, 23, 15,  5,\n",
       "        20, 33, 31, 20, 13, 26,  0,  1, 21,  9,  9, 33, 31, 20, 13, 26, 33, 23,\n",
       "        15, 22, 13, 19, 15, 13,  7, 31, 34, 17, 30, 17,  8, 33, 31, 20, 13, 26,\n",
       "         0,  8,  9,  7, 18, 16, 19, 18, 21, 13, 22, 34, 17,  2,  4, 35])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eot = tok.eot_id\n",
    "tokens = []\n",
    "for example in [raw_example_1, raw_example_2]:\n",
    "    tokens.extend([eot])\n",
    "    tokens.extend(tok.encode(example.lower()))\n",
    "all_tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f45597-e81b-4dfb-a34a-fb210883041d",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51779778-ccaf-4baf-b009-03c11fec951e",
   "metadata": {},
   "source": [
    "A machine learning model forward pass now uses the tokenization information, runs several layers of linear algebra on it, and then \"predicts\" the next token. When it is noisy (like you will see in this example), this process results in gibberish.  The training process changes the noise to pattern during the \"backward pass\" as you'll see.    We'll show 3 steps that are focused on training:\n",
    "1. **Data Loading** `x, y = train_loader.next_batch()` - this step pulls from the raw data enough tokens to complete a forward and backward pass.  If the model is inference only, this step is replaced with taking in the inference input and preparing it similarly as the forward pass.\n",
    "2. **Forward Pass** `logits, loss = model(x, y)` - using the data and the model architecture to predict the next token. When training we also compare against the expected to get loss, but in infrerence, we use the logits to complete the inference task.\n",
    "3. **Back Propagation, aka Backward Pass & Training** `loss.backward(); optimizer.step()` - using differentials to understand what parameters most impact the forward pass' impact on its prediction, comparing that against what is actually right based on the data loading step, and then making very minor adjustments to the impactful parameters with the hope it improves future predictions.\n",
    "\n",
    "The we'll show a final **Forward Pass** with the updated weights we did in #3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aefcbe-b05e-4e9a-a82f-46089bad50a5",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8293b18-5bdf-4693-82b3-466c20d679e9",
   "metadata": {},
   "source": [
    "To start, we need to get enough data to run the forward and backward passes.  Since our total dataset is likely too big to hold all at once in real practice, we would read just enough file information into memory so that we can run the passes, leaving memory and compute to be used on the passes instead of static data holding. \n",
    "To start, we have to identify the batch size and the model context length to determine how much data we need.  Consequently, these dimensions also form 2 of the 3 dimensions in the initial matrix.\n",
    "- **Batch Size (B)** - This is the number of examples you'll train on in a single pass. \n",
    "- **Context Length (T)** - This is the max number of tokens that a model can use in a single pass to generate the next token. If an example is below this length, it can be padded.\n",
    "  \n",
    "*Ideally both B and T are multiples of 2 to work nicely with chip architecture. This is a common theme across the board*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "454fc40b-459c-457c-b5db-db0733549246",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_batch = 2 # Batch\n",
    "T_context = 8 # context length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62799c29-4030-4f06-af11-0192aa61b1e3",
   "metadata": {},
   "source": [
    "To start, we need to pull from our long raw_token list enough tokens for the forward pass. To be able to satisfy training `B` Batches `T` Context length, we need to pull out `B*T` tokens to slide the context window across the examples enough to satisfy the batch size.  Since the training will attempt to predict the last token given the previous tokens in context, we also need 1 more token at the end so that the last training example in the last batch can have the next token to validate against. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68c595f6-0538-4a0b-b666-996fdc30f1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35, 15, 32,  9,  5, 20, 30, 15, 11,  9,  6, 20,  5,  0, 13, 21,  0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_position = 0\n",
    "tok_for_training = all_tokens[current_position:current_position + B_batch*T_context +1 ]\n",
    "tok_for_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6978a23-ad08-4ad3-870c-079b05a26b64",
   "metadata": {},
   "source": [
    "Now that we have our initial tokens to train on, we now need to convert it to a matrix that's ready for training. In this step we'll need to create our batches and setup two different arrays: 1/ the input, `x`, tokens that will result in 2/ the output `y` tokens. To create each example in the batch, every `T` tokens will be placed into its own row. \n",
    "\n",
    "Recall that training takes in a string of tokens the length of the context and then predicts the next token. Recall that when we extracted `tok_for_training` we added 1 extra token so that we can evaluate the prediction for the last example. Because of this, the input, `x`, will be all of the tokens up to the second to last element `[:-1]`.  \n",
    "\n",
    "It might be natural to think the output `y` would then just be the last token.But this is actually wasting valuable training loops.  Yes, there is the example that fills the context `T`, but we also have enough tokens in `tok_for_training` where any context length of `n` where `n<T` can also be used for inference since we have the `n+1` token available.  You can think of the following example:\n",
    "\n",
    "sentence: `Hi I am learning`. This sentence contains the following \"next tokens\" that can be learned:\n",
    "1. x: Hi I am  | y: learning\n",
    "2. x: Hi I     | y: am\n",
    "3. x: Hi       | y: I\n",
    "\n",
    "Because we have this triangle to create, our `y` can be much larger.  We can start with the second token and, go all the way to the last element we added for the last example `[1:'`.   \n",
    "\n",
    "\n",
    "We will now put this together and do the following:\n",
    "1. Extract the input `x` and then split it into an example for each batch `B`\n",
    "2. Extract the output `y` and then split it into an example for each batch `B`\n",
    "\n",
    "*Note: View can take `-1` which allows the matrix to infer the dimension so we do not need to pass in `T`, but given how many matrices we'll work with we want to make sure we're controlling the dimensions or erroring out if they do not match our expectations.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8632328a-d0c0-4036-abea-601786682dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[35, 15, 32,  9,  5, 20, 30, 15],\n",
       "        [11,  9,  6, 20,  5,  0, 13, 21]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=tok_for_training[:-1].view(B_batch, T_context)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3440807c-eec6-4dfe-b671-31aaea855172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15, 32,  9,  5, 20, 30, 15, 11],\n",
       "        [ 9,  6, 20,  5,  0, 13, 21,  0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=tok_for_training[1:].view(B_batch, T_context)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811ab64c-4cae-4095-89b1-4534085323be",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b16235f-5aec-4268-baae-f36a2a377b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_batch, T_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6affd37f-a150-4bd1-b72d-1aff283d2624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 36)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_embd = 6 # level of embedding of input tokens\n",
    "n_embd, vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308803fa-ae04-4b10-88b7-78f756b36daa",
   "metadata": {},
   "source": [
    "**Embedding Projection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9697d1c-02bd-49de-8b78-66de9eef7ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500]], requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wte = nn.Embedding(vocab_size, n_embd)\n",
    "torch.nn.init.constant_(wte.weight, 0.250)\n",
    "wte.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f4a3355-fa6b-4396-8643-ae8602bef769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 6]),\n",
       " tensor([[[0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500]],\n",
       " \n",
       "         [[0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500]]],\n",
       "        grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = wte(x)\n",
    "x.shape, x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af748fae-1f4b-4cbe-a361-e3719dcf1908",
   "metadata": {},
   "source": [
    "**We'll use our channels to be our multi-dimension for convolution, so reshape.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35617286-087d-4897-bbeb-69e6dd5ebde6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 8]),\n",
       " tensor([[[[0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500]],\n",
       " \n",
       "          [[0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500]],\n",
       " \n",
       "          [[0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500]],\n",
       " \n",
       "          [[0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500]],\n",
       " \n",
       "          [[0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500]],\n",
       " \n",
       "          [[0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500]]],\n",
       " \n",
       " \n",
       "         [[[0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500]],\n",
       " \n",
       "          [[0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500]],\n",
       " \n",
       "          [[0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500]],\n",
       " \n",
       "          [[0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500]],\n",
       " \n",
       "          [[0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500]],\n",
       " \n",
       "          [[0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500]]]],\n",
       "        grad_fn=<UnsqueezeBackward0>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.permute(0,2,1) # [B,C,T]\n",
    "x = x.unsqueeze(2)  # [B,C,1,T]\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82c654c-a572-47d8-ae1a-db0c494193f5",
   "metadata": {},
   "source": [
    "### First convolution block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b330da1e-5ab2-4b57-add0-fdc246a11f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d55e1872-79ff-43b7-a2a0-081be545a525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 6)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channel = n_embd\n",
    "out_channel = n_embd\n",
    "in_channel, out_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3871f2b-0d2d-492a-a920-03f43e71d618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernal': (1, 3), 'stride': (1, 1), 'padding': (0, 1)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernal_height = 1\n",
    "kernal_width = 3\n",
    "stride_height = 1\n",
    "stride_width = 1\n",
    "padding_height = 0\n",
    "padding_width = 1\n",
    "{'kernal': (kernal_height, kernal_width),\n",
    " 'stride': (stride_height, stride_width),\n",
    " 'padding': (padding_height, padding_width)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cdd7cda-440a-48fc-9a46-26d1b12ad441",
   "metadata": {},
   "outputs": [],
   "source": [
    "## weight layer for convolution (similar to linear, just more explicit)\n",
    "conv_stride1 = nn.Parameter(\n",
    "    torch.empty(out_channel, in_channel, kernal_height, kernal_width), \n",
    "    requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ec5b799-3a2d-49bc-bb32-57d74b54c3c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 6, 1, 3]),\n",
       " Parameter containing:\n",
       " tensor([[[[0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "          [[0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "          [[0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "          [[0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "          [[0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "          [[0.1000, 0.2000, 0.3000]]],\n",
       " \n",
       " \n",
       "         [[[0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "          [[0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "          [[0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "          [[0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "          [[0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "          [[0.1000, 0.2000, 0.3000]]],\n",
       " \n",
       " \n",
       "         [[[0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "          [[0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "          [[0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "          [[0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "          [[0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "          [[0.1000, 0.2000, 0.3000]]],\n",
       " \n",
       " \n",
       "         [[[0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "          [[0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "          [[0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "          [[0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "          [[0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "          [[0.1000, 0.2000, 0.3000]]],\n",
       " \n",
       " \n",
       "         [[[0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "          [[0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "          [[0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "          [[0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "          [[0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "          [[0.1000, 0.2000, 0.3000]]],\n",
       " \n",
       " \n",
       "         [[[0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "          [[0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "          [[0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "          [[0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "          [[0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "          [[0.1000, 0.2000, 0.3000]]]], requires_grad=True))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iniate rows as 0.1, 0.2, and 0.3 for easier view of the weight impact\n",
    "with torch.no_grad():\n",
    "    pattern = torch.tensor([0.1,0.2,0.3]).view(1,1,1,kernal_width).expand(conv_stride1.size()).clone()\n",
    "    conv_stride1.copy_(pattern)\n",
    "conv_stride1.size(), conv_stride1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449163be-7cf9-4313-878f-9af77d7e0838",
   "metadata": {},
   "source": [
    "**Run Convolution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a693e572-baa2-4306-9bac-0474bbed73b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 8]), 2, 6, 1, 8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch, channel, height, width = x.size()\n",
    "x.size(), batch, channel, height, width, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877f1715-4607-4da6-a531-06af3f4f02ae",
   "metadata": {},
   "source": [
    "This operation flattens each sliding kernel_size-sized block within the spatial dimensions of input into a column (i.e., last dimension) of a 3-D output tensor of shape (N,C×∏(kernel_size),L)(N,C×∏(kernel_size),L),\n",
    "\n",
    "unfold (aka “im2col”) takes a 4-D tensor ((N,C,H,W)) and extracts all sliding local blocks of size $(k_h,k_w)$ into columns. The result has shape $(N,C*k_h*k_w,L)$ where $(L = H_{\\text{out}} W_{\\text{out}})$ and\n",
    "$$\n",
    "H_{\\text{out}}=\\left\\lfloor\\frac{H+2p_h-d_h,(k_h-1)-1}{s_h}+1\\right\\rfloor,\\quad\n",
    "W_{\\text{out}}=\\left\\lfloor\\frac{W+2p_w-d_w,(k_w-1)-1}{s_w}+1\\right\\rfloor.\n",
    "$$\n",
    "\n",
    "Here (p) = padding, (s) = stride, (d) = dilation. Patches are flattened in channel-major, then row-major within each channel, and ordered left→right, top→bottom.\n",
    "\n",
    "Mental model: `unfold` linearizes all local receptive fields so you can do per-patch operations with a single batched matrix multiply. Convolution is exactly this with shared weights. High confidence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1602cf4e-82df-48da-a74a-6c60ef96583b",
   "metadata": {},
   "source": [
    "**Manual unfold to show**  We'll then use unfold, show they're equal, then use unfold going forward.  Unfold basically pads, then uses stride to extract a moving window on the last 2 dimensions to create a column.  \n",
    "\n",
    "recall we go from $(N,C,H,W)$ to $(N,C*k_h*k_w,L)$  let's calculate the dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97f70c1c-6a3d-4277-80ec-e32d07442d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width out 8, height out 1, final dimension (2,18,8)\n"
     ]
    }
   ],
   "source": [
    "c_khw = channel*kernal_height*kernal_width\n",
    "\n",
    "height_out = (height + 2*padding_height - 1*(kernal_height-1) - 1)//stride_height + 1   # = 1, \n",
    "width_out = (width + 2*padding_width - 1*(kernal_width-1) - 1)//stride_width + 1   # = 4\n",
    "L = height_out * width_out\n",
    "\n",
    "print(f'width out {width_out}, height out {height_out}, final dimension ({batch},{c_khw},{L})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e830999-85d7-46b7-bddc-bcbc1c646f85",
   "metadata": {},
   "source": [
    "**Padding** is (0,1), we pad on both sides so we get output of `[2, 6, 1+0, 8+2]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "068c1f6a-6e9f-4ea6-b028-b1d028f59992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 10]),\n",
       " tensor([[[[0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
       "            0.2500, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
       "            0.2500, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
       "            0.2500, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
       "            0.2500, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
       "            0.2500, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
       "            0.2500, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
       "            0.2500, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
       "            0.2500, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
       "            0.2500, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
       "            0.2500, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
       "            0.2500, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
       "            0.2500, 0.0000]]]], grad_fn=<ConstantPadNdBackward0>))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad last dim by (width, width) and 2nd to last by (height, height). width = 1, height = 0\n",
    "x_pad = F.pad(x, pad=(padding_width,padding_width,padding_height,padding_height))\n",
    "\n",
    "x_pad.size(), x_pad #total size and show first example in batch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17938995-a7ba-4140-af02-d1698afbebd3",
   "metadata": {},
   "source": [
    "## now show sliding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4172ea99-b2de-460f-8d8d-c91854d95746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first patch and cols\n",
      "patch:torch.Size([2, 6, 1, 3])\n",
      "tensor([[[[0.0000, 0.2500, 0.2500]],\n",
      "\n",
      "         [[0.0000, 0.2500, 0.2500]],\n",
      "\n",
      "         [[0.0000, 0.2500, 0.2500]],\n",
      "\n",
      "         [[0.0000, 0.2500, 0.2500]],\n",
      "\n",
      "         [[0.0000, 0.2500, 0.2500]],\n",
      "\n",
      "         [[0.0000, 0.2500, 0.2500]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.2500, 0.2500]],\n",
      "\n",
      "         [[0.0000, 0.2500, 0.2500]],\n",
      "\n",
      "         [[0.0000, 0.2500, 0.2500]],\n",
      "\n",
      "         [[0.0000, 0.2500, 0.2500]],\n",
      "\n",
      "         [[0.0000, 0.2500, 0.2500]],\n",
      "\n",
      "         [[0.0000, 0.2500, 0.2500]]]], grad_fn=<SliceBackward0>)\n",
      "col:torch.Size([2, 18])\n",
      "tensor([[0.0000, 0.2500, 0.2500, 0.0000, 0.2500, 0.2500, 0.0000, 0.2500, 0.2500,\n",
      "         0.0000, 0.2500, 0.2500, 0.0000, 0.2500, 0.2500, 0.0000, 0.2500, 0.2500],\n",
      "        [0.0000, 0.2500, 0.2500, 0.0000, 0.2500, 0.2500, 0.0000, 0.2500, 0.2500,\n",
      "         0.0000, 0.2500, 0.2500, 0.0000, 0.2500, 0.2500, 0.0000, 0.2500, 0.2500]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 18, 8]),\n",
       " tensor([[[0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
       "          [0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
       "          [0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
       "          [0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
       "          [0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
       "          [0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
       "          [0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
       "          [0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
       "          [0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
       "          [0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
       "          [0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.0000]]],\n",
       "        grad_fn=<StackBackward0>))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_cols = []\n",
    "for j in range(width_out):  # slide across width out: 8 \n",
    "    # Extract the kernal (1,3) window at width slice [j : j+3] (j max is 7 so 7+3 == 10, max width of X_pad\n",
    "    #         for all channels (H is 1 so don't need to slide on that channel\n",
    "    patch = x_pad[:, :, 0:kernal_height, j:j+kernal_width]        # (2,6,1,3)\n",
    "    \n",
    "    # stack the entries in each batch together into a row\n",
    "    col = patch.reshape(batch, c_khw) # shape to [2,18]\n",
    "\n",
    "    if j==0: \n",
    "        print('first patch and cols')\n",
    "        print(f'patch:{ patch.size()}')\n",
    "        print(patch) # print first patch \n",
    "        print(f'col:{ col.size()}')\n",
    "        print(col) # print first patch \n",
    "\n",
    "    manual_cols.append(col)\n",
    "\n",
    "# turn all the rows in the list into columns while maintaining the batch\n",
    "manual_unfold = torch.stack(manual_cols, dim=2)  # (N, 18, 8)\n",
    "manual_unfold.size(), manual_unfold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade75289-413e-4597-ab98-fa3c91d97333",
   "metadata": {},
   "source": [
    "**more efficint way** show that we have a function to do this and it's equal to show how we do it in the future, don't need padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fc5670d-b3ad-4f9f-ab1d-6d1609733662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 18, 8]),\n",
       " tensor([[[0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
       "          [0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
       "          [0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
       "          [0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
       "          [0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
       "          [0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
       "          [0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
       "          [0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
       "          [0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
       "          [0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.0000],\n",
       "          [0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.0000]]],\n",
       "        grad_fn=<Im2ColBackward0>))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfolded = F.unfold(x, \n",
    "\t\tkernel_size=(kernal_height, kernal_width),  # (1,3)\n",
    "\t\tpadding=(padding_height, padding_width), #(0,1)\n",
    "\t\tstride=(stride_height, stride_width))#(1,1)\n",
    "unfolded.size() , unfolded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0a9b12-18f7-484c-b120-74c854ea97f8",
   "metadata": {},
   "source": [
    "**Compare now**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbf0f2dd-85a2-40f1-a01b-2db27bacc778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manual equals unfold: True\n"
     ]
    }
   ],
   "source": [
    "print(\"manual equals unfold:\", torch.allclose(unfolded, manual_unfold))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec055fe-103a-4ea4-ad03-5095ab0e4a0a",
   "metadata": {},
   "source": [
    "**Dot Product of Weight by sliding window**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41e453ff-f282-44ed-a85b-b4990499fcfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 18]),\n",
       " tensor([[0.1000, 0.2000, 0.3000, 0.1000, 0.2000, 0.3000, 0.1000, 0.2000, 0.3000,\n",
       "          0.1000, 0.2000, 0.3000, 0.1000, 0.2000, 0.3000, 0.1000, 0.2000, 0.3000],\n",
       "         [0.1000, 0.2000, 0.3000, 0.1000, 0.2000, 0.3000, 0.1000, 0.2000, 0.3000,\n",
       "          0.1000, 0.2000, 0.3000, 0.1000, 0.2000, 0.3000, 0.1000, 0.2000, 0.3000],\n",
       "         [0.1000, 0.2000, 0.3000, 0.1000, 0.2000, 0.3000, 0.1000, 0.2000, 0.3000,\n",
       "          0.1000, 0.2000, 0.3000, 0.1000, 0.2000, 0.3000, 0.1000, 0.2000, 0.3000],\n",
       "         [0.1000, 0.2000, 0.3000, 0.1000, 0.2000, 0.3000, 0.1000, 0.2000, 0.3000,\n",
       "          0.1000, 0.2000, 0.3000, 0.1000, 0.2000, 0.3000, 0.1000, 0.2000, 0.3000],\n",
       "         [0.1000, 0.2000, 0.3000, 0.1000, 0.2000, 0.3000, 0.1000, 0.2000, 0.3000,\n",
       "          0.1000, 0.2000, 0.3000, 0.1000, 0.2000, 0.3000, 0.1000, 0.2000, 0.3000],\n",
       "         [0.1000, 0.2000, 0.3000, 0.1000, 0.2000, 0.3000, 0.1000, 0.2000, 0.3000,\n",
       "          0.1000, 0.2000, 0.3000, 0.1000, 0.2000, 0.3000, 0.1000, 0.2000, 0.3000]],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stacks creates a 2-d matrix of `out_channelX rest` so `6*18` by stacking the weights we match the shape of \n",
    "conv_1_weigth = conv_stride1.view(out_channel, -1) # [6,6,1,3] > [6,18]\n",
    "conv_1_weigth.size(), conv_1_weigth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5328b95a-6d03-4afb-b060-145c3a77c43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 8]),\n",
       " tensor([[[0.7500, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.4500],\n",
       "          [0.7500, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.4500],\n",
       "          [0.7500, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.4500],\n",
       "          [0.7500, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.4500],\n",
       "          [0.7500, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.4500],\n",
       "          [0.7500, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.4500]],\n",
       " \n",
       "         [[0.7500, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.4500],\n",
       "          [0.7500, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.4500],\n",
       "          [0.7500, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.4500],\n",
       "          [0.7500, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.4500],\n",
       "          [0.7500, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.4500],\n",
       "          [0.7500, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.4500]]],\n",
       "        grad_fn=<CloneBackward0>))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [6, 18] dot product with [2, 18, 8] resulting in [2x6x8]\n",
    "# This will auto broadcast across each of the 2 batches (shared weigth) so it results in [2x6x8]\n",
    "out = conv_1_weigth @ unfolded\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d782195-9352-4fdf-b26d-cdd71cdb145f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 8]),\n",
       " tensor([[[[0.7500, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.4500]],\n",
       " \n",
       "          [[0.7500, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.4500]],\n",
       " \n",
       "          [[0.7500, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.4500]],\n",
       " \n",
       "          [[0.7500, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.4500]],\n",
       " \n",
       "          [[0.7500, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.4500]],\n",
       " \n",
       "          [[0.7500, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.4500]]],\n",
       " \n",
       " \n",
       "         [[[0.7500, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.4500]],\n",
       " \n",
       "          [[0.7500, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.4500]],\n",
       " \n",
       "          [[0.7500, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.4500]],\n",
       " \n",
       "          [[0.7500, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.4500]],\n",
       " \n",
       "          [[0.7500, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.4500]],\n",
       " \n",
       "          [[0.7500, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.4500]]]],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert in the channel dimension to go back to [B,C,1,T] [2,6,1,8]\n",
    "out = out.view(batch,out_channel, height_out, width_out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1926f12-bb37-426e-985f-f3f0c502152e",
   "metadata": {},
   "source": [
    "### Batch Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20809e41-d391-4b13-abc4-f7b4bdf7b258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0.], requires_grad=True))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_a = nn.BatchNorm2d(n_embd)\n",
    "bn_a.weight, bn_a.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a29c64c5-7447-4c12-8594-a96be21188b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 8]),\n",
       " tensor([[[[-0.4999,  0.4999,  0.4999,  0.4999,  0.4999,  0.4999,  0.4999,\n",
       "            -2.4994]],\n",
       " \n",
       "          [[-0.4999,  0.4999,  0.4999,  0.4999,  0.4999,  0.4999,  0.4999,\n",
       "            -2.4994]],\n",
       " \n",
       "          [[-0.4999,  0.4999,  0.4999,  0.4999,  0.4999,  0.4999,  0.4999,\n",
       "            -2.4994]],\n",
       " \n",
       "          [[-0.4999,  0.4999,  0.4999,  0.4999,  0.4999,  0.4999,  0.4999,\n",
       "            -2.4994]],\n",
       " \n",
       "          [[-0.4999,  0.4999,  0.4999,  0.4999,  0.4999,  0.4999,  0.4999,\n",
       "            -2.4994]],\n",
       " \n",
       "          [[-0.4999,  0.4999,  0.4999,  0.4999,  0.4999,  0.4999,  0.4999,\n",
       "            -2.4994]]],\n",
       " \n",
       " \n",
       "         [[[-0.4999,  0.4999,  0.4999,  0.4999,  0.4999,  0.4999,  0.4999,\n",
       "            -2.4994]],\n",
       " \n",
       "          [[-0.4999,  0.4999,  0.4999,  0.4999,  0.4999,  0.4999,  0.4999,\n",
       "            -2.4994]],\n",
       " \n",
       "          [[-0.4999,  0.4999,  0.4999,  0.4999,  0.4999,  0.4999,  0.4999,\n",
       "            -2.4994]],\n",
       " \n",
       "          [[-0.4999,  0.4999,  0.4999,  0.4999,  0.4999,  0.4999,  0.4999,\n",
       "            -2.4994]],\n",
       " \n",
       "          [[-0.4999,  0.4999,  0.4999,  0.4999,  0.4999,  0.4999,  0.4999,\n",
       "            -2.4994]],\n",
       " \n",
       "          [[-0.4999,  0.4999,  0.4999,  0.4999,  0.4999,  0.4999,  0.4999,\n",
       "            -2.4994]]]], grad_fn=<NativeBatchNormBackward0>))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = bn_a(out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8d4c38-4d8b-40e7-9acc-30a49b4a5d93",
   "metadata": {},
   "source": [
    "### First RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3477c9ed-0cfa-42de-9903-47b602578df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 8]),\n",
       " tensor([[[[0.0000, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.0000, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.0000]]]],\n",
       "        grad_fn=<ReluBackward0>))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = F.relu(out) \n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2170f11-7281-4422-9398-7734d9602d46",
   "metadata": {},
   "source": [
    "### Second convolution that downsamples using a stride of 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26adb1ff-55ee-4d5b-bc34-4e112ad1a3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 6)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2_in_channel = n_embd\n",
    "c2_out_channel = n_embd\n",
    "c2_in_channel, out_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "203d10a8-2a14-4105-8115-0c9af2f8643b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernal': (1, 3), 'stride': (1, 2), 'padding': (0, 1)}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2_kernal_height = 1\n",
    "c2_kernal_width = 3\n",
    "c2_stride_height = 1\n",
    "c2_stride_width = 2\n",
    "c2_padding_height = 0\n",
    "c2_padding_width = 1\n",
    "{'kernal': (c2_kernal_height, c2_kernal_width),\n",
    " 'stride': (c2_stride_height, c2_stride_width),\n",
    " 'padding': (c2_padding_height, c2_padding_width)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9fef676c-3cbd-4c86-a53c-1dca20938567",
   "metadata": {},
   "outputs": [],
   "source": [
    "## weight layer for convolution (similar to linear, just more explicit)\n",
    "conv_stride2 = nn.Parameter(\n",
    "    torch.empty(c2_out_channel, c2_in_channel, c2_kernal_height, c2_kernal_width), \n",
    "    requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e2e8415-6f2a-444b-9ead-d332a7309590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 6, 1, 3]),\n",
       " Parameter containing:\n",
       " tensor([[[[0.3000, 0.2000, 0.1000]],\n",
       " \n",
       "          [[0.3000, 0.2000, 0.1000]],\n",
       " \n",
       "          [[0.3000, 0.2000, 0.1000]],\n",
       " \n",
       "          [[0.3000, 0.2000, 0.1000]],\n",
       " \n",
       "          [[0.3000, 0.2000, 0.1000]],\n",
       " \n",
       "          [[0.3000, 0.2000, 0.1000]]],\n",
       " \n",
       " \n",
       "         [[[0.3000, 0.2000, 0.1000]],\n",
       " \n",
       "          [[0.3000, 0.2000, 0.1000]],\n",
       " \n",
       "          [[0.3000, 0.2000, 0.1000]],\n",
       " \n",
       "          [[0.3000, 0.2000, 0.1000]],\n",
       " \n",
       "          [[0.3000, 0.2000, 0.1000]],\n",
       " \n",
       "          [[0.3000, 0.2000, 0.1000]]],\n",
       " \n",
       " \n",
       "         [[[0.3000, 0.2000, 0.1000]],\n",
       " \n",
       "          [[0.3000, 0.2000, 0.1000]],\n",
       " \n",
       "          [[0.3000, 0.2000, 0.1000]],\n",
       " \n",
       "          [[0.3000, 0.2000, 0.1000]],\n",
       " \n",
       "          [[0.3000, 0.2000, 0.1000]],\n",
       " \n",
       "          [[0.3000, 0.2000, 0.1000]]],\n",
       " \n",
       " \n",
       "         [[[0.3000, 0.2000, 0.1000]],\n",
       " \n",
       "          [[0.3000, 0.2000, 0.1000]],\n",
       " \n",
       "          [[0.3000, 0.2000, 0.1000]],\n",
       " \n",
       "          [[0.3000, 0.2000, 0.1000]],\n",
       " \n",
       "          [[0.3000, 0.2000, 0.1000]],\n",
       " \n",
       "          [[0.3000, 0.2000, 0.1000]]],\n",
       " \n",
       " \n",
       "         [[[0.3000, 0.2000, 0.1000]],\n",
       " \n",
       "          [[0.3000, 0.2000, 0.1000]],\n",
       " \n",
       "          [[0.3000, 0.2000, 0.1000]],\n",
       " \n",
       "          [[0.3000, 0.2000, 0.1000]],\n",
       " \n",
       "          [[0.3000, 0.2000, 0.1000]],\n",
       " \n",
       "          [[0.3000, 0.2000, 0.1000]]],\n",
       " \n",
       " \n",
       "         [[[0.3000, 0.2000, 0.1000]],\n",
       " \n",
       "          [[0.3000, 0.2000, 0.1000]],\n",
       " \n",
       "          [[0.3000, 0.2000, 0.1000]],\n",
       " \n",
       "          [[0.3000, 0.2000, 0.1000]],\n",
       " \n",
       "          [[0.3000, 0.2000, 0.1000]],\n",
       " \n",
       "          [[0.3000, 0.2000, 0.1000]]]], requires_grad=True))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iniate rows as 0.1, 0.2, and 0.3 for easier view of the weight impact\n",
    "with torch.no_grad():\n",
    "    c2_pattern = torch.tensor([0.3,0.2,0.1]).view(1,1,1,c2_kernal_width).expand(conv_stride2.size()).clone()\n",
    "    conv_stride2.copy_(c2_pattern)\n",
    "conv_stride2.size(), conv_stride2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef61c77d-3200-482f-8ca8-5e94a66d0099",
   "metadata": {},
   "source": [
    "### Second convolution, see that the dimensions change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2e6cb01c-bd16-443f-adcc-8a4549189412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 8]), 2, 6, 1, 8)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "c2_batch, c2_channel, c2_height, c2_width = out.size()\n",
    "out.size(), c2_batch, c2_channel, c2_height, c2_width, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b33e2e65-d7eb-47e8-a757-b6a181d7596f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Conv: width out 8, height out 1, final dimension (2,18,8)\n",
      "This  Conv: width out 4, height out 1, final dimension (2,18,4)\n"
     ]
    }
   ],
   "source": [
    "c2_c_khw = c2_channel*c2_kernal_height*c2_kernal_width\n",
    "\n",
    "c2_height_out = (c2_height + 2*c2_padding_height - 1*(c2_kernal_height-1) - 1)//c2_stride_height + 1   # = 1, \n",
    "c2_width_out = (c2_width + 2*c2_padding_width - 1*(c2_kernal_width-1) - 1)//c2_stride_width + 1   # = 4\n",
    "c2_L = c2_height_out * c2_width_out\n",
    "\n",
    "print(f'First Conv: width out {width_out}, height out {height_out}, final dimension ({batch},{c_khw},{L})')\n",
    "print(f'This  Conv: width out {c2_width_out}, height out {c2_height_out}, final dimension ({c2_batch},{c2_c_khw},{c2_L})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0772c91c-9af4-412f-9785-adc29ac3e96b",
   "metadata": {},
   "source": [
    "**notice** how the width is reduced in half,  this is cause the stride is 2, we'll have to deal with this when we do our residual step to match our input embedding with this output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f4bb4915-534b-46f0-8bc6-f290fe17da71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 3), (0, 1), (1, 2))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(c2_kernal_height, c2_kernal_width), (c2_padding_height, c2_padding_width), (c2_stride_height, c2_stride_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8d0c21f0-b930-4d43-ad0e-72ffbfa82925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0000, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.4999, 0.0000]]]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8c75c07f-688c-4405-abd1-977123fd3ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 18, 4]),\n",
       " tensor([[[0.0000, 0.4999, 0.4999, 0.4999],\n",
       "          [0.0000, 0.4999, 0.4999, 0.4999],\n",
       "          [0.4999, 0.4999, 0.4999, 0.0000],\n",
       "          [0.0000, 0.4999, 0.4999, 0.4999],\n",
       "          [0.0000, 0.4999, 0.4999, 0.4999],\n",
       "          [0.4999, 0.4999, 0.4999, 0.0000],\n",
       "          [0.0000, 0.4999, 0.4999, 0.4999],\n",
       "          [0.0000, 0.4999, 0.4999, 0.4999],\n",
       "          [0.4999, 0.4999, 0.4999, 0.0000],\n",
       "          [0.0000, 0.4999, 0.4999, 0.4999],\n",
       "          [0.0000, 0.4999, 0.4999, 0.4999],\n",
       "          [0.4999, 0.4999, 0.4999, 0.0000],\n",
       "          [0.0000, 0.4999, 0.4999, 0.4999],\n",
       "          [0.0000, 0.4999, 0.4999, 0.4999],\n",
       "          [0.4999, 0.4999, 0.4999, 0.0000],\n",
       "          [0.0000, 0.4999, 0.4999, 0.4999],\n",
       "          [0.0000, 0.4999, 0.4999, 0.4999],\n",
       "          [0.4999, 0.4999, 0.4999, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.4999, 0.4999, 0.4999],\n",
       "          [0.0000, 0.4999, 0.4999, 0.4999],\n",
       "          [0.4999, 0.4999, 0.4999, 0.0000],\n",
       "          [0.0000, 0.4999, 0.4999, 0.4999],\n",
       "          [0.0000, 0.4999, 0.4999, 0.4999],\n",
       "          [0.4999, 0.4999, 0.4999, 0.0000],\n",
       "          [0.0000, 0.4999, 0.4999, 0.4999],\n",
       "          [0.0000, 0.4999, 0.4999, 0.4999],\n",
       "          [0.4999, 0.4999, 0.4999, 0.0000],\n",
       "          [0.0000, 0.4999, 0.4999, 0.4999],\n",
       "          [0.0000, 0.4999, 0.4999, 0.4999],\n",
       "          [0.4999, 0.4999, 0.4999, 0.0000],\n",
       "          [0.0000, 0.4999, 0.4999, 0.4999],\n",
       "          [0.0000, 0.4999, 0.4999, 0.4999],\n",
       "          [0.4999, 0.4999, 0.4999, 0.0000],\n",
       "          [0.0000, 0.4999, 0.4999, 0.4999],\n",
       "          [0.0000, 0.4999, 0.4999, 0.4999],\n",
       "          [0.4999, 0.4999, 0.4999, 0.0000]]], grad_fn=<Im2ColBackward0>))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2_unfolded = F.unfold(out, \n",
    "\t\tkernel_size=(c2_kernal_height, c2_kernal_width),  # (1,3)\n",
    "\t\tpadding=(c2_padding_height, c2_padding_width), #(0,1)\n",
    "\t\tstride=(c2_stride_height, c2_stride_width))#(1,2)\n",
    "c2_unfolded.size() , c2_unfolded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fee877f-b612-4dcf-9c62-d7b78539414b",
   "metadata": {},
   "source": [
    "### Convolution dot product# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ba5c5c52-4640-484c-b72f-a3c366f1f359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 18]),\n",
       " tensor([[0.3000, 0.2000, 0.1000, 0.3000, 0.2000, 0.1000, 0.3000, 0.2000, 0.1000,\n",
       "          0.3000, 0.2000, 0.1000, 0.3000, 0.2000, 0.1000, 0.3000, 0.2000, 0.1000],\n",
       "         [0.3000, 0.2000, 0.1000, 0.3000, 0.2000, 0.1000, 0.3000, 0.2000, 0.1000,\n",
       "          0.3000, 0.2000, 0.1000, 0.3000, 0.2000, 0.1000, 0.3000, 0.2000, 0.1000],\n",
       "         [0.3000, 0.2000, 0.1000, 0.3000, 0.2000, 0.1000, 0.3000, 0.2000, 0.1000,\n",
       "          0.3000, 0.2000, 0.1000, 0.3000, 0.2000, 0.1000, 0.3000, 0.2000, 0.1000],\n",
       "         [0.3000, 0.2000, 0.1000, 0.3000, 0.2000, 0.1000, 0.3000, 0.2000, 0.1000,\n",
       "          0.3000, 0.2000, 0.1000, 0.3000, 0.2000, 0.1000, 0.3000, 0.2000, 0.1000],\n",
       "         [0.3000, 0.2000, 0.1000, 0.3000, 0.2000, 0.1000, 0.3000, 0.2000, 0.1000,\n",
       "          0.3000, 0.2000, 0.1000, 0.3000, 0.2000, 0.1000, 0.3000, 0.2000, 0.1000],\n",
       "         [0.3000, 0.2000, 0.1000, 0.3000, 0.2000, 0.1000, 0.3000, 0.2000, 0.1000,\n",
       "          0.3000, 0.2000, 0.1000, 0.3000, 0.2000, 0.1000, 0.3000, 0.2000, 0.1000]],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stacks creates a 2-d matrix of `out_channelX rest` so `6*18` by stacking the weights we match the shape of \n",
    "conv_2_weigth = conv_stride2.view(c2_out_channel, -1) # [6,6,1,3] > [6,18]\n",
    "conv_2_weigth.size(), conv_2_weigth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "44222855-5c5d-4d01-a9f9-a6cda20965bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 4]),\n",
       " tensor([[[0.2999, 1.7996, 1.7996, 1.4997],\n",
       "          [0.2999, 1.7996, 1.7996, 1.4997],\n",
       "          [0.2999, 1.7996, 1.7996, 1.4997],\n",
       "          [0.2999, 1.7996, 1.7996, 1.4997],\n",
       "          [0.2999, 1.7996, 1.7996, 1.4997],\n",
       "          [0.2999, 1.7996, 1.7996, 1.4997]],\n",
       " \n",
       "         [[0.2999, 1.7996, 1.7996, 1.4997],\n",
       "          [0.2999, 1.7996, 1.7996, 1.4997],\n",
       "          [0.2999, 1.7996, 1.7996, 1.4997],\n",
       "          [0.2999, 1.7996, 1.7996, 1.4997],\n",
       "          [0.2999, 1.7996, 1.7996, 1.4997],\n",
       "          [0.2999, 1.7996, 1.7996, 1.4997]]], grad_fn=<CloneBackward0>))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [6, 18] dot product with [2, 18, 4] resulting in [2x6x4]\n",
    "# This will auto broadcast across each of the 2 batches (shared weigth) so it results in [2x6x8]\n",
    "out = conv_2_weigth @ c2_unfolded\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "126da870-19af-41b7-aa8e-f0fcb0d2c635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 4]),\n",
       " tensor([[[[-1.6977,  0.7276,  0.7276,  0.2425]],\n",
       " \n",
       "          [[-1.6977,  0.7276,  0.7276,  0.2425]],\n",
       " \n",
       "          [[-1.6977,  0.7276,  0.7276,  0.2425]],\n",
       " \n",
       "          [[-1.6977,  0.7276,  0.7276,  0.2425]],\n",
       " \n",
       "          [[-1.6977,  0.7276,  0.7276,  0.2425]],\n",
       " \n",
       "          [[-1.6977,  0.7276,  0.7276,  0.2425]]],\n",
       " \n",
       " \n",
       "         [[[-1.6977,  0.7276,  0.7276,  0.2425]],\n",
       " \n",
       "          [[-1.6977,  0.7276,  0.7276,  0.2425]],\n",
       " \n",
       "          [[-1.6977,  0.7276,  0.7276,  0.2425]],\n",
       " \n",
       "          [[-1.6977,  0.7276,  0.7276,  0.2425]],\n",
       " \n",
       "          [[-1.6977,  0.7276,  0.7276,  0.2425]],\n",
       " \n",
       "          [[-1.6977,  0.7276,  0.7276,  0.2425]]]], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert in the channel dimension to go back to 1/2 of [B,C,1,T] since we took a stride of 2\n",
    "out = out.view(c2_batch,c2_out_channel, c2_height_out, c2_width_out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c911dd3-47b6-4534-a7e2-f0af6ea7f7d7",
   "metadata": {},
   "source": [
    "### Batch Norm #2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "85a762c9-fd66-44ed-83e2-50838ecdac6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0.], requires_grad=True))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_b = nn.BatchNorm2d(n_embd)   \n",
    "bn_b.weight, bn_b.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7ba14628-8817-48ed-87d2-2c580c995461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 4]),\n",
       " tensor([[[[-1.6977,  0.7276,  0.7276,  0.2425]],\n",
       " \n",
       "          [[-1.6977,  0.7276,  0.7276,  0.2425]],\n",
       " \n",
       "          [[-1.6977,  0.7276,  0.7276,  0.2425]],\n",
       " \n",
       "          [[-1.6977,  0.7276,  0.7276,  0.2425]],\n",
       " \n",
       "          [[-1.6977,  0.7276,  0.7276,  0.2425]],\n",
       " \n",
       "          [[-1.6977,  0.7276,  0.7276,  0.2425]]],\n",
       " \n",
       " \n",
       "         [[[-1.6977,  0.7276,  0.7276,  0.2425]],\n",
       " \n",
       "          [[-1.6977,  0.7276,  0.7276,  0.2425]],\n",
       " \n",
       "          [[-1.6977,  0.7276,  0.7276,  0.2425]],\n",
       " \n",
       "          [[-1.6977,  0.7276,  0.7276,  0.2425]],\n",
       " \n",
       "          [[-1.6977,  0.7276,  0.7276,  0.2425]],\n",
       " \n",
       "          [[-1.6977,  0.7276,  0.7276,  0.2425]]]],\n",
       "        grad_fn=<NativeBatchNormBackward0>))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = bn_b(out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84da273-76fb-498a-a127-30181a99f0d4",
   "metadata": {},
   "source": [
    "## Residual connection, \n",
    "bring in X, but need to convolute X to match Out dimension\n",
    "\n",
    "convolution with 1x1 kernal and a stride of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b908e0ed-fe54-45c8-848a-31860db746e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 6)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_in_channel = n_embd\n",
    "res_out_channel = n_embd\n",
    "res_in_channel, x_out_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fe985974-0c06-4f6a-998f-28bad44946d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernal': (1, 1), 'stride': (1, 2), 'padding': (0, 0)}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_kernal_height = 1\n",
    "res_kernal_width = 1\n",
    "res_stride_height = 1\n",
    "res_stride_width = 2\n",
    "res_padding_height = 0\n",
    "res_padding_width = 0\n",
    "{'kernal': (res_kernal_height, res_kernal_width),\n",
    " 'stride': (res_stride_height, res_stride_width),\n",
    " 'padding': (res_padding_height, res_padding_width)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dbbcf6b0-4bd5-4dd5-aa42-ea00864be1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## weight layer for convolution (similar to linear, just more explicit)\n",
    "res_conv_1x1 = nn.Parameter(\n",
    "    torch.empty(res_out_channel, res_in_channel, res_kernal_height, res_kernal_width), \n",
    "    requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "db6d368b-0930-425a-acb0-33a02bce26b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 6, 1, 1]),\n",
       " Parameter containing:\n",
       " tensor([[[[0.0500]],\n",
       " \n",
       "          [[0.0500]],\n",
       " \n",
       "          [[0.0500]],\n",
       " \n",
       "          [[0.0500]],\n",
       " \n",
       "          [[0.0500]],\n",
       " \n",
       "          [[0.0500]]],\n",
       " \n",
       " \n",
       "         [[[0.0500]],\n",
       " \n",
       "          [[0.0500]],\n",
       " \n",
       "          [[0.0500]],\n",
       " \n",
       "          [[0.0500]],\n",
       " \n",
       "          [[0.0500]],\n",
       " \n",
       "          [[0.0500]]],\n",
       " \n",
       " \n",
       "         [[[0.0500]],\n",
       " \n",
       "          [[0.0500]],\n",
       " \n",
       "          [[0.0500]],\n",
       " \n",
       "          [[0.0500]],\n",
       " \n",
       "          [[0.0500]],\n",
       " \n",
       "          [[0.0500]]],\n",
       " \n",
       " \n",
       "         [[[0.0500]],\n",
       " \n",
       "          [[0.0500]],\n",
       " \n",
       "          [[0.0500]],\n",
       " \n",
       "          [[0.0500]],\n",
       " \n",
       "          [[0.0500]],\n",
       " \n",
       "          [[0.0500]]],\n",
       " \n",
       " \n",
       "         [[[0.0500]],\n",
       " \n",
       "          [[0.0500]],\n",
       " \n",
       "          [[0.0500]],\n",
       " \n",
       "          [[0.0500]],\n",
       " \n",
       "          [[0.0500]],\n",
       " \n",
       "          [[0.0500]]],\n",
       " \n",
       " \n",
       "         [[[0.0500]],\n",
       " \n",
       "          [[0.0500]],\n",
       " \n",
       "          [[0.0500]],\n",
       " \n",
       "          [[0.0500]],\n",
       " \n",
       "          [[0.0500]],\n",
       " \n",
       "          [[0.0500]]]], requires_grad=True))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iniate rows as 0.1, 0.2, and 0.3 for easier view of the weight impact\n",
    "with torch.no_grad():\n",
    "    res_pattern = torch.tensor([0.05]).view(1,1,1,res_kernal_width).expand(res_conv_1x1.size()).clone()\n",
    "    res_conv_1x1.copy_(res_pattern)\n",
    "res_conv_1x1.size(), res_conv_1x1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab74a0cc-cf1e-4444-82ba-7d5be9a00e88",
   "metadata": {},
   "source": [
    "### Residual connection with 1x1 convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138e18ba-a867-4d6d-9298-01755cc62e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d3493684-8e7e-4b75-a56b-45dc6e8e2b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 6, 1, 8)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_batch, res_channel, res_height, res_width = x.size()\n",
    "res_batch, res_channel, res_height, res_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4a1a069d-3c1e-4dda-9c67-0208b00c341d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second  Conv: width out 4, height out 1, final dimension (2,18,4)\n",
      "This  Conv: width out 4, height out 1, final dimension (2,6,4)\n"
     ]
    }
   ],
   "source": [
    "res_c_khw = channel*res_kernal_height*res_kernal_width\n",
    "\n",
    "res_height_out = (height + 2*res_padding_height - 1*(res_kernal_height-1) - 1)//res_stride_height + 1   # = 1, \n",
    "res_width_out = (width + 2*res_padding_width - 1*(res_kernal_width-1) - 1)//res_stride_width + 1   # = 4\n",
    "res_L = res_height_out * res_width_out\n",
    "\n",
    "print(f'Second  Conv: width out {c2_width_out}, height out {c2_height_out}, final dimension ({c2_batch},{c2_c_khw},{c2_L})')\n",
    "print(f'This  Conv: width out {res_width_out}, height out {res_height_out}, final dimension ({batch},{res_c_khw},{res_L})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8438064-18c3-4af0-aae8-bbfdf293f118",
   "metadata": {},
   "source": [
    "**1/3 less but we'll have to add them together** we'll take care of this as we resize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "673dfef5-e401-4ab9-bf95-2bd5c6ecac34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 4]),\n",
       " tensor([[[0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500]],\n",
       " \n",
       "         [[0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "          [0.2500, 0.2500, 0.2500, 0.2500]]], grad_fn=<Im2ColBackward0>))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_unfolded = F.unfold(x, \n",
    "\t\tkernel_size=(res_kernal_height, res_kernal_width),  # (1,1)\n",
    "\t\tpadding=(res_padding_height, res_padding_width), #(0,0)\n",
    "\t\tstride=(res_stride_height, res_stride_width))#(1,2)\n",
    "x_unfolded.size() , x_unfolded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a604be43-82a3-40e2-a5bc-939af199fa2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 6]),\n",
       " tensor([[0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500],\n",
       "         [0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500],\n",
       "         [0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500],\n",
       "         [0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500],\n",
       "         [0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500],\n",
       "         [0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500]],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stacks creates a 2-d matrix of `out_channelX rest` so `6*18` by stacking the weights we match the shape of \n",
    "res_weigth = res_conv_1x1.view(res_out_channel, -1) # [6,6,1,1] > [6,6]\n",
    "res_weigth.size(), res_weigth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dca05ebd-aa90-445b-9110-45782c2fd100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 4]),\n",
       " tensor([[[0.0750, 0.0750, 0.0750, 0.0750],\n",
       "          [0.0750, 0.0750, 0.0750, 0.0750],\n",
       "          [0.0750, 0.0750, 0.0750, 0.0750],\n",
       "          [0.0750, 0.0750, 0.0750, 0.0750],\n",
       "          [0.0750, 0.0750, 0.0750, 0.0750],\n",
       "          [0.0750, 0.0750, 0.0750, 0.0750]],\n",
       " \n",
       "         [[0.0750, 0.0750, 0.0750, 0.0750],\n",
       "          [0.0750, 0.0750, 0.0750, 0.0750],\n",
       "          [0.0750, 0.0750, 0.0750, 0.0750],\n",
       "          [0.0750, 0.0750, 0.0750, 0.0750],\n",
       "          [0.0750, 0.0750, 0.0750, 0.0750],\n",
       "          [0.0750, 0.0750, 0.0750, 0.0750]]], grad_fn=<CloneBackward0>))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will auto broadcast across each of the 2 batches (shared weigth) so it results in [2x6x8]\n",
    "identity = res_weigth @ x_unfolded\n",
    "identity.size(), identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d70de44a-41be-4941-ac9d-21c8ff8316f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 4]),\n",
       " tensor([[[[0.0750, 0.0750, 0.0750, 0.0750]],\n",
       " \n",
       "          [[0.0750, 0.0750, 0.0750, 0.0750]],\n",
       " \n",
       "          [[0.0750, 0.0750, 0.0750, 0.0750]],\n",
       " \n",
       "          [[0.0750, 0.0750, 0.0750, 0.0750]],\n",
       " \n",
       "          [[0.0750, 0.0750, 0.0750, 0.0750]],\n",
       " \n",
       "          [[0.0750, 0.0750, 0.0750, 0.0750]]],\n",
       " \n",
       " \n",
       "         [[[0.0750, 0.0750, 0.0750, 0.0750]],\n",
       " \n",
       "          [[0.0750, 0.0750, 0.0750, 0.0750]],\n",
       " \n",
       "          [[0.0750, 0.0750, 0.0750, 0.0750]],\n",
       " \n",
       "          [[0.0750, 0.0750, 0.0750, 0.0750]],\n",
       " \n",
       "          [[0.0750, 0.0750, 0.0750, 0.0750]],\n",
       " \n",
       "          [[0.0750, 0.0750, 0.0750, 0.0750]]]], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert in the channel dimension to go back to [B,C,1,T/2] since we took a stride of 2\n",
    "identity = identity.view(res_batch,res_out_channel, res_height_out, res_width_out)\n",
    "identity.size(), identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c55f19-e328-40ab-a9b5-c833c66c53e4",
   "metadata": {},
   "source": [
    "## Residual connection sum\n",
    "see we now have the same size for our identity connection and our output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3b8325ea-c406-4937-8422-75111a581a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 4]), torch.Size([2, 6, 1, 4]))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size(), identity.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9b339043-c914-46fe-8cac-e9a7201329a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 4]),\n",
       " tensor([[[[-1.6227,  0.8026,  0.8026,  0.3175]],\n",
       " \n",
       "          [[-1.6227,  0.8026,  0.8026,  0.3175]],\n",
       " \n",
       "          [[-1.6227,  0.8026,  0.8026,  0.3175]],\n",
       " \n",
       "          [[-1.6227,  0.8026,  0.8026,  0.3175]],\n",
       " \n",
       "          [[-1.6227,  0.8026,  0.8026,  0.3175]],\n",
       " \n",
       "          [[-1.6227,  0.8026,  0.8026,  0.3175]]],\n",
       " \n",
       " \n",
       "         [[[-1.6227,  0.8026,  0.8026,  0.3175]],\n",
       " \n",
       "          [[-1.6227,  0.8026,  0.8026,  0.3175]],\n",
       " \n",
       "          [[-1.6227,  0.8026,  0.8026,  0.3175]],\n",
       " \n",
       "          [[-1.6227,  0.8026,  0.8026,  0.3175]],\n",
       " \n",
       "          [[-1.6227,  0.8026,  0.8026,  0.3175]],\n",
       " \n",
       "          [[-1.6227,  0.8026,  0.8026,  0.3175]]]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = out + identity\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a423132b-aacc-4fce-8690-6fa1173081a0",
   "metadata": {},
   "source": [
    "### Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4c540ba2-5790-40d6-b1bf-398916437c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 4]),\n",
       " tensor([[[[0.0000, 0.8026, 0.8026, 0.3175]],\n",
       " \n",
       "          [[0.0000, 0.8026, 0.8026, 0.3175]],\n",
       " \n",
       "          [[0.0000, 0.8026, 0.8026, 0.3175]],\n",
       " \n",
       "          [[0.0000, 0.8026, 0.8026, 0.3175]],\n",
       " \n",
       "          [[0.0000, 0.8026, 0.8026, 0.3175]],\n",
       " \n",
       "          [[0.0000, 0.8026, 0.8026, 0.3175]]],\n",
       " \n",
       " \n",
       "         [[[0.0000, 0.8026, 0.8026, 0.3175]],\n",
       " \n",
       "          [[0.0000, 0.8026, 0.8026, 0.3175]],\n",
       " \n",
       "          [[0.0000, 0.8026, 0.8026, 0.3175]],\n",
       " \n",
       "          [[0.0000, 0.8026, 0.8026, 0.3175]],\n",
       " \n",
       "          [[0.0000, 0.8026, 0.8026, 0.3175]],\n",
       " \n",
       "          [[0.0000, 0.8026, 0.8026, 0.3175]]]], grad_fn=<ReluBackward0>))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = F.relu(x)\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d232b03-eead-4665-b17c-0912c5d47228",
   "metadata": {},
   "source": [
    "### Remove our extra dimesion we added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "43be7c21-86d8-4f7a-9293-f78e700c759f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 4]),\n",
       " tensor([[[0.0000, 0.8026, 0.8026, 0.3175],\n",
       "          [0.0000, 0.8026, 0.8026, 0.3175],\n",
       "          [0.0000, 0.8026, 0.8026, 0.3175],\n",
       "          [0.0000, 0.8026, 0.8026, 0.3175],\n",
       "          [0.0000, 0.8026, 0.8026, 0.3175],\n",
       "          [0.0000, 0.8026, 0.8026, 0.3175]],\n",
       " \n",
       "         [[0.0000, 0.8026, 0.8026, 0.3175],\n",
       "          [0.0000, 0.8026, 0.8026, 0.3175],\n",
       "          [0.0000, 0.8026, 0.8026, 0.3175],\n",
       "          [0.0000, 0.8026, 0.8026, 0.3175],\n",
       "          [0.0000, 0.8026, 0.8026, 0.3175],\n",
       "          [0.0000, 0.8026, 0.8026, 0.3175]]], grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.squeeze(2)\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dec617-264f-4cae-b899-af0f05336899",
   "metadata": {},
   "source": [
    "### Flip our channel and context back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a7967d7d-de6b-456e-8234-b96748df6b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 6]),\n",
       " tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.8026, 0.8026, 0.8026, 0.8026, 0.8026, 0.8026],\n",
       "          [0.8026, 0.8026, 0.8026, 0.8026, 0.8026, 0.8026],\n",
       "          [0.3175, 0.3175, 0.3175, 0.3175, 0.3175, 0.3175]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.8026, 0.8026, 0.8026, 0.8026, 0.8026, 0.8026],\n",
       "          [0.8026, 0.8026, 0.8026, 0.8026, 0.8026, 0.8026],\n",
       "          [0.3175, 0.3175, 0.3175, 0.3175, 0.3175, 0.3175]]],\n",
       "        grad_fn=<PermuteBackward0>))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.permute(0,2,1)\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9259a8-2457-442f-9137-0f8e18aebd3a",
   "metadata": {},
   "source": [
    "## Final Linear Projectionself.head = nn.Linear(n_embd, vocab_size, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bceb13c6-b1c0-4e2b-80c4-e118affbfaf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([36, 6]),\n",
       " Parameter containing:\n",
       " tensor([[1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1.]], requires_grad=True))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "torch.nn.init.ones_(lm_head.weight)\n",
    "lm_head.weight.size(), lm_head.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3544b98-ce88-4b1a-9043-bc1c324a67ac",
   "metadata": {},
   "source": [
    "### logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "34c78a02-212c-423a-b956-19ffa41ca820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 36]),\n",
       " tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156,\n",
       "           4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156,\n",
       "           4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156,\n",
       "           4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156,\n",
       "           4.8156, 4.8156, 4.8156, 4.8156],\n",
       "          [4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156,\n",
       "           4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156,\n",
       "           4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156,\n",
       "           4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156,\n",
       "           4.8156, 4.8156, 4.8156, 4.8156],\n",
       "          [1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052,\n",
       "           1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052,\n",
       "           1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052,\n",
       "           1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052,\n",
       "           1.9052, 1.9052, 1.9052, 1.9052]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156,\n",
       "           4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156,\n",
       "           4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156,\n",
       "           4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156,\n",
       "           4.8156, 4.8156, 4.8156, 4.8156],\n",
       "          [4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156,\n",
       "           4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156,\n",
       "           4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156,\n",
       "           4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156,\n",
       "           4.8156, 4.8156, 4.8156, 4.8156],\n",
       "          [1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052,\n",
       "           1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052,\n",
       "           1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052,\n",
       "           1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052,\n",
       "           1.9052, 1.9052, 1.9052, 1.9052]]], grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = lm_head(x)\n",
    "\n",
    "logits.shape, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc4f9b1-fde1-48fc-9fa9-f22809343bc6",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3227e026-d682-4603-bd65-5a886433eca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, 36)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logits: [B, T_out, V], Y: [B, T]\n",
    "B, T_out, V = logits.shape\n",
    "B, T_out, V "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24a53b0-0413-449d-900d-3fd23a40d4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9d2e61ea-a730-43e6-b7c6-0c87316f707b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e6fcadc1-cd20-49a5-aac9-0f3d89d58249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[15, 32,  9,  5, 20, 30, 15, 11],\n",
       "         [ 9,  6, 20,  5,  0, 13, 21,  0]]),\n",
       " tensor([0, 2, 4, 6]),\n",
       " tensor([[15,  9, 20, 15],\n",
       "         [ 9, 20,  0, 21]]))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## adjust y for stride 2, aka every other \n",
    "s = T_context // T_out \n",
    "centers = torch.arange(T_out, device=logits.device) * s                 # [0, 2, 4, 6]\n",
    "y_aligned = y.gather(1, centers.expand(B, -1)) \n",
    "y, centers, y_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f5557f36-dafa-4ed0-8a98-98b1b6d9f343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8]), tensor([15,  9, 20, 15,  9, 20,  0, 21]))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_flat = y_aligned.reshape(B*T_out)\n",
    "y_flat.size(), y_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4d0f8edf-70f7-4aeb-98d5-9af0c19dc580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 36]),\n",
       " tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156,\n",
       "          4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156,\n",
       "          4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156,\n",
       "          4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156],\n",
       "         [4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156,\n",
       "          4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156,\n",
       "          4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156,\n",
       "          4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156],\n",
       "         [1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052,\n",
       "          1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052,\n",
       "          1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052,\n",
       "          1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156,\n",
       "          4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156,\n",
       "          4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156,\n",
       "          4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156],\n",
       "         [4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156,\n",
       "          4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156,\n",
       "          4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156,\n",
       "          4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156, 4.8156],\n",
       "         [1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052,\n",
       "          1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052,\n",
       "          1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052,\n",
       "          1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052, 1.9052]],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_flat = logits.reshape(B*T_out, V)\n",
    "logits_flat.shape, logits_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a43958e5-efd3-4312-b5e8-8c5a04720912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([]), tensor(3.5835, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.cross_entropy(logits_flat, y_flat)\n",
    "loss.shape, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ede886e-1b8c-4a21-8e13-488fd198e536",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
