{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbfe8b6e-ca81-4bf0-8cbc-e7503784f06a",
   "metadata": {},
   "source": [
    "# CNN ResNet Explainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f393843-8889-4c95-9c49-759a8e264095",
   "metadata": {},
   "source": [
    "Convolutional Neural Nets, or CNNs, learn the pattern in data by sliding small learnable filters across the sequence to spot local patterns—like short “n-gram” features—and turns them into higher-level signals. Because of this, CNNs are most commonly used for analyzing spatially structured data, like images or videos, because they can efficiently learn local patterns such as edges, textures, and shapes. They are also used in natural language processing and time-series tasks, where the same idea of sliding filters helps capture local dependencies in text or signal data. Modern architectures extend CNNs to higher-level tasks such as object detection, segmentation, and even audio or biological sequence modeling.\n",
    "\n",
    "For our example we will be taking text and using our embedding layer to add a second dimension to it for the CNN to learn across.  Recall that a discrete convolution of 2 matrices results in summation of a series of element-wise dot products. \n",
    "$$\n",
    "(a * b)_n = \\sum_{\\substack{i,j\\\\ i+j=n}} a_i \\cdot b_j\n",
    "$$\n",
    "\n",
    "In our example, the embedding of the input sequence is, $A$. We pad $A$ so the output has the same length as the input. The learnable kernel weights are (B). Each output value is the dot product between a local patch of $A$ and $B$ running our discrete cross-correlation. We also include the stride controls how far the kernel window moves along $A$ to show how we can downsample A. \n",
    "\n",
    "Because of this, we actually run a different calculation, similar to a convolution called the 2-D discrete cross-correlation. With the input reshaped to $[B,C,1,T]$ and a $1\\times k$ kernel, each output token index $t$ is\n",
    "$$\n",
    "y_{t}=\\sum_{c=1}^{C}\\sum_{u=0}^{k-1} W_{c,u} x_{c,,t+u}\\quad\n",
    "$$\n",
    "\n",
    "\n",
    "To help display how the CNNs works, we'll actually use the c-major note letters for 3 popular songs: [Hot Cross Buns](https://en.wikipedia.org/wiki/Hot_Cross_Buns_(song)), [Twinkle Twinkle Little Star](https://en.wikipedia.org/wiki/Twinkle,_Twinkle,_Little_Star), and [Happy Birtday To You](https://en.wikipedia.org/wiki/Happy_Birthday_to_You). \n",
    "\n",
    "In today's notebooks we'll take in 2 different examples and predict the next note from them. In other notebooks you might have seen that we predicted notes many examples in each batch during a loop. Since we are using our inputXembedding, we'll just have the single example in each batch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5968de25-053a-45ce-9028-c03e90bdfa7c",
   "metadata": {},
   "source": [
    "## Text Prep/Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2685be2-e06d-4009-97b5-f26a3f61bac4",
   "metadata": {},
   "source": [
    "We'll start with a common preprocessing step of tokenizing the data.  This converts the string text into an array of numbers that can be used during the training loop.  I've built a very subtle byte-pair encoding that has each unique character that appears and the top 5 merges. This keeps our vocab size small and manageable for this example. Typically the vocab size is in the 100K+ range. A great library for this is `tiktoken`. Tokenization simply finds the longest pattern of characters that's in common with what was trained and replaces it with an integer that represents it.  This way we turn the text into a numeric array to simplify computing. import torch\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a6ac3c8-73c9-4b54-b6eb-233b5c405223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4919114e-83f0-4d2a-8a97-c4648da21f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBPETokenizer:\n",
    "    def __init__(self, num_merges=5, eot_token='<|endoftext|>'):\n",
    "        self.num_merges = num_merges\n",
    "        self.eot_token = eot_token\n",
    "        self.eot_id = None\n",
    "        self.merges = []\n",
    "        self.pair_ranks = {}\n",
    "        self.vocab = {}\n",
    "        self.id_to_token = {}\n",
    "\n",
    "    def _add_token(self, tok):\n",
    "        if tok in self.vocab:\n",
    "            return self.vocab[tok]\n",
    "        i = len(self.vocab)\n",
    "        self.vocab[tok] = i\n",
    "        self.id_to_token[i] = tok\n",
    "        return i\n",
    "\n",
    "    def _get_bigrams(self, seq):\n",
    "        for i in range(len(seq) - 1):\n",
    "            yield (seq[i], seq[i + 1])\n",
    "\n",
    "    def _merge_once(self, seq, pair):\n",
    "        a, b = pair\n",
    "        out = []\n",
    "        i = 0\n",
    "        while i < len(seq):\n",
    "            if i < len(seq) - 1 and seq[i] == a and seq[i + 1] == b:\n",
    "                out.append(a + b)\n",
    "                i += 2\n",
    "            else:\n",
    "                out.append(seq[i])\n",
    "                i += 1\n",
    "        return out\n",
    "\n",
    "    def train(self, corpus):\n",
    "        # corpus: list[str]\n",
    "        text = ''.join(corpus).lower()\n",
    "        seq = list(text)\n",
    "        merges = []\n",
    "        for _ in range(self.num_merges):\n",
    "            counts = Counter(self._get_bigrams(seq))\n",
    "            if not counts: break\n",
    "            best_pair, _ = counts.most_common(1)[0]\n",
    "            merges.append(best_pair)\n",
    "            seq = self._merge_once(seq, best_pair)\n",
    "        self.merges = merges\n",
    "        self.pair_ranks = {p: i for i, p in enumerate(self.merges)}\n",
    "\n",
    "        self.vocab = {}\n",
    "        self.id_to_token = {}\n",
    "        for ch in sorted(set(text)):\n",
    "            self._add_token(ch)\n",
    "        for a, b in self.merges:\n",
    "            self._add_token(a + b)\n",
    "        self.eot_id = self._add_token(self.eot_token)\n",
    "\n",
    "    def encode(self, text, force_last_eot=True):\n",
    "        # treat literal eot marker as special; remove it from content\n",
    "        if self.eot_token in text:\n",
    "            text = text.replace(self.eot_token, '')\n",
    "        seq = list(text)\n",
    "\n",
    "        # make sure all seen base chars exist\n",
    "        for ch in set(seq):\n",
    "            if ch not in self.vocab:\n",
    "                self._add_token(ch)\n",
    "\n",
    "        # greedy BPE using learned pair ranks\n",
    "        if self.merges:\n",
    "            while True:\n",
    "                best_pair, best_rank = None, None\n",
    "                for p in self._get_bigrams(seq):\n",
    "                    r = self.pair_ranks.get(p)\n",
    "                    if r is not None and (best_rank is None or r < best_rank):\n",
    "                        best_pair, best_rank = p, r\n",
    "                if best_pair is None:\n",
    "                    break\n",
    "                seq = self._merge_once(seq, best_pair)\n",
    "\n",
    "        # ensure all tokens in seq exist in vocab (e.g., if new chars appeared)\n",
    "        for tok in seq:\n",
    "            if tok not in self.vocab:\n",
    "                self._add_token(tok)\n",
    "\n",
    "        ids = [self.vocab[tok] for tok in seq]\n",
    "\n",
    "        # FORCE: append EOT id if not already last\n",
    "        if force_last_eot:\n",
    "            if not ids or ids[-1] != self.eot_id:\n",
    "                ids.append(self.eot_id)\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # drop trailing EOT if present\n",
    "        if ids and self.eot_id is not None and ids[-1] == self.eot_id:\n",
    "            ids = ids[:-1]\n",
    "        toks = [self.id_to_token[i] for i in ids]\n",
    "        return ''.join(toks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edf21736-94b1-44f1-9e90-833807b38e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "twinkle_twinkle = r'CCGGAAG,FFEEDDC,GGFFEED,GGFFEED,CCGGAAG,FFEEDDC'\n",
    "hot_cross_buns = r'EDC,EDC,CCCC,DDDD,EDC'\n",
    "happy_birthday = r'GGAGCB,GGAGDC,GGGECBA,FFECDC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb02db67-40d1-4a88-95a1-c497e3bfdaad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('g', 'g'), ('e', 'd'), ('c', 'c'), ('f', 'f'), ('ff', 'e'), ('gg', 'a')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = SimpleBPETokenizer(num_merges=6)\n",
    "examples = [twinkle_twinkle,hot_cross_buns, happy_birthday]\n",
    "tok.train(examples)\n",
    "tok.merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94546825-52f3-4668-bc9c-80bed6d0a444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',': 0,\n",
       " 'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'gg': 8,\n",
       " 'ed': 9,\n",
       " 'cc': 10,\n",
       " 'ff': 11,\n",
       " 'ffe': 12,\n",
       " 'gga': 13,\n",
       " '<|endoftext|>': 14}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a47bae6f-3a84-4677-8b9a-5851a27ccff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tok.vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "210d0bdb-2e94-4788-8feb-9a445dfaa12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 10, 13,  1,  7,  0, 12,  9,  4,  3,  0,  8, 12,  9,  0,  8, 12,  9,\n",
       "         0, 10, 13,  1,  7,  0, 12,  9,  4,  3, 14, 14,  9,  3,  0,  9,  3,  0,\n",
       "        10, 10,  0,  4,  4,  4,  4,  0,  9,  3, 14, 14, 13,  7,  3,  2,  0, 13,\n",
       "         7,  4,  3,  0,  8,  7,  5,  3,  2,  1,  0, 12,  3,  4,  3, 14])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eot = tok.eot_id\n",
    "tokens = []\n",
    "for example in examples:\n",
    "    tokens.extend([eot])\n",
    "    tokens.extend(tok.encode(example.lower()))\n",
    "all_tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f45597-e81b-4dfb-a34a-fb210883041d",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51779778-ccaf-4baf-b009-03c11fec951e",
   "metadata": {},
   "source": [
    "A machine learning model forward pass now uses the tokenization information, runs several layers of linear algebra on it, and then \"predicts\" the probability that each token in the vocab is next. When it is noisy (like you will see in this example), this process results in gibberish.  The training process changes the noise to pattern during the \"backward pass\" as you'll see. We'll show 3 steps that are focused on training:\n",
    "1. **Data Loading** `x, y = train_loader.next_batch()` - this step pulls from the raw data enough tokens to complete a forward pass and loss calcualtion.  If the model is inference only, this step is replaced with taking in the inference input and preparing it similarly as the forward pass.\n",
    "2. **Forward Pass** `logits, loss = model(x, y)` - using the data and the model architecture to we run a prediction for the tokens. When training we also compare against the expected to get loss, but in inference, we use the logits to complete the inference task.\n",
    "3. **Back Propagation, aka Backward Pass & Training** `loss.backward(); optimizer.step()` - using differentials to understand what parameters most impact the forward pass' impact on its prediction, comparing that against what is actually right based on the data loading step, and then making very minor adjustments to the impactful parameters with the hope it improves future predictions.\n",
    "\n",
    "The we'll show a final **Forward Pass** with the updated weights we did in #3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aefcbe-b05e-4e9a-a82f-46089bad50a5",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8293b18-5bdf-4693-82b3-466c20d679e9",
   "metadata": {},
   "source": [
    "To start, we need to get enough data to run the forward and backward passes.  Since our total dataset is likely too big to be held in memory all at once in real practice, we will read just enough file information into memory so that we can run the passes, leaving memory and compute to be used on the passes instead of static data holding. \n",
    "To start, we have to identify the batch size and the model context length to determine how much data we need.  Consequently, these dimensions also form 2 of the 3 dimensions in the initial matrix.\n",
    "- **Batch Size (B)** - This is the number of examples you'll train on in a single pass. \n",
    "- **Context Length (T)** - This is the max number of tokens that a model can use in a single pass to generate the next token. If an example is below this length, it can be padded.\n",
    "  \n",
    "*Ideally both B and T are multiples of 2 to work nicely with chip architecture. This is a common theme across the board*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "454fc40b-459c-457c-b5db-db0733549246",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_batch = 2 # Batch\n",
    "T_context = 8 # context length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62799c29-4030-4f06-af11-0192aa61b1e3",
   "metadata": {},
   "source": [
    "To start, we need to pull from our long raw_token list enough tokens for the forward pass. To be able to satisfy training `B_batch` Batches `T_context` context length, we need to pull out `B*T` tokens to slide the context window across the examples enough to satisfy the batch size. Since our training will attempt to predict the next token after the context, we also need 1 more token at the end so that the last last batch can have the next token to validate against. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68c595f6-0538-4a0b-b666-996fdc30f1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 10, 13,  1,  7,  0, 12,  9,  4,  3,  0,  8, 12,  9,  0,  8, 12])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_position = 0\n",
    "tok_for_training = all_tokens[current_position:current_position + B_batch*T_context +1 ]\n",
    "tok_for_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6978a23-ad08-4ad3-870c-079b05a26b64",
   "metadata": {},
   "source": [
    "Now that we have our initial tokens to train on, we now need to convert it to a matrix that's ready for training. In this step we'll need to create our batches and setup two different arrays: 1/ the input, `x`, tokens that will result in 2/ the output `y` tokens. To create each example in the batch, every `T` tokens will be placed into its own row. \n",
    "\n",
    "Recall that training takes in a string of tokens the length of the context and then predicts the next token. Recall that when we extracted `tok_for_training` we added 1 extra token so that we can evaluate the prediction for the last example. Because of this, the input, `x`, will be all of the tokens up to the second to last element `[:-1]`.  \n",
    "\n",
    "\n",
    "Finally, for `y` we will need to extract a token for every batch. That token will be the one immediatly following the context length or every token at positions `B*T_context +1` where B corresponds to a multiple of every batch. \n",
    "\n",
    "We will now put this together and do the following:\n",
    "1. Extract the input `x` and then split it into an example for each batch `B`\n",
    "2. Extract the output `y` and then split it into an example for each batch `B`\n",
    "\n",
    "*Note: View can take `-1` which allows the matrix to infer the dimension so we do not need to pass in `T`, but given how many matrices we'll work with we want to make sure we're controlling the dimensions or erroring out if they do not match our expectations.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8632328a-d0c0-4036-abea-601786682dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8]),\n",
       " tensor([[14, 10, 13,  1,  7,  0, 12,  9],\n",
       "         [ 4,  3,  0,  8, 12,  9,  0,  8]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=tok_for_training[:-1].view(B_batch, T_context)\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee388227-0a8c-469d-bd51-c683b323d177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 10, 13,  1,  7,  0, 12,  9,  4,  3,  0,  8, 12,  9,  0,  8, 12])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_for_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3440807c-eec6-4dfe-b671-31aaea855172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1]),\n",
       " tensor([[ 4],\n",
       "         [12]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=tok_for_training[T_context::T_context].view(B_batch, 1)\n",
    "y.size(), y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811ab64c-4cae-4095-89b1-4534085323be",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99517d1f-2b40-4db4-9f85-bfdfc5c3b392",
   "metadata": {},
   "source": [
    "<img src=\"explainer_screenshots/cnn/full_network.png\" width=\"200\">\n",
    "\n",
    "\n",
    "During training, in the CNN we've built, the forward pass takes a string of tokens in and predicts the likelihood of of the next token for each batch. This is different than the other models we've used as there's only a single example in each batch. This is mainly because CNNs do best with multi-dimension data and so we're hacking our text input for this explainer by using our `text x embedding` to be our 2 dimensions, isntead of an image or other 2d data. \n",
    "\n",
    "This explainer for the forward pass is focused on training where we'll pass in the input `x`, carry that input through the layers, and generate a matrix of the probability of each token being the next one, something we call `logits`. During the forward pass, since this is an CNN, we will actually pass each example through different convolution layers and even show downsampling, which reduces our matrix size. \n",
    "\n",
    "At the end of the forward pass we then compare the probability in the logits to the actual next token in `y` and calculate `loss` based on the difference. This difference is what we'll then use in the backprop/training steps.  \n",
    "\n",
    "*Note that we will do some layer initialization to simplify following along.  In reality layers are often initialized to normal distribution with some adjustments made for parameter sizes, called Kaiming normal, to keep the weights properly noisy.  We will not cover initialization in this series*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b16235f-5aec-4268-baae-f36a2a377b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_batch, T_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d001e8-02e0-4b9c-8407-6fc7ef07838d",
   "metadata": {},
   "source": [
    "### Input Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20f3332-2ed8-4262-97a4-8d2a8de8473c",
   "metadata": {},
   "source": [
    "<img src=\"explainer_screenshots/cnn/input_layer.png\" width=\"200\">\n",
    "\n",
    "We'll first create an initial **embedding layer** for our input tokens. Recall that this is the layer that will add the second dimension to our text examples. We start with only supplying our embedding positions, though, if we wanted to add more learning capability, we could also do position.  Since CNNs generally take in multi-dimension examples and then use multi-dimension patches for learning in the convolutional layers, position is generally avoided since the goal would be to learn patterns in the data regardless of the position. We will make sure that our embedding weights are larger than 1 to visualize the convolutions well.  The output becomes `vocab_size X n_embd` so that each position can store weights that correspond with each token.  The more embedding layers added the more complex data the model can learn. \n",
    "\n",
    "After the embedding layer we'll then insert in fourth dimension of 1 to better suit our convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308803fa-ae04-4b10-88b7-78f756b36daa",
   "metadata": {},
   "source": [
    "**Embedding** \n",
    "\n",
    "To start we'll initialize our embeddings with an iterative weight so that we can see how it changes through our convolutions.  \n",
    "of 1.000 so that all inputs are equally weighted. We'll also set our embedding dimension to 6 to allow us to see how our convolution strides across the embedding dimension.  You'll see that because our `x` plucks our different embedding rows, we are quickly adjusting away from the nicely ordered initial embeddings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49238829-17be-4c83-aed1-51810f374b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 15)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_embd = 6 # level of embedding of input tokens\n",
    "n_embd, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9697d1c-02bd-49de-8b78-66de9eef7ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600],\n",
       "        [0.0200, 0.0300, 0.0400, 0.0500, 0.0600, 0.0700],\n",
       "        [0.0300, 0.0400, 0.0500, 0.0600, 0.0700, 0.0800],\n",
       "        [0.0400, 0.0500, 0.0600, 0.0700, 0.0800, 0.0900],\n",
       "        [0.0500, 0.0600, 0.0700, 0.0800, 0.0900, 0.1000],\n",
       "        [0.0600, 0.0700, 0.0800, 0.0900, 0.1000, 0.1100],\n",
       "        [0.0700, 0.0800, 0.0900, 0.1000, 0.1100, 0.1200],\n",
       "        [0.0800, 0.0900, 0.1000, 0.1100, 0.1200, 0.1300],\n",
       "        [0.0900, 0.1000, 0.1100, 0.1200, 0.1300, 0.1400],\n",
       "        [0.1000, 0.1100, 0.1200, 0.1300, 0.1400, 0.1500],\n",
       "        [0.1100, 0.1200, 0.1300, 0.1400, 0.1500, 0.1600],\n",
       "        [0.1200, 0.1300, 0.1400, 0.1500, 0.1600, 0.1700],\n",
       "        [0.1300, 0.1400, 0.1500, 0.1600, 0.1700, 0.1800],\n",
       "        [0.1400, 0.1500, 0.1600, 0.1700, 0.1800, 0.1900],\n",
       "        [0.1500, 0.1600, 0.1700, 0.1800, 0.1900, 0.2000]], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wte = nn.Embedding(vocab_size, n_embd)\n",
    "with torch.no_grad(): # initilize to W[i,j] = 0.001*(1+i+j) for easy following \n",
    "    vs, d = wte.num_embeddings, wte.embedding_dim\n",
    "    rows = torch.arange(vs).unsqueeze(1)  # (vs,1)\n",
    "    cols = torch.arange(d).unsqueeze(0)  # (1,d)\n",
    "    pattern = 0.01*(1 + rows + cols)  # W[i,j] = 0.001*(1+i+j)\n",
    "    wte.weight.copy_(pattern)\n",
    "wte.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f4a3355-fa6b-4396-8643-ae8602bef769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 6]),\n",
       " tensor([[[0.1500, 0.1600, 0.1700, 0.1800, 0.1900, 0.2000],\n",
       "          [0.1100, 0.1200, 0.1300, 0.1400, 0.1500, 0.1600],\n",
       "          [0.1400, 0.1500, 0.1600, 0.1700, 0.1800, 0.1900],\n",
       "          [0.0200, 0.0300, 0.0400, 0.0500, 0.0600, 0.0700],\n",
       "          [0.0800, 0.0900, 0.1000, 0.1100, 0.1200, 0.1300],\n",
       "          [0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600],\n",
       "          [0.1300, 0.1400, 0.1500, 0.1600, 0.1700, 0.1800],\n",
       "          [0.1000, 0.1100, 0.1200, 0.1300, 0.1400, 0.1500]],\n",
       " \n",
       "         [[0.0500, 0.0600, 0.0700, 0.0800, 0.0900, 0.1000],\n",
       "          [0.0400, 0.0500, 0.0600, 0.0700, 0.0800, 0.0900],\n",
       "          [0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600],\n",
       "          [0.0900, 0.1000, 0.1100, 0.1200, 0.1300, 0.1400],\n",
       "          [0.1300, 0.1400, 0.1500, 0.1600, 0.1700, 0.1800],\n",
       "          [0.1000, 0.1100, 0.1200, 0.1300, 0.1400, 0.1500],\n",
       "          [0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600],\n",
       "          [0.0900, 0.1000, 0.1100, 0.1200, 0.1300, 0.1400]]],\n",
       "        grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = wte(x)\n",
    "x.shape, x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af748fae-1f4b-4cbe-a361-e3719dcf1908",
   "metadata": {},
   "source": [
    "### Add Dimension\n",
    "\n",
    "We projected our input tokens `x` that was `[B×T]` into the embedding to get `[B×T×C]` so that we now have our `T×C` for each batch. To run our convolution per batch, though, we also need a spatial dimension for the kernel to slide over. PyTorch-style convolution layers expect tensors in `[B, C, H, W]` (channels-first), where the kernel slides over `H,W` while mixing across `C`. Because of this we add a singleton spatial dimension and reorder axes. With this process, the embedding dimension `C` becomes the channels and the token axis `T` becomes the width to slide across:\n",
    "\n",
    "`[B, T, C]  →  [B, C, T]  →  [B, C, 1, T]`\n",
    "\n",
    "The convlution we show is a `1×k` convolution which slides only along our tokens `T`, and aggregates over all `C` channels at each position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35617286-087d-4897-bbeb-69e6dd5ebde6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 8]),\n",
       " tensor([[[[0.1500, 0.1100, 0.1400, 0.0200, 0.0800, 0.0100, 0.1300, 0.1000]],\n",
       " \n",
       "          [[0.1600, 0.1200, 0.1500, 0.0300, 0.0900, 0.0200, 0.1400, 0.1100]],\n",
       " \n",
       "          [[0.1700, 0.1300, 0.1600, 0.0400, 0.1000, 0.0300, 0.1500, 0.1200]],\n",
       " \n",
       "          [[0.1800, 0.1400, 0.1700, 0.0500, 0.1100, 0.0400, 0.1600, 0.1300]],\n",
       " \n",
       "          [[0.1900, 0.1500, 0.1800, 0.0600, 0.1200, 0.0500, 0.1700, 0.1400]],\n",
       " \n",
       "          [[0.2000, 0.1600, 0.1900, 0.0700, 0.1300, 0.0600, 0.1800, 0.1500]]],\n",
       " \n",
       " \n",
       "         [[[0.0500, 0.0400, 0.0100, 0.0900, 0.1300, 0.1000, 0.0100, 0.0900]],\n",
       " \n",
       "          [[0.0600, 0.0500, 0.0200, 0.1000, 0.1400, 0.1100, 0.0200, 0.1000]],\n",
       " \n",
       "          [[0.0700, 0.0600, 0.0300, 0.1100, 0.1500, 0.1200, 0.0300, 0.1100]],\n",
       " \n",
       "          [[0.0800, 0.0700, 0.0400, 0.1200, 0.1600, 0.1300, 0.0400, 0.1200]],\n",
       " \n",
       "          [[0.0900, 0.0800, 0.0500, 0.1300, 0.1700, 0.1400, 0.0500, 0.1300]],\n",
       " \n",
       "          [[0.1000, 0.0900, 0.0600, 0.1400, 0.1800, 0.1500, 0.0600, 0.1400]]]],\n",
       "        grad_fn=<UnsqueezeBackward0>))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.permute(0,2,1) # [B,C,T]\n",
    "x = x.unsqueeze(2)  # [B,C,1,T]\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82c654c-a572-47d8-ae1a-db0c494193f5",
   "metadata": {},
   "source": [
    "### Convolution Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef77ecbb-ddc8-4f83-a7c0-c6f4cec083d0",
   "metadata": {},
   "source": [
    "<img src=\"explainer_screenshots/cnn/convolutional_layers.png\" width=\"400\">\n",
    "\n",
    "As is common in CNNs, we use multiple convolution layers with normalization and nonlinearity to learn increasingly expressive features from the input. Each convolution “looks” at a local patch whose size and stride we choose; stacking layers (sequentially). We also use residual skips to let the model capture richer patterns and relationships.\n",
    "\n",
    "In our model, our input to the convolution is $[B,C,1,T]$ with a $1\\times k$ kernel. The convolution runs as 2-D discrete cross-correlation along the token axis. For output channel $m$,\n",
    "$$\n",
    "y^{m}_{t}=\\sum_{c=1}^{C}\\sum_{u=0}^{k-1} W^{m}_{c,u}x_{c,ts+u-p}+b^{m}.\n",
    "$$\n",
    "\n",
    "Under the hood we:\n",
    "\n",
    "1. Build the matrix of local patches $P\\in\\mathbb{R}^{(Ck)\\times L}$ by extracting all sliding $1\\times k$ windows; $L$ is the number of output positions.\n",
    "2. Flatten the kernel bank into $W_{\\text{flat}}\\in\\mathbb{R}^{C_{\\text{out}}\\times (Ck)}$.\n",
    "3. Compute all positions at once: $Y = W_{\\text{flat}},P \\in \\mathbb{R}^{C_{\\text{out}}\\times L}$ independently for each batch element, then reshape back to $[B,C_{\\text{out}},1,T_{\\text{out}}]$.\n",
    "\n",
    "We interleave batch normalization and ReLU to stabilize activations, improve gradient flow, and add nonlinearity. \n",
    "\n",
    "The second convolution in the block downsamples with stride 2, reducing the token length $T\\to\\lceil T/2\\rceil$. This both cuts compute and expands the effective receptive field of subsequent layers, helping the model capture longer-range patterns over the sequence.\n",
    "\n",
    "\n",
    "Finally, as a nod to ResNets, the convolutional block also uses a residual path. For this path we add a projected skip $S(x)$ to the main path $F(x)$, yielding $y=F(x)+S(x)$.  Since we used downsampling on our main path, the residual path also uses a $1\\times 1$ projection with stride 2 downsample so dimensions of the residual path match that of the convolutional block output. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f3189e-d94b-4afc-aa96-63ecfff22863",
   "metadata": {},
   "source": [
    "#### Convolution Block - 1x3 Conv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a2ab84-9a4b-4850-9418-f8038ebdea5e",
   "metadata": {},
   "source": [
    "##### 1x3 Conv - Initialize weights$\n",
    "Our first convolutional block uses a kernal width of `(1,3)`, a stride of `(1,1)` and padding both at the start and end of the token dimension so that we can slide across all entries. For this first convolution layer we'll go through step by step showing how the convolution is built.  \n",
    "\n",
    "To start, we will setup our weights to be based on the channel dimension, currently equal to our embedding, and our kernal. By matching the kernal we allow the layer to learn what parts of the kernal are more important for our final prediction. \n",
    "\n",
    "We'll also initialize our weights to be iterative so that we can see the impact clearly as they interact with our input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3871f2b-0d2d-492a-a920-03f43e71d618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv 1 kernal': (1, 3), 'conv 1 stride': (1, 1), 'conv 1 padding': (0, 1)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1_kernal_height = 1\n",
    "c1_kernal_width = 3\n",
    "c1_stride_height = 1\n",
    "c1_stride_width = 1\n",
    "c1_padding_height = 0\n",
    "c1_padding_width = 1\n",
    "{'conv 1 kernal': (c1_kernal_height, c1_kernal_width),\n",
    " 'conv 1 stride': (c1_stride_height, c1_stride_width),\n",
    " 'conv 1 padding': (c1_padding_height, c1_padding_width)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cdd7cda-440a-48fc-9a46-26d1b12ad441",
   "metadata": {},
   "outputs": [],
   "source": [
    "## weight layer for convolution (similar to linear, just more explicit)\n",
    "conv1 = nn.Parameter(\n",
    "    torch.empty(n_embd, n_embd, c1_kernal_height, c1_kernal_width), \n",
    "    requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7ec5b799-3a2d-49bc-bb32-57d74b54c3c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 6, 1, 3]),\n",
       " Parameter containing:\n",
       " tensor([[[[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]]],\n",
       " \n",
       " \n",
       "         [[[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]]],\n",
       " \n",
       " \n",
       "         [[[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]]],\n",
       " \n",
       " \n",
       "         [[[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]]],\n",
       " \n",
       " \n",
       "         [[[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]]],\n",
       " \n",
       " \n",
       "         [[[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]],\n",
       " \n",
       "          [[0.0010, 0.0020, 0.0010]]]], requires_grad=True))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iniate rows as 0.1, 0.2, and 0.3 for easier view of the weight impact\n",
    "with torch.no_grad():\n",
    "    c1_pattern = torch.tensor([0.001,0.002,0.001]).view(1,1,1,c1_kernal_width).expand(conv1.size()).clone()\n",
    "    conv1.copy_(c1_pattern)\n",
    "conv1.size(), conv1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449163be-7cf9-4313-878f-9af77d7e0838",
   "metadata": {},
   "source": [
    "**Run Convolution**\n",
    "\n",
    "Now we'll calculate the 2-D discrete cross-correlation for our weight and input `x`.  Since we know we have a residual connection we'll branch `x` and rejoin it after the convolutional block. For our convolutional layer, in our step by step guide we'll do the following: \n",
    "1. Since we have padding, pad our channel\n",
    "2. Flattens, or **unfolds** each sliding kernel_size-sized block within the spatial dimensions of input into a column (i.e., last dimension) of a 3-D output tensor of shape $(N,C*k_h*k_w,L)$\n",
    "3. Stack our weights so that it is resued across batches meaning our learning benefits from both. \n",
    "4. Take the dot product of the unstacked input and the stacked weights and reshape the result back to our batch and channels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e161d7-abb2-4116-9982-b05c66aeab44",
   "metadata": {},
   "source": [
    "##### 1x3 Conv - Step-by-step unfolding\n",
    "In particular we'll focus on step #2, as this specifically creates a sliding view that estracts a kernal size view across our input. By converting them to columns, when we do $W_{flat} \\cdot X_{unfolded}$ the result is a sum of the row in the weight times what was previously a row in the input. Mentally, **unfold** linearizes all local receptive fields so you can do per-patch operations with a single batched matrix multiply. Convolution is exactly this with shared weights, hence the name.  After walking through step by step, we'll show you `F.unfold` a function that does the padding and unfolding for you and use it from there on out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a693e572-baa2-4306-9bac-0474bbed73b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 8]), 2, 6, 1, 8)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = B_batch\n",
    "channel = n_embd\n",
    "height = 1\n",
    "width = T_context\n",
    "x.size(),batch, channel, height, width, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1602cf4e-82df-48da-a74a-6c60ef96583b",
   "metadata": {},
   "source": [
    "**Calculate expected unfolded dimensions**  \n",
    "\n",
    "Since we're doing the unfolding manually, we need to calculate the expected dimensions for our loop.  \n",
    "Recall that we expect to go from $(B,C,1,T)$ to $(B,C*k_h*k_w,L)$ where $L$ is a flattening or our ouput height and width as follows: \n",
    "$$\n",
    "\\begin{align}\n",
    "height_{out} &= (height + 2*pad_h - 1*(kernal_h-1) -1)\\ //\\ stride_{h}\\\\ \n",
    "width_{out} &= (width + 2*pad_w - 1*(kernal_w-1) -1)\\ //\\ stride_{w}\\\\\n",
    "L &= height_{out} * width_{out}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97f70c1c-6a3d-4277-80ec-e32d07442d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width out 8, height out 1, final dimension (2,18,8)\n"
     ]
    }
   ],
   "source": [
    "c1_khw = channel*c1_kernal_height*c1_kernal_width\n",
    "\n",
    "c1_height_out = (height + 2*c1_padding_height - 1*(c1_kernal_height-1) - 1)//c1_stride_height + 1   # = 1, \n",
    "c1_width_out = (width + 2*c1_padding_width - 1*(c1_kernal_width-1) - 1)//c1_stride_width + 1   # = 4\n",
    "c1_L = c1_height_out * c1_width_out\n",
    "\n",
    "print(f'width out {c1_width_out}, height out {c1_height_out}, final dimension ({batch},{c1_khw},{c1_L})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e830999-85d7-46b7-bddc-bcbc1c646f85",
   "metadata": {},
   "source": [
    "**Padding** \n",
    "\n",
    "We first start by padding.  Since we're using a stride of `(1,3)` we need to padd both the start and end of the tokens so that we can slide across it without losing an increment on the dimension. Padding simply adds `0` though we can add other values if we wanted.  When we pad on both sides we get output of `[2, 6, 1+0, 8+2]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "068c1f6a-6e9f-4ea6-b028-b1d028f59992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 10]),\n",
       " tensor([[[[0.0000, 0.1500, 0.1100, 0.1400, 0.0200, 0.0800, 0.0100, 0.1300,\n",
       "            0.1000, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.1600, 0.1200, 0.1500, 0.0300, 0.0900, 0.0200, 0.1400,\n",
       "            0.1100, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.1700, 0.1300, 0.1600, 0.0400, 0.1000, 0.0300, 0.1500,\n",
       "            0.1200, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.1800, 0.1400, 0.1700, 0.0500, 0.1100, 0.0400, 0.1600,\n",
       "            0.1300, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.1900, 0.1500, 0.1800, 0.0600, 0.1200, 0.0500, 0.1700,\n",
       "            0.1400, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.2000, 0.1600, 0.1900, 0.0700, 0.1300, 0.0600, 0.1800,\n",
       "            0.1500, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.0000, 0.0500, 0.0400, 0.0100, 0.0900, 0.1300, 0.1000, 0.0100,\n",
       "            0.0900, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.0600, 0.0500, 0.0200, 0.1000, 0.1400, 0.1100, 0.0200,\n",
       "            0.1000, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.0700, 0.0600, 0.0300, 0.1100, 0.1500, 0.1200, 0.0300,\n",
       "            0.1100, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.0800, 0.0700, 0.0400, 0.1200, 0.1600, 0.1300, 0.0400,\n",
       "            0.1200, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.0900, 0.0800, 0.0500, 0.1300, 0.1700, 0.1400, 0.0500,\n",
       "            0.1300, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.1000, 0.0900, 0.0600, 0.1400, 0.1800, 0.1500, 0.0600,\n",
       "            0.1400, 0.0000]]]], grad_fn=<ConstantPadNdBackward0>))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad last dim by (width, width) and 2nd to last by (height, height). width = 1, height = 0\n",
    "c1_x_pad = F.pad(x, pad=(c1_padding_width,c1_padding_width,c1_padding_height,c1_padding_height))\n",
    "\n",
    "c1_x_pad.size(), c1_x_pad #total size and show first example in batch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17938995-a7ba-4140-af02-d1698afbebd3",
   "metadata": {},
   "source": [
    "**Maual Unfolding - First Stride** \n",
    "\n",
    "Now we will nmanually unfold our padded input.  The process of unfolding flattens each sliding kernel-sized block within the spatial dimensions of input into a column (i.e., last dimension) of a 3-D output tensor of shape $(N,C*k_h*k_w,L)$ \n",
    "\n",
    "We'll first start by pullling out the first patch.  Since we have a kernal of `(1,3)` we pull out the first 3 tokens from first spatial dimension for each Channel in each batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f35fbee0-cb49-4eef-b000-bd6ae742024e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 3]),\n",
       " tensor([[[[0.0000, 0.1500, 0.1100]],\n",
       " \n",
       "          [[0.0000, 0.1600, 0.1200]],\n",
       " \n",
       "          [[0.0000, 0.1700, 0.1300]],\n",
       " \n",
       "          [[0.0000, 0.1800, 0.1400]],\n",
       " \n",
       "          [[0.0000, 0.1900, 0.1500]],\n",
       " \n",
       "          [[0.0000, 0.2000, 0.1600]]],\n",
       " \n",
       " \n",
       "         [[[0.0000, 0.0500, 0.0400]],\n",
       " \n",
       "          [[0.0000, 0.0600, 0.0500]],\n",
       " \n",
       "          [[0.0000, 0.0700, 0.0600]],\n",
       " \n",
       "          [[0.0000, 0.0800, 0.0700]],\n",
       " \n",
       "          [[0.0000, 0.0900, 0.0800]],\n",
       " \n",
       "          [[0.0000, 0.1000, 0.0900]]]], grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = 0\n",
    "patch = c1_x_pad[:, :, step:c1_kernal_height, step:step+c1_kernal_width]\n",
    "patch.size(), patch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2290686c-7693-4523-b5b7-188b8f294e6d",
   "metadata": {},
   "source": [
    "Now we need to stack our channels together. Since we want to make sure that eventually we can do a dot product of the weight and input where the weight column multiplies by the entry row, flattening our patches into a single entry gives us that.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fb79101d-be10-4d00-856b-ffb14676ae40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 18]),\n",
       " tensor([[0.0000, 0.1500, 0.1100, 0.0000, 0.1600, 0.1200, 0.0000, 0.1700, 0.1300,\n",
       "          0.0000, 0.1800, 0.1400, 0.0000, 0.1900, 0.1500, 0.0000, 0.2000, 0.1600],\n",
       "         [0.0000, 0.0500, 0.0400, 0.0000, 0.0600, 0.0500, 0.0000, 0.0700, 0.0600,\n",
       "          0.0000, 0.0800, 0.0700, 0.0000, 0.0900, 0.0800, 0.0000, 0.1000, 0.0900]],\n",
       "        grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = patch.reshape(batch, c1_khw)\n",
    "col.size(), col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb8bb4e-5214-43c9-b44c-a554179264b4",
   "metadata": {},
   "source": [
    "Finally we want to make sure to save this since this is just the first pass of the patch. Let's create a list for now and store them. After we complete all the strides we can reshape our final output of the unfolded step to make each entry a column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e5bf5e34-345b-4023-89d8-4441fb07137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_cols = []\n",
    "manual_cols.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfbff38-59a5-4ea3-8828-f58faaa972d3",
   "metadata": {},
   "source": [
    "**Maual Unfolding - Second Stride** \n",
    "\n",
    "We now need to move our patch by the stride amount, in this case `(1,1)`. Using a stride of 1 on both dimensions ensures that we continue covering every input token in the example. As you'll see in future convolutions, changing the stride can downsample an input.  Let's start by again extracting the patch. You'll see that we just shifted to the \"left\" by 1 and took the next 3 columns in our input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5d8dc717-aee8-4625-b8ba-09f5eb108a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 3]),\n",
       " tensor([[[[0.1500, 0.1100, 0.1400]],\n",
       " \n",
       "          [[0.1600, 0.1200, 0.1500]],\n",
       " \n",
       "          [[0.1700, 0.1300, 0.1600]],\n",
       " \n",
       "          [[0.1800, 0.1400, 0.1700]],\n",
       " \n",
       "          [[0.1900, 0.1500, 0.1800]],\n",
       " \n",
       "          [[0.2000, 0.1600, 0.1900]]],\n",
       " \n",
       " \n",
       "         [[[0.0500, 0.0400, 0.0100]],\n",
       " \n",
       "          [[0.0600, 0.0500, 0.0200]],\n",
       " \n",
       "          [[0.0700, 0.0600, 0.0300]],\n",
       " \n",
       "          [[0.0800, 0.0700, 0.0400]],\n",
       " \n",
       "          [[0.0900, 0.0800, 0.0500]],\n",
       " \n",
       "          [[0.1000, 0.0900, 0.0600]]]], grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = 1\n",
    "patch = c1_x_pad[:, :, 0:c1_kernal_height, step:step+c1_kernal_width]\n",
    "patch.size(), patch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531b8fcf-d7f5-451f-b6cb-b2cfb26ea86e",
   "metadata": {},
   "source": [
    "we'll again flatten this the same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "60c4f1ef-7b65-4b60-aade-9d96dd8cb15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 18]),\n",
       " tensor([[0.1500, 0.1100, 0.1400, 0.1600, 0.1200, 0.1500, 0.1700, 0.1300, 0.1600,\n",
       "          0.1800, 0.1400, 0.1700, 0.1900, 0.1500, 0.1800, 0.2000, 0.1600, 0.1900],\n",
       "         [0.0500, 0.0400, 0.0100, 0.0600, 0.0500, 0.0200, 0.0700, 0.0600, 0.0300,\n",
       "          0.0800, 0.0700, 0.0400, 0.0900, 0.0800, 0.0500, 0.1000, 0.0900, 0.0600]],\n",
       "        grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = patch.reshape(batch, c1_khw)\n",
    "col.size(), col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c236208-2c2f-4c14-a70b-71d72678acdc",
   "metadata": {},
   "source": [
    "and now add it to our list.  We can now see that we have entries for our first 2 steps already in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "609ab8e0-dab8-4ca5-a3cc-aacbb5962331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.0000, 0.1500, 0.1100, 0.0000, 0.1600, 0.1200, 0.0000, 0.1700, 0.1300,\n",
       "          0.0000, 0.1800, 0.1400, 0.0000, 0.1900, 0.1500, 0.0000, 0.2000, 0.1600],\n",
       "         [0.0000, 0.0500, 0.0400, 0.0000, 0.0600, 0.0500, 0.0000, 0.0700, 0.0600,\n",
       "          0.0000, 0.0800, 0.0700, 0.0000, 0.0900, 0.0800, 0.0000, 0.1000, 0.0900]],\n",
       "        grad_fn=<UnsafeViewBackward0>),\n",
       " tensor([[0.1500, 0.1100, 0.1400, 0.1600, 0.1200, 0.1500, 0.1700, 0.1300, 0.1600,\n",
       "          0.1800, 0.1400, 0.1700, 0.1900, 0.1500, 0.1800, 0.2000, 0.1600, 0.1900],\n",
       "         [0.0500, 0.0400, 0.0100, 0.0600, 0.0500, 0.0200, 0.0700, 0.0600, 0.0300,\n",
       "          0.0800, 0.0700, 0.0400, 0.0900, 0.0800, 0.0500, 0.1000, 0.0900, 0.0600]],\n",
       "        grad_fn=<UnsafeViewBackward0>)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_cols.append(col)\n",
    "manual_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55718f56-dc69-4430-9782-b0e542e4e778",
   "metadata": {},
   "source": [
    "**Maual Unfolding - Remaining Strides** \n",
    "\n",
    "We'll now loop through the remaining steps for the manual unfolding to fill in the rest of the list.  This is the same set of steps done before, just in a loop but appending to the same list.  We'll start from 2 onward since we already did steps 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4172ea99-b2de-460f-8d8d-c91854d95746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execting stride 2\n",
      "execting stride 3\n",
      "execting stride 4\n",
      "execting stride 5\n",
      "execting stride 6\n",
      "execting stride 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[0.0000, 0.1500, 0.1100, 0.0000, 0.1600, 0.1200, 0.0000, 0.1700, 0.1300,\n",
       "          0.0000, 0.1800, 0.1400, 0.0000, 0.1900, 0.1500, 0.0000, 0.2000, 0.1600],\n",
       "         [0.0000, 0.0500, 0.0400, 0.0000, 0.0600, 0.0500, 0.0000, 0.0700, 0.0600,\n",
       "          0.0000, 0.0800, 0.0700, 0.0000, 0.0900, 0.0800, 0.0000, 0.1000, 0.0900]],\n",
       "        grad_fn=<UnsafeViewBackward0>),\n",
       " tensor([[0.1500, 0.1100, 0.1400, 0.1600, 0.1200, 0.1500, 0.1700, 0.1300, 0.1600,\n",
       "          0.1800, 0.1400, 0.1700, 0.1900, 0.1500, 0.1800, 0.2000, 0.1600, 0.1900],\n",
       "         [0.0500, 0.0400, 0.0100, 0.0600, 0.0500, 0.0200, 0.0700, 0.0600, 0.0300,\n",
       "          0.0800, 0.0700, 0.0400, 0.0900, 0.0800, 0.0500, 0.1000, 0.0900, 0.0600]],\n",
       "        grad_fn=<UnsafeViewBackward0>),\n",
       " tensor([[0.1100, 0.1400, 0.0200, 0.1200, 0.1500, 0.0300, 0.1300, 0.1600, 0.0400,\n",
       "          0.1400, 0.1700, 0.0500, 0.1500, 0.1800, 0.0600, 0.1600, 0.1900, 0.0700],\n",
       "         [0.0400, 0.0100, 0.0900, 0.0500, 0.0200, 0.1000, 0.0600, 0.0300, 0.1100,\n",
       "          0.0700, 0.0400, 0.1200, 0.0800, 0.0500, 0.1300, 0.0900, 0.0600, 0.1400]],\n",
       "        grad_fn=<UnsafeViewBackward0>),\n",
       " tensor([[0.1400, 0.0200, 0.0800, 0.1500, 0.0300, 0.0900, 0.1600, 0.0400, 0.1000,\n",
       "          0.1700, 0.0500, 0.1100, 0.1800, 0.0600, 0.1200, 0.1900, 0.0700, 0.1300],\n",
       "         [0.0100, 0.0900, 0.1300, 0.0200, 0.1000, 0.1400, 0.0300, 0.1100, 0.1500,\n",
       "          0.0400, 0.1200, 0.1600, 0.0500, 0.1300, 0.1700, 0.0600, 0.1400, 0.1800]],\n",
       "        grad_fn=<UnsafeViewBackward0>),\n",
       " tensor([[0.0200, 0.0800, 0.0100, 0.0300, 0.0900, 0.0200, 0.0400, 0.1000, 0.0300,\n",
       "          0.0500, 0.1100, 0.0400, 0.0600, 0.1200, 0.0500, 0.0700, 0.1300, 0.0600],\n",
       "         [0.0900, 0.1300, 0.1000, 0.1000, 0.1400, 0.1100, 0.1100, 0.1500, 0.1200,\n",
       "          0.1200, 0.1600, 0.1300, 0.1300, 0.1700, 0.1400, 0.1400, 0.1800, 0.1500]],\n",
       "        grad_fn=<UnsafeViewBackward0>),\n",
       " tensor([[0.0800, 0.0100, 0.1300, 0.0900, 0.0200, 0.1400, 0.1000, 0.0300, 0.1500,\n",
       "          0.1100, 0.0400, 0.1600, 0.1200, 0.0500, 0.1700, 0.1300, 0.0600, 0.1800],\n",
       "         [0.1300, 0.1000, 0.0100, 0.1400, 0.1100, 0.0200, 0.1500, 0.1200, 0.0300,\n",
       "          0.1600, 0.1300, 0.0400, 0.1700, 0.1400, 0.0500, 0.1800, 0.1500, 0.0600]],\n",
       "        grad_fn=<UnsafeViewBackward0>),\n",
       " tensor([[0.0100, 0.1300, 0.1000, 0.0200, 0.1400, 0.1100, 0.0300, 0.1500, 0.1200,\n",
       "          0.0400, 0.1600, 0.1300, 0.0500, 0.1700, 0.1400, 0.0600, 0.1800, 0.1500],\n",
       "         [0.1000, 0.0100, 0.0900, 0.1100, 0.0200, 0.1000, 0.1200, 0.0300, 0.1100,\n",
       "          0.1300, 0.0400, 0.1200, 0.1400, 0.0500, 0.1300, 0.1500, 0.0600, 0.1400]],\n",
       "        grad_fn=<UnsafeViewBackward0>),\n",
       " tensor([[0.1300, 0.1000, 0.0000, 0.1400, 0.1100, 0.0000, 0.1500, 0.1200, 0.0000,\n",
       "          0.1600, 0.1300, 0.0000, 0.1700, 0.1400, 0.0000, 0.1800, 0.1500, 0.0000],\n",
       "         [0.0100, 0.0900, 0.0000, 0.0200, 0.1000, 0.0000, 0.0300, 0.1100, 0.0000,\n",
       "          0.0400, 0.1200, 0.0000, 0.0500, 0.1300, 0.0000, 0.0600, 0.1400, 0.0000]],\n",
       "        grad_fn=<UnsafeViewBackward0>)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for step in range(2,c1_width_out): \n",
    "    print(f'execting stride {step}')\n",
    "    # extract step\n",
    "    patch = c1_x_pad[:, :, 0:c1_kernal_height, step:step+c1_kernal_width]        # (2,6,1,3)\n",
    "    \n",
    "    # stack the entries in each batch together into a row\n",
    "    col = patch.reshape(batch, c1_khw) # shape to [2,18]\n",
    "\n",
    "    manual_cols.append(col)\n",
    "\n",
    "manual_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b165195b-0714-4208-8e41-9cb80b475e3b",
   "metadata": {},
   "source": [
    "**Maual Unfolding - Flatten List** \n",
    "\n",
    "Now that we've completed the patch extractsion we have a list of tensors. We want to create a new tensore where we maintain the batch of 2 but convert our row length of 18 into the column dimension. We'll use stack to complete this and result in a `(2,18,8)` tensor, just like we calculated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "92f1c1b0-b56d-4745-b0b4-a0df400d35a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 18, 8]),\n",
       " tensor([[[0.0000, 0.1500, 0.1100, 0.1400, 0.0200, 0.0800, 0.0100, 0.1300],\n",
       "          [0.1500, 0.1100, 0.1400, 0.0200, 0.0800, 0.0100, 0.1300, 0.1000],\n",
       "          [0.1100, 0.1400, 0.0200, 0.0800, 0.0100, 0.1300, 0.1000, 0.0000],\n",
       "          [0.0000, 0.1600, 0.1200, 0.1500, 0.0300, 0.0900, 0.0200, 0.1400],\n",
       "          [0.1600, 0.1200, 0.1500, 0.0300, 0.0900, 0.0200, 0.1400, 0.1100],\n",
       "          [0.1200, 0.1500, 0.0300, 0.0900, 0.0200, 0.1400, 0.1100, 0.0000],\n",
       "          [0.0000, 0.1700, 0.1300, 0.1600, 0.0400, 0.1000, 0.0300, 0.1500],\n",
       "          [0.1700, 0.1300, 0.1600, 0.0400, 0.1000, 0.0300, 0.1500, 0.1200],\n",
       "          [0.1300, 0.1600, 0.0400, 0.1000, 0.0300, 0.1500, 0.1200, 0.0000],\n",
       "          [0.0000, 0.1800, 0.1400, 0.1700, 0.0500, 0.1100, 0.0400, 0.1600],\n",
       "          [0.1800, 0.1400, 0.1700, 0.0500, 0.1100, 0.0400, 0.1600, 0.1300],\n",
       "          [0.1400, 0.1700, 0.0500, 0.1100, 0.0400, 0.1600, 0.1300, 0.0000],\n",
       "          [0.0000, 0.1900, 0.1500, 0.1800, 0.0600, 0.1200, 0.0500, 0.1700],\n",
       "          [0.1900, 0.1500, 0.1800, 0.0600, 0.1200, 0.0500, 0.1700, 0.1400],\n",
       "          [0.1500, 0.1800, 0.0600, 0.1200, 0.0500, 0.1700, 0.1400, 0.0000],\n",
       "          [0.0000, 0.2000, 0.1600, 0.1900, 0.0700, 0.1300, 0.0600, 0.1800],\n",
       "          [0.2000, 0.1600, 0.1900, 0.0700, 0.1300, 0.0600, 0.1800, 0.1500],\n",
       "          [0.1600, 0.1900, 0.0700, 0.1300, 0.0600, 0.1800, 0.1500, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0500, 0.0400, 0.0100, 0.0900, 0.1300, 0.1000, 0.0100],\n",
       "          [0.0500, 0.0400, 0.0100, 0.0900, 0.1300, 0.1000, 0.0100, 0.0900],\n",
       "          [0.0400, 0.0100, 0.0900, 0.1300, 0.1000, 0.0100, 0.0900, 0.0000],\n",
       "          [0.0000, 0.0600, 0.0500, 0.0200, 0.1000, 0.1400, 0.1100, 0.0200],\n",
       "          [0.0600, 0.0500, 0.0200, 0.1000, 0.1400, 0.1100, 0.0200, 0.1000],\n",
       "          [0.0500, 0.0200, 0.1000, 0.1400, 0.1100, 0.0200, 0.1000, 0.0000],\n",
       "          [0.0000, 0.0700, 0.0600, 0.0300, 0.1100, 0.1500, 0.1200, 0.0300],\n",
       "          [0.0700, 0.0600, 0.0300, 0.1100, 0.1500, 0.1200, 0.0300, 0.1100],\n",
       "          [0.0600, 0.0300, 0.1100, 0.1500, 0.1200, 0.0300, 0.1100, 0.0000],\n",
       "          [0.0000, 0.0800, 0.0700, 0.0400, 0.1200, 0.1600, 0.1300, 0.0400],\n",
       "          [0.0800, 0.0700, 0.0400, 0.1200, 0.1600, 0.1300, 0.0400, 0.1200],\n",
       "          [0.0700, 0.0400, 0.1200, 0.1600, 0.1300, 0.0400, 0.1200, 0.0000],\n",
       "          [0.0000, 0.0900, 0.0800, 0.0500, 0.1300, 0.1700, 0.1400, 0.0500],\n",
       "          [0.0900, 0.0800, 0.0500, 0.1300, 0.1700, 0.1400, 0.0500, 0.1300],\n",
       "          [0.0800, 0.0500, 0.1300, 0.1700, 0.1400, 0.0500, 0.1300, 0.0000],\n",
       "          [0.0000, 0.1000, 0.0900, 0.0600, 0.1400, 0.1800, 0.1500, 0.0600],\n",
       "          [0.1000, 0.0900, 0.0600, 0.1400, 0.1800, 0.1500, 0.0600, 0.1400],\n",
       "          [0.0900, 0.0600, 0.1400, 0.1800, 0.1500, 0.0600, 0.1400, 0.0000]]],\n",
       "        grad_fn=<StackBackward0>))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn all the rows in the list into columns while maintaining the batch\n",
    "manual_unfold = torch.stack(manual_cols, dim=2)  # (N, 18, 8)\n",
    "manual_unfold.size(), manual_unfold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade75289-413e-4597-ab98-fa3c91d97333",
   "metadata": {},
   "source": [
    "**Unfolding - Efficiently**\n",
    "\n",
    "While the above is great for demonstration purposes, it eats up a lot of time and code space.  Let's switch to the help of a pytorch function `F.unfold`.  This unfold fucntion does the same steps as above: padding, patch extraction, reshaping, stacking. \n",
    "\n",
    "Lets setup our unfold of the original input `x`.  We'll also do a comparison of the previous output `manual_unfold` with this functions output to demonstrate that it is infact equal and we can use it going forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5fc5670d-b3ad-4f9f-ab1d-6d1609733662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manual equals unfold: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 18, 8]),\n",
       " tensor([[[0.0000, 0.1500, 0.1100, 0.1400, 0.0200, 0.0800, 0.0100, 0.1300],\n",
       "          [0.1500, 0.1100, 0.1400, 0.0200, 0.0800, 0.0100, 0.1300, 0.1000],\n",
       "          [0.1100, 0.1400, 0.0200, 0.0800, 0.0100, 0.1300, 0.1000, 0.0000],\n",
       "          [0.0000, 0.1600, 0.1200, 0.1500, 0.0300, 0.0900, 0.0200, 0.1400],\n",
       "          [0.1600, 0.1200, 0.1500, 0.0300, 0.0900, 0.0200, 0.1400, 0.1100],\n",
       "          [0.1200, 0.1500, 0.0300, 0.0900, 0.0200, 0.1400, 0.1100, 0.0000],\n",
       "          [0.0000, 0.1700, 0.1300, 0.1600, 0.0400, 0.1000, 0.0300, 0.1500],\n",
       "          [0.1700, 0.1300, 0.1600, 0.0400, 0.1000, 0.0300, 0.1500, 0.1200],\n",
       "          [0.1300, 0.1600, 0.0400, 0.1000, 0.0300, 0.1500, 0.1200, 0.0000],\n",
       "          [0.0000, 0.1800, 0.1400, 0.1700, 0.0500, 0.1100, 0.0400, 0.1600],\n",
       "          [0.1800, 0.1400, 0.1700, 0.0500, 0.1100, 0.0400, 0.1600, 0.1300],\n",
       "          [0.1400, 0.1700, 0.0500, 0.1100, 0.0400, 0.1600, 0.1300, 0.0000],\n",
       "          [0.0000, 0.1900, 0.1500, 0.1800, 0.0600, 0.1200, 0.0500, 0.1700],\n",
       "          [0.1900, 0.1500, 0.1800, 0.0600, 0.1200, 0.0500, 0.1700, 0.1400],\n",
       "          [0.1500, 0.1800, 0.0600, 0.1200, 0.0500, 0.1700, 0.1400, 0.0000],\n",
       "          [0.0000, 0.2000, 0.1600, 0.1900, 0.0700, 0.1300, 0.0600, 0.1800],\n",
       "          [0.2000, 0.1600, 0.1900, 0.0700, 0.1300, 0.0600, 0.1800, 0.1500],\n",
       "          [0.1600, 0.1900, 0.0700, 0.1300, 0.0600, 0.1800, 0.1500, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0500, 0.0400, 0.0100, 0.0900, 0.1300, 0.1000, 0.0100],\n",
       "          [0.0500, 0.0400, 0.0100, 0.0900, 0.1300, 0.1000, 0.0100, 0.0900],\n",
       "          [0.0400, 0.0100, 0.0900, 0.1300, 0.1000, 0.0100, 0.0900, 0.0000],\n",
       "          [0.0000, 0.0600, 0.0500, 0.0200, 0.1000, 0.1400, 0.1100, 0.0200],\n",
       "          [0.0600, 0.0500, 0.0200, 0.1000, 0.1400, 0.1100, 0.0200, 0.1000],\n",
       "          [0.0500, 0.0200, 0.1000, 0.1400, 0.1100, 0.0200, 0.1000, 0.0000],\n",
       "          [0.0000, 0.0700, 0.0600, 0.0300, 0.1100, 0.1500, 0.1200, 0.0300],\n",
       "          [0.0700, 0.0600, 0.0300, 0.1100, 0.1500, 0.1200, 0.0300, 0.1100],\n",
       "          [0.0600, 0.0300, 0.1100, 0.1500, 0.1200, 0.0300, 0.1100, 0.0000],\n",
       "          [0.0000, 0.0800, 0.0700, 0.0400, 0.1200, 0.1600, 0.1300, 0.0400],\n",
       "          [0.0800, 0.0700, 0.0400, 0.1200, 0.1600, 0.1300, 0.0400, 0.1200],\n",
       "          [0.0700, 0.0400, 0.1200, 0.1600, 0.1300, 0.0400, 0.1200, 0.0000],\n",
       "          [0.0000, 0.0900, 0.0800, 0.0500, 0.1300, 0.1700, 0.1400, 0.0500],\n",
       "          [0.0900, 0.0800, 0.0500, 0.1300, 0.1700, 0.1400, 0.0500, 0.1300],\n",
       "          [0.0800, 0.0500, 0.1300, 0.1700, 0.1400, 0.0500, 0.1300, 0.0000],\n",
       "          [0.0000, 0.1000, 0.0900, 0.0600, 0.1400, 0.1800, 0.1500, 0.0600],\n",
       "          [0.1000, 0.0900, 0.0600, 0.1400, 0.1800, 0.1500, 0.0600, 0.1400],\n",
       "          [0.0900, 0.0600, 0.1400, 0.1800, 0.1500, 0.0600, 0.1400, 0.0000]]],\n",
       "        grad_fn=<Im2ColBackward0>))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1_unfolded = F.unfold(x, \n",
    "\t\tkernel_size=(c1_kernal_height, c1_kernal_width),  # (1,3)\n",
    "\t\tpadding=(c1_padding_height, c1_padding_width), #(0,1)\n",
    "\t\tstride=(c1_stride_height, c1_stride_width))#(1,1)\n",
    "\n",
    "print(\"manual equals unfold:\", torch.allclose(c1_unfolded, manual_unfold))\n",
    "c1_unfolded.size() , c1_unfolded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec055fe-103a-4ea4-ad03-5095ab0e4a0a",
   "metadata": {},
   "source": [
    "##### 1x3 Conv - $W\\cdot X_{unfolded}$\n",
    "\n",
    "Now that we have our unstacked patches, we can then let our network decide how much of the patch, and which part of the patch, influences our output.  To do this we take the dot product of the weight with the unfoleded input.  We do have an issue though since our weight is `[6,6,1,3]` but our input is `[2x18x8]`. We will solve this simply by squeezing the last two dimensions of our Weights together to result in a `[6,18]` tensor that we can multiply.  \n",
    "\n",
    "You might be now asking \"what about the batch dimesnions of 2\".  We do want to make sure the 2 different batches actually share the same weight so we don't actually want to increase our weight dimension.  Instead we rely on the pytorch which broadcasts the same matrix to each of the batches in the input automatically.  This allows the two batches to share the weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "41e453ff-f282-44ed-a85b-b4990499fcfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 18]),\n",
       " tensor([[0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010,\n",
       "          0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010],\n",
       "         [0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010,\n",
       "          0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010],\n",
       "         [0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010,\n",
       "          0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010],\n",
       "         [0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010,\n",
       "          0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010],\n",
       "         [0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010,\n",
       "          0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010],\n",
       "         [0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010,\n",
       "          0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010, 0.0010, 0.0020, 0.0010]],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1_weigth = conv1.view(n_embd, -1) # [6,6,1,3] > [6,18]\n",
    "conv1_weigth.size(), conv1_weigth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258056eb-8544-4bd0-bbd6-6a0a0a7ba561",
   "metadata": {},
   "source": [
    "Now that we have the weigths in the dimension we want them we're ready to multiply them with the unfolded input.  Because in our weight matrix each \"row\" is the same, each of our column entries in the result will be eqal.  We'll also get a final output of `[2,6,8]` compressing the 18 down. Also note that the batch dimension is maintained as the weight is broadcast across the batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5328b95a-6d03-4afb-b060-145c3a77c43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 8]),\n",
       " tensor([[[0.0029, 0.0037, 0.0031, 0.0022, 0.0017, 0.0020, 0.0028, 0.0024],\n",
       "          [0.0029, 0.0037, 0.0031, 0.0022, 0.0017, 0.0020, 0.0028, 0.0024],\n",
       "          [0.0029, 0.0037, 0.0031, 0.0022, 0.0017, 0.0020, 0.0028, 0.0024],\n",
       "          [0.0029, 0.0037, 0.0031, 0.0022, 0.0017, 0.0020, 0.0028, 0.0024],\n",
       "          [0.0029, 0.0037, 0.0031, 0.0022, 0.0017, 0.0020, 0.0028, 0.0024],\n",
       "          [0.0029, 0.0037, 0.0031, 0.0022, 0.0017, 0.0020, 0.0028, 0.0024]],\n",
       " \n",
       "         [[0.0013, 0.0014, 0.0015, 0.0025, 0.0033, 0.0026, 0.0019, 0.0016],\n",
       "          [0.0013, 0.0014, 0.0015, 0.0025, 0.0033, 0.0026, 0.0019, 0.0016],\n",
       "          [0.0013, 0.0014, 0.0015, 0.0025, 0.0033, 0.0026, 0.0019, 0.0016],\n",
       "          [0.0013, 0.0014, 0.0015, 0.0025, 0.0033, 0.0026, 0.0019, 0.0016],\n",
       "          [0.0013, 0.0014, 0.0015, 0.0025, 0.0033, 0.0026, 0.0019, 0.0016],\n",
       "          [0.0013, 0.0014, 0.0015, 0.0025, 0.0033, 0.0026, 0.0019, 0.0016]]],\n",
       "        grad_fn=<CloneBackward0>))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = conv1_weigth @ c1_unfolded\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e458f84d-f6a3-47c8-b005-d4c92911a317",
   "metadata": {},
   "source": [
    "Finally we need to resize our last dimension back to our target channel heigh and width.  Since our height is 1 it will just insert in another dimension of 1 without looking signficantly different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6d782195-9352-4fdf-b26d-cdd71cdb145f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 1, 8]),\n",
       " tensor([[[[0.0029, 0.0037, 0.0031, 0.0022, 0.0017, 0.0020, 0.0028, 0.0024]],\n",
       " \n",
       "          [[0.0029, 0.0037, 0.0031, 0.0022, 0.0017, 0.0020, 0.0028, 0.0024]],\n",
       " \n",
       "          [[0.0029, 0.0037, 0.0031, 0.0022, 0.0017, 0.0020, 0.0028, 0.0024]],\n",
       " \n",
       "          [[0.0029, 0.0037, 0.0031, 0.0022, 0.0017, 0.0020, 0.0028, 0.0024]],\n",
       " \n",
       "          [[0.0029, 0.0037, 0.0031, 0.0022, 0.0017, 0.0020, 0.0028, 0.0024]],\n",
       " \n",
       "          [[0.0029, 0.0037, 0.0031, 0.0022, 0.0017, 0.0020, 0.0028, 0.0024]]],\n",
       " \n",
       " \n",
       "         [[[0.0013, 0.0014, 0.0015, 0.0025, 0.0033, 0.0026, 0.0019, 0.0016]],\n",
       " \n",
       "          [[0.0013, 0.0014, 0.0015, 0.0025, 0.0033, 0.0026, 0.0019, 0.0016]],\n",
       " \n",
       "          [[0.0013, 0.0014, 0.0015, 0.0025, 0.0033, 0.0026, 0.0019, 0.0016]],\n",
       " \n",
       "          [[0.0013, 0.0014, 0.0015, 0.0025, 0.0033, 0.0026, 0.0019, 0.0016]],\n",
       " \n",
       "          [[0.0013, 0.0014, 0.0015, 0.0025, 0.0033, 0.0026, 0.0019, 0.0016]],\n",
       " \n",
       "          [[0.0013, 0.0014, 0.0015, 0.0025, 0.0033, 0.0026, 0.0019, 0.0016]]]],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out.view(batch,n_embd, c1_height_out, c1_width_out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1926f12-bb37-426e-985f-f3f0c502152e",
   "metadata": {},
   "source": [
    "#### Convolution Block - First Batch Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bc8b28-866e-4e4a-b04c-a1916f1aeb14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20809e41-d391-4b13-abc4-f7b4bdf7b258",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_a = nn.BatchNorm2d(n_embd)\n",
    "bn_a.weight, bn_a.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29c64c5-7447-4c12-8594-a96be21188b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = bn_a(out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8d4c38-4d8b-40e7-9acc-30a49b4a5d93",
   "metadata": {},
   "source": [
    "### First RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3477c9ed-0cfa-42de-9903-47b602578df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = F.relu(out) \n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2170f11-7281-4422-9398-7734d9602d46",
   "metadata": {},
   "source": [
    "### Second convolution that downsamples using a stride of 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26adb1ff-55ee-4d5b-bc34-4e112ad1a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "c2_in_channel = n_embd\n",
    "c2_out_channel = n_embd\n",
    "c2_in_channel, out_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203d10a8-2a14-4105-8115-0c9af2f8643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "c2_kernal_height = 1\n",
    "c2_kernal_width = 3\n",
    "c2_stride_height = 1\n",
    "c2_stride_width = 2\n",
    "c2_padding_height = 0\n",
    "c2_padding_width = 1\n",
    "{'kernal': (c2_kernal_height, c2_kernal_width),\n",
    " 'stride': (c2_stride_height, c2_stride_width),\n",
    " 'padding': (c2_padding_height, c2_padding_width)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fef676c-3cbd-4c86-a53c-1dca20938567",
   "metadata": {},
   "outputs": [],
   "source": [
    "## weight layer for convolution (similar to linear, just more explicit)\n",
    "conv_stride2 = nn.Parameter(\n",
    "    torch.empty(c2_out_channel, c2_in_channel, c2_kernal_height, c2_kernal_width), \n",
    "    requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2e8415-6f2a-444b-9ead-d332a7309590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iniate rows as 0.1, 0.2, and 0.3 for easier view of the weight impact\n",
    "with torch.no_grad():\n",
    "    c2_pattern = torch.tensor([0.3,0.2,0.1]).view(1,1,1,c2_kernal_width).expand(conv_stride2.size()).clone()\n",
    "    conv_stride2.copy_(c2_pattern)\n",
    "conv_stride2.size(), conv_stride2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef61c77d-3200-482f-8ca8-5e94a66d0099",
   "metadata": {},
   "source": [
    "### Second convolution, see that the dimensions change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6cb01c-bd16-443f-adcc-8a4549189412",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c2_batch, c2_channel, c2_height, c2_width = out.size()\n",
    "out.size(), c2_batch, c2_channel, c2_height, c2_width, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e2e65-d7eb-47e8-a757-b6a181d7596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c2_c_khw = c2_channel*c2_kernal_height*c2_kernal_width\n",
    "\n",
    "c2_height_out = (c2_height + 2*c2_padding_height - 1*(c2_kernal_height-1) - 1)//c2_stride_height + 1   # = 1, \n",
    "c2_width_out = (c2_width + 2*c2_padding_width - 1*(c2_kernal_width-1) - 1)//c2_stride_width + 1   # = 4\n",
    "c2_L = c2_height_out * c2_width_out\n",
    "\n",
    "print(f'First Conv: width out {width_out}, height out {height_out}, final dimension ({batch},{c_khw},{L})')\n",
    "print(f'This  Conv: width out {c2_width_out}, height out {c2_height_out}, final dimension ({c2_batch},{c2_c_khw},{c2_L})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0772c91c-9af4-412f-9785-adc29ac3e96b",
   "metadata": {},
   "source": [
    "**notice** how the width is reduced in half,  this is cause the stride is 2, we'll have to deal with this when we do our residual step to match our input embedding with this output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bb4915-534b-46f0-8bc6-f290fe17da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "(c2_kernal_height, c2_kernal_width), (c2_padding_height, c2_padding_width), (c2_stride_height, c2_stride_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0c21f0-b930-4d43-ad0e-72ffbfa82925",
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c75c07f-688c-4405-abd1-977123fd3ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "c2_unfolded = F.unfold(out, \n",
    "\t\tkernel_size=(c2_kernal_height, c2_kernal_width),  # (1,3)\n",
    "\t\tpadding=(c2_padding_height, c2_padding_width), #(0,1)\n",
    "\t\tstride=(c2_stride_height, c2_stride_width))#(1,2)\n",
    "c2_unfolded.size() , c2_unfolded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fee877f-b612-4dcf-9c62-d7b78539414b",
   "metadata": {},
   "source": [
    "### Convolution dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5c5c52-4640-484c-b72f-a3c366f1f359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacks creates a 2-d matrix of `out_channelX rest` so `6*18` by stacking the weights we match the shape of \n",
    "conv_2_weigth = conv_stride2.view(c2_out_channel, -1) # [6,6,1,3] > [6,18]\n",
    "conv_2_weigth.size(), conv_2_weigth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44222855-5c5d-4d01-a9f9-a6cda20965bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [6, 18] dot product with [2, 18, 4] resulting in [2x6x4]\n",
    "# This will auto broadcast across each of the 2 batches (shared weigth) so it results in [2x6x8]\n",
    "out = conv_2_weigth @ c2_unfolded\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126da870-19af-41b7-aa8e-f0fcb0d2c635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert in the channel dimension to go back to 1/2 of [B,C,1,T] since we took a stride of 2\n",
    "out = out.view(c2_batch,c2_out_channel, c2_height_out, c2_width_out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c911dd3-47b6-4534-a7e2-f0af6ea7f7d7",
   "metadata": {},
   "source": [
    "### Batch Norm #2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a762c9-fd66-44ed-83e2-50838ecdac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_b = nn.BatchNorm2d(n_embd)   \n",
    "bn_b.weight, bn_b.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba14628-8817-48ed-87d2-2c580c995461",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = bn_b(out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84da273-76fb-498a-a127-30181a99f0d4",
   "metadata": {},
   "source": [
    "## Residual connection, \n",
    "bring in X, but need to convolute X to match Out dimension\n",
    "\n",
    "convolution with 1x1 kernal and a stride of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b908e0ed-fe54-45c8-848a-31860db746e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_in_channel = n_embd\n",
    "res_out_channel = n_embd\n",
    "res_in_channel, res_out_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe985974-0c06-4f6a-998f-28bad44946d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_kernal_height = 1\n",
    "res_kernal_width = 1\n",
    "res_stride_height = 1\n",
    "res_stride_width = 2\n",
    "res_padding_height = 0\n",
    "res_padding_width = 0\n",
    "{'kernal': (res_kernal_height, res_kernal_width),\n",
    " 'stride': (res_stride_height, res_stride_width),\n",
    " 'padding': (res_padding_height, res_padding_width)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbcf6b0-4bd5-4dd5-aa42-ea00864be1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## weight layer for convolution (similar to linear, just more explicit)\n",
    "res_conv_1x1 = nn.Parameter(\n",
    "    torch.empty(res_out_channel, res_in_channel, res_kernal_height, res_kernal_width), \n",
    "    requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6d368b-0930-425a-acb0-33a02bce26b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iniate rows as 0.1, 0.2, and 0.3 for easier view of the weight impact\n",
    "with torch.no_grad():\n",
    "    res_pattern = torch.tensor([0.05]).view(1,1,1,res_kernal_width).expand(res_conv_1x1.size()).clone()\n",
    "    res_conv_1x1.copy_(res_pattern)\n",
    "res_conv_1x1.size(), res_conv_1x1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab74a0cc-cf1e-4444-82ba-7d5be9a00e88",
   "metadata": {},
   "source": [
    "### Residual connection with 1x1 convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3493684-8e7e-4b75-a56b-45dc6e8e2b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_batch, res_channel, res_height, res_width = x.size()\n",
    "res_batch, res_channel, res_height, res_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1a069d-3c1e-4dda-9c67-0208b00c341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_c_khw = channel*res_kernal_height*res_kernal_width\n",
    "\n",
    "res_height_out = (height + 2*res_padding_height - 1*(res_kernal_height-1) - 1)//res_stride_height + 1   # = 1, \n",
    "res_width_out = (width + 2*res_padding_width - 1*(res_kernal_width-1) - 1)//res_stride_width + 1   # = 4\n",
    "res_L = res_height_out * res_width_out\n",
    "\n",
    "print(f'Second  Conv: width out {c2_width_out}, height out {c2_height_out}, final dimension ({c2_batch},{c2_c_khw},{c2_L})')\n",
    "print(f'This  Conv: width out {res_width_out}, height out {res_height_out}, final dimension ({batch},{res_c_khw},{res_L})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8438064-18c3-4af0-aae8-bbfdf293f118",
   "metadata": {},
   "source": [
    "**1/3 less but we'll have to add them together** we'll take care of this as we resize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673dfef5-e401-4ab9-bf95-2bd5c6ecac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_unfolded = F.unfold(x, \n",
    "\t\tkernel_size=(res_kernal_height, res_kernal_width),  # (1,1)\n",
    "\t\tpadding=(res_padding_height, res_padding_width), #(0,0)\n",
    "\t\tstride=(res_stride_height, res_stride_width))#(1,2)\n",
    "x_unfolded.size() , x_unfolded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a604be43-82a3-40e2-a5bc-939af199fa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacks creates a 2-d matrix of `out_channelX rest` so `6*18` by stacking the weights we match the shape of \n",
    "res_weigth = res_conv_1x1.view(res_out_channel, -1) # [6,6,1,1] > [6,6]\n",
    "res_weigth.size(), res_weigth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca05ebd-aa90-445b-9110-45782c2fd100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will auto broadcast across each of the 2 batches (shared weigth) so it results in [2x6x8]\n",
    "identity = res_weigth @ x_unfolded\n",
    "identity.size(), identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70de44a-41be-4941-ac9d-21c8ff8316f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert in the channel dimension to go back to [B,C,1,T/2] since we took a stride of 2\n",
    "identity = identity.view(res_batch,res_out_channel, res_height_out, res_width_out)\n",
    "identity.size(), identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c55f19-e328-40ab-a9b5-c833c66c53e4",
   "metadata": {},
   "source": [
    "## Residual connection sum\n",
    "see we now have the same size for our identity connection and our output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8325ea-c406-4937-8422-75111a581a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.size(), identity.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b339043-c914-46fe-8cac-e9a7201329a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = out + identity\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a0b7e7-59fa-4810-9697-fa594006ed3f",
   "metadata": {},
   "source": [
    "### Adaptive Average Pooling\n",
    "Applies a 2D adaptive average pooling over an input signal composed of several input planes.\n",
    "\n",
    "since we're treating this as 1 example, not 8, we now need to bring our token dimension down to a final single \"example\". this means squeezing down from a `[2,6,1,4]` to a `[2,6,1,1]`\n",
    "\n",
    "equivalent to It is equivalent to x.mean(dim=(2,3), keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0d304f-11ed-4de2-9f36-7e0c6ccdcb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "gap2d = nn.AdaptiveAvgPool2d((1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90b0895-5f15-4d63-9f47-0f44bef12942",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = gap2d(x)\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d232b03-eead-4665-b17c-0912c5d47228",
   "metadata": {},
   "source": [
    "### Remove our extra dimesion we added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43be7c21-86d8-4f7a-9293-f78e700c759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.squeeze(2)\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dec617-264f-4cae-b899-af0f05336899",
   "metadata": {},
   "source": [
    "### Flip our channel and context back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7967d7d-de6b-456e-8234-b96748df6b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.permute(0,2,1)\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9259a8-2457-442f-9137-0f8e18aebd3a",
   "metadata": {},
   "source": [
    "## Final Linear Projectionself.head = nn.Linear(n_embd, vocab_size, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceb13c6-b1c0-4e2b-80c4-e118affbfaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "torch.nn.init.constant_(lm_head.weight,0.01)\n",
    "lm_head.weight.size(), lm_head.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3544b98-ce88-4b1a-9043-bc1c324a67ac",
   "metadata": {},
   "source": [
    "### logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c78a02-212c-423a-b956-19ffa41ca820",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = lm_head(x)\n",
    "\n",
    "logits.shape, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc4f9b1-fde1-48fc-9fa9-f22809343bc6",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2476d669-b37e-4517-ac9a-095db4d9fae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_flat = y.view(-1)\n",
    "y_flat.shape, y_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff319dd-ae68-4660-84f1-9ee11e417eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_flat = logits.view(-1, logits.size(-1))\n",
    "logits_flat.shape, logits_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93007fd-b635-4066-9fb9-86d783f1abfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.cross_entropy(logits_flat, y_flat)\n",
    "loss.shape, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ede886e-1b8c-4a21-8e13-488fd198e536",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
