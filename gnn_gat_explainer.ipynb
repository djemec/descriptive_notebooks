{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a55a7864-bcb0-41e6-902b-8d162fd9bd62",
   "metadata": {},
   "source": [
    "# GNN GAT Explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eca57eb-1a17-4ce1-887d-6d08b36240bd",
   "metadata": {},
   "source": [
    "The goal of this notebook is to walk through Graph Attention Network (GATs) flavor of a graph neural network (GNN). GATs operate on graphs where nodes are entities  and edges encode relations. Like other attention based networks (e.g. GPT), GATs use attention layers to learn the importance of different relationships. A standard GAT layer builds 3 attention layers: 1 for the features, and 2 for each direction of connection for a node (out, back). The standard GAT typically does not use the graph's adjacency matrix (node neighbor map) and rather uses attention + masking to learn it. There are variants that take add in attention based on the adjacency to make it easier to learn.  A GAT stacks attention layers with ELU normalizations, requiring as many layers as hops that the model is trying to learn. Because GATs learn their networks they are preferable when neighbor quality is uneven or noisy and you want the model to learn which neighbors to trust instead of averaging them uniformly, like a GCn. They’re also better when relationships are asymmetric or context-dependent, since attention can weight edges differently in each direction and based on node features. Also, GATs offer more interpretable edge-level importance scores, which is valuable in domains like biology where explaining which genes or interactions drove a prediction matters. In this notebook we'll show both a traditional GAT layer and a signed GAT layer. to show how they adapt differently \n",
    "\n",
    "To help display how GATs work, we'll use a representation of a gene regulatory network where we have 2 specific cell types, a set of genes that are up/down regulated, and then a label for each sample to flag if it's cancerous or not. With our GAT, the goal will be to train it to take in a set of genes and how they're regulated and then predict the cell type as the *node task*, or node level prediction and whether it's cancerous as the *graph task* or graph level prediction. This dual goal will require us to balance 2 loss functions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e757d71e-e53e-40ea-85db-6c8229b1c79d",
   "metadata": {},
   "source": [
    "## Graph Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a39342f-3d6f-47ba-bbd2-8140bce5e506",
   "metadata": {},
   "source": [
    "We'll start with a common preprocessing step. Instead of a typical tokenizer, we have to create a numerical representation of our graph by enumerating the nodes and edges. This process first starts by creating a series of \"token-like\" IDS for our node vocabulary. In our example that becomes the gene and cell types.  We then use integers to map the relationship of the gene and cell types. This is where our **signed graph** starts since we use `+1` edges for up-regulation and `-1` for down-regulation.   As a result we end up with the following generated:\n",
    "1. **x_tokens** - a list of nodes\n",
    "2. **y_node** - per-node (gene) gene cell type labels. This uses a balance of the up-regulated and down-regulated genes and flags for each sample if the gene is more common with B-cells or T-cells.  We use a `-1` here to flag cells to ignore\n",
    "3. **y_graph** - per-graph (sample) cancer type label. We use `1` for cancerous and `0` for benign\n",
    "4. **a_list** - the nodes in our graph that link the different cell types and genes together.  We pool all the samples together into a large block-diagonal matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "736484bd-3859-4b69-9a1f-36c9eddce277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcbcbe30-349a-48d4-9053-2ab972162cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "benign = {\n",
    "    'Tcells':{'up': ['CD3D','LCK','ZAP70'],'down': ['CD19']},\n",
    "    'Bcells':{'up': ['CD19','LCK'],'down': ['CD3D','ZAP70']},\n",
    "  #  'Macrophages':{'up': ['CSF1R'],'down': ['CD3D','MS4A1','CD19']},\n",
    "}\n",
    "\n",
    "cancerous = {\n",
    "     'Tcells':{'up': ['ZAP70'],'down': ['CD19','CD3D','LCK']},\n",
    "     'Bcells':{'up': ['CD19','CD3D'],'down': ['ZAP70','LCK']},\n",
    "   # 'Macrophages':{'up': ['CSF1R'],'down': ['CD3D','MS4A1','CD19']},\n",
    "}\n",
    "\n",
    "# cancerous_2 = {\n",
    "#    'Tcells':{'up': ['CD3D','ZAP70'],'down': ['CD19','LCK']},\n",
    "#    'Bcells':{'up': ['CD19','LCK','CD3D'],'down': ['ZAP70']},\n",
    "#    # 'Macrophages':{'up': ['CSF1R'],'down': ['CD3D','MS4A1','CD19']},\n",
    "# }\n",
    "\n",
    "# cancerous_3 = {\n",
    "#     'Tcells':{'up': ['LCK'],'down': ['ZAP70','CD19','CD3D']},\n",
    "#     'Bcells':{'up': ['CD19','ZAP70'],'down': ['LCK','CD3D']},\n",
    "#    # 'Macrophages':{'up': ['CSF1R'],'down': ['CD3D','MS4A1','CD19']},\n",
    "# }\n",
    "graphs = [benign, cancerous] #, cancerous_2, cancerous_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5b348e6-4bde-4391-849a-cd787d83b397",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_types = ['Tcells','Bcells']\n",
    "genes = ['CD3D','LCK','ZAP70','CD19'] #only focused on genes present\n",
    "node_order = genes + [f'CT_{ct}' for ct in cell_types]   # genes first, then CT nodes\n",
    "gene_mask = torch.tensor([1,1,1,1,0,0], dtype=torch.bool)\n",
    "N = len(node_order) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cfd44a1-953e-4a90-8a1d-56bd122b068e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,\n",
       " {'CD3D': 0, 'LCK': 1, 'ZAP70': 2, 'CD19': 3, 'CT_Tcells': 4, 'CT_Bcells': 5})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_vocab(genes, cell_types):\n",
    "    toks = genes + [f'CT_{ct}' for ct in cell_types]\n",
    "    stoi = {t:i for i,t in enumerate(toks)}; itos = {i:t for t,i in stoi.items()}\n",
    "    return stoi, itos\n",
    "\n",
    "stoi, itos = build_vocab(genes, cell_types)\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "vocab_size, stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10be2b0f-4aca-4219-971a-7d9876007957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph_signed(spec):\n",
    "    N = len(node_order); \n",
    "    A_signed = torch.zeros(N, N)\n",
    "    def idx(n): \n",
    "        return node_order.index(n)\n",
    "    def add_edge(g, ct, s):\n",
    "        i, j = idx(g), idx(f\"CT_{ct}\")\n",
    "        A_signed[i,j] = s\n",
    "        A_signed[j,i] = s\n",
    "    for ct in cell_types:\n",
    "        for g in spec[ct].get('up', []):   \n",
    "            add_edge(g, ct, +1)\n",
    "        for g in spec[ct].get('down', []): \n",
    "            add_edge(g, ct, -1)\n",
    "    # labels for node task (0=T, 1=B) on genes only\n",
    "    y_node = torch.full((N,), -1, dtype=torch.long)\n",
    "    X = torch.tensor([stoi[n] for n in node_order], dtype=torch.long)\n",
    "    \n",
    "    # per-gene cell-type label: 0=Tcells, 1=Bcells (prefer UP, tie-break T→B, fallback from DOWN)\n",
    "    y_node = torch.full((N,), -1, dtype=torch.long)\n",
    "    for g in genes:\n",
    "        t_up = g in spec['Tcells'].get('up', [])\n",
    "        b_up = g in spec['Bcells'].get('up', [])\n",
    "        t_dn = g in spec['Tcells'].get('down', [])\n",
    "        b_dn = g in spec['Bcells'].get('down', [])\n",
    "        lab = None\n",
    "        if t_up and not b_up: \n",
    "            lab = 0\n",
    "        elif b_up and not t_up: \n",
    "            lab = 1\n",
    "        elif t_up and b_up: \n",
    "            lab = 0\n",
    "        else:\n",
    "            if t_dn and not b_dn: \n",
    "                lab = 1\n",
    "            elif b_dn and not t_dn: \n",
    "                lab = 0\n",
    "        if lab is not None: \n",
    "            y_node[idx(g)] = lab\n",
    "    return X, y_node, A_signed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59713212-5422-4580-8a4f-4aa49ed067a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([0, 1, 2, 3, 4, 5]), tensor([0, 1, 2, 3, 4, 5])],\n",
       " [tensor([ 0,  0,  0,  1, -1, -1]), tensor([ 1, -1,  0,  1, -1, -1])],\n",
       " [tensor([[ 0.,  0.,  0.,  0.,  1., -1.],\n",
       "          [ 0.,  0.,  0.,  0.,  1.,  1.],\n",
       "          [ 0.,  0.,  0.,  0.,  1., -1.],\n",
       "          [ 0.,  0.,  0.,  0., -1.,  1.],\n",
       "          [ 1.,  1.,  1., -1.,  0.,  0.],\n",
       "          [-1.,  1., -1.,  1.,  0.,  0.]]),\n",
       "  tensor([[ 0.,  0.,  0.,  0., -1.,  1.],\n",
       "          [ 0.,  0.,  0.,  0., -1., -1.],\n",
       "          [ 0.,  0.,  0.,  0.,  1., -1.],\n",
       "          [ 0.,  0.,  0.,  0., -1.,  1.],\n",
       "          [-1., -1.,  1., -1.,  0.,  0.],\n",
       "          [ 1., -1., -1.,  1.,  0.,  0.]])])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_list, y_node_list, a_list = [], [], []\n",
    "for spec in graphs:\n",
    "    X_i, y_i, A_i = make_graph_signed(spec)\n",
    "    x_list.append(X_i); y_node_list.append(y_i); a_list.append(A_i)\n",
    "\n",
    "x_list, y_node_list, a_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a32ecc-62b1-49dc-b086-f3446193cbdc",
   "metadata": {},
   "source": [
    "**x_tokens** - a list of nodes\n",
    "\n",
    "Notice that all nodes are present in each of our samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ff129f5-5109-4537-a284-fca7becbc618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6]),\n",
       " tensor([[0, 1, 2, 3, 4, 5],\n",
       "         [0, 1, 2, 3, 4, 5]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tokens = torch.stack(x_list)\n",
    "x_tokens.size(), x_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e78cf94-1c19-46a1-b2d5-263a103a06af",
   "metadata": {},
   "source": [
    "**y_node** - per-node (gene) gene cell type labels. This uses a balance of the up-regulated and down-regulated genes and flags for each sample. If the gene is more commonly upregulated with B-cells we flag it as `0` and if it's more commonly up-regulated in T-cells we flag it as `1`.  This evaluation is done per sample (seen here as per row). \n",
    "\n",
    "The last two columns are embeddings purely for cell type so we flag them as -1 to be masked during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6275918-276e-45ba-9532-b8f0c1f5e599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6]),\n",
       " tensor([[ 0,  0,  0,  1, -1, -1],\n",
       "         [ 1, -1,  0,  1, -1, -1]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_node = torch.stack(y_node_list)\n",
    "y_node.size(), y_node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6a8ff8-e05c-43d7-aad5-6d089e7b421a",
   "metadata": {},
   "source": [
    "**y_graph** - per-graph (sample) cancer type label. We use `1` for cancerous and `0` for benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67fe0752-f9b0-4dad-8a5f-e3e4fab3082c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2]), tensor([0, 1]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_graph = torch.tensor([0,1])  # 0=wild, 1=cancer\n",
    "y_graph.size(), y_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30046b6d-4bae-4820-9c77-537e861d37af",
   "metadata": {},
   "source": [
    "### Data Graph\n",
    "\n",
    "Since this is a GNN explainer, let's visualize actually how these graphs look. This is in essence the graph the model is looking at and learning how to read so that if it sees a new one, it can predict the properties we are minimizing loss on (cancerous, cell type). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b08438b-16e2-4c3a-b509-f7b29aedbb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_positions(genes, cell_types):\n",
    "    # left column (genes), right column (cell types)\n",
    "    gx, cx = 0.1, 0.9\n",
    "    gy = list(range(len(genes)-1, -1, -1))  # top-to-bottom\n",
    "    if len(cell_types) > 1:\n",
    "        # spread CTs across same vertical span as genes\n",
    "        span = (len(genes)-1)\n",
    "        cy = [span * (1 - i/(len(cell_types)-1)) for i in range(len(cell_types))]\n",
    "    else:\n",
    "        cy = [(len(genes)-1)/2]\n",
    "    pos = {g:(gx, gy[i]) for i,g in enumerate(genes)}\n",
    "    pos.update({f\"CT_{ct}\":(cx, cy[j]) for j,ct in enumerate(cell_types)})\n",
    "    return pos\n",
    "\n",
    "def draw_sample(ax, spec, title, genes, cell_types):\n",
    "    pos = compute_positions(genes, cell_types)\n",
    "    # draw nodes\n",
    "    for i,g in enumerate(genes):\n",
    "        x,y = pos[g]\n",
    "        ax.scatter([x],[y], s=300, marker='o', zorder=3)\n",
    "        ax.text(x-0.03, y, g, va='center', ha='right', fontsize=10)\n",
    "    for ct in cell_types:\n",
    "        name = f\"CT_{ct}\"\n",
    "        x,y = pos[name]\n",
    "        ax.scatter([x],[y], s=350, marker='s', zorder=3)\n",
    "        ax.text(x+0.03, y, ct, va='center', ha='left', fontsize=10)\n",
    "    # draw edges: solid = upregulated (+1), dashed = downregulated (-1)\n",
    "    for ct in cell_types:\n",
    "        ct_name = f\"CT_{ct}\"\n",
    "        for g in spec[ct].get('up', []):\n",
    "            if g in genes:\n",
    "                x1,y1 = pos[g]; x2,y2 = pos[ct_name]\n",
    "                ax.plot([x1,x2],[y1,y2], linestyle='-', linewidth=2, zorder=2)\n",
    "        for g in spec[ct].get('down', []):\n",
    "            if g in genes:\n",
    "                x1,y1 = pos[g]; x2,y2 = pos[ct_name]\n",
    "                ax.plot([x1,x2],[y1,y2], linestyle='--', linewidth=2, zorder=2)\n",
    "\n",
    "    # aesthetics\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim(0.0, 1.0)\n",
    "    ys = list(range(len(genes)))\n",
    "    ax.set_ylim(-0.5, len(genes)-0.5)\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    for spine in ax.spines.values(): spine.set_visible(False)\n",
    "\n",
    "    # legend (solid=up, dashed=down)\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_lines = [\n",
    "        Line2D([0],[0], linestyle='-', linewidth=2, label='Upregulated'),\n",
    "        Line2D([0],[0], linestyle='--', linewidth=2, label='Downregulated'),\n",
    "    ]\n",
    "    ax.legend(handles=legend_lines, loc='upper center', frameon=False, fontsize=9)\n",
    "\n",
    "# ----- draw all four samples -----\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "samples = [\n",
    "    ('Sample 1 [benign]',    benign),\n",
    "    ('Sample 2 [cancerous]',    cancerous),\n",
    "    #('Sample 3 [cancerous]',  cancerous_2),\n",
    "    #('Sample 4 [cancerous]',  cancerous_3),\n",
    "]\n",
    "for ax, (name, spec) in zip(axes.ravel(), samples):\n",
    "    draw_sample(ax, spec, name, genes, cell_types)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99e6534-842b-499f-888b-dddb402d03d6",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04587318-bfc6-43d1-a95c-83f1d9ff4f4c",
   "metadata": {},
   "source": [
    "A machine learning model forward pass now uses the data prep information, runs several layers of linear algebra on it, and then \"predicts\" the probability of our different tasks, in this case the cell type based on the node and whether a graph is cancerous. When it is noisy (like you will see in this example), this process results in gibberish.  The training process changes the noise to pattern during the \"backward pass\" as you'll see. We'll show 3 steps that are focused on training:\n",
    "1. **Data Loading** - this step pulls from the raw data enough examples and batches to complete a forward pass and loss calculation.  If the model is inference only, this step is replaced with taking in the inference input and preparing it similarly as the forward pass. \n",
    "2. **Forward Pass** - using the data and the model architecture we run a prediction for the tokens. When training we also compare against the expected to get loss, but in inference, we use the logits to complete the inference task.\n",
    "3. **Back Propagation, aka Backward Pass & Training** - using differentials we can understand what parameters most drive the difference between forward pass' impact on its prediction versus what is actually right based on the data loading step. We compare this based on the loss function and use the partial derivative gradients to make very minor adjustments to the impactful parameters with the hope it improves future predictions.\n",
    "\n",
    "After our back prop, we'll show a final **Forward Pass** with the updated weights we did in #3 and then convert those final weights to a **Model Inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27430abf-ee0c-4cdf-af24-5634a09a2e92",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267f237e-d685-41d6-b403-1712be222fbe",
   "metadata": {},
   "source": [
    "To start, we need to get enough data to run the forward and backward passes.  Since our total dataset in a real experiment is likely too big to be held in memory all at once in real practice, we will read just enough file information into memory so that we can run the passes, leaving memory and compute to be used on the passes instead of static data holding. \n",
    "To start, we have to identify the batch size and the model context length to determine how much data we need.  Consequently, these dimensions also form 2 of the 3 dimensions in the initial matrix.\n",
    "- **Batch Size (B)** - This is the number of examples you'll train on in a single pass. \n",
    "- **Number of Nodes (N)** - This is basically the \"context length\" for a GNN.  This is the max number of nodes that a model can use in a pass.\n",
    "\n",
    "Beyond these, in a GCN, the depth also controls how much context, or complexity, can be learned. This is because each GCN layer learns 1 hop, or 1 relationship of neighbors. This means that after $L$ layers, a model can learn $L$-hops worth of context.\n",
    "\n",
    "In our case we'll set our batch to be our 4 examples, and nodes to the nodes we have configured, 6. As we walk through you'll also see our GCN will have 2 layers to model 2 network hops: gene > cell type > other genes.\n",
    "\n",
    "We'll prepare 2 sets of data. Our **Inputs** will be the `x_token`, or our list of nodes for each example, and `a_list`, our list of node connections.  Our **Outputs** will be `y_node`, our node level cell type identification, and `y_graph`, the graph level cancerous identification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3157770a-afb9-424b-be9d-50cba46f0dd9",
   "metadata": {},
   "source": [
    "**x_tokens** — list of nodes for each example. Each entry is an integer token id for the node at that position in node_order (e.g.`['CD3D','LCK','ZAP70','CD19','CT_Tcells','CT_Bcells']`).  In our case you'll notice that each example contains all the nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1cf69c7-8f81-4d86-9228-2b69c0ded301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " 6,\n",
       " tensor([[0, 1, 2, 3, 4, 5],\n",
       "         [0, 1, 2, 3, 4, 5]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_batch, N_nodes = x_tokens.shape\n",
    "B_batch, N_nodes, x_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4b9aea-2c17-4fa2-bc63-a4c1135eea2c",
   "metadata": {},
   "source": [
    "**y_node** - per-gene label for cell-type. `0` for T-cell marker and tie, `1` for B-cell marker. As a reminder, this is an aggregation of the up-regulated and down-regulated genes. We focus on which cell type has the gene up-regulated and, if both have it, we use 0. There are ways to handle ties better but we won't get into it.  Since `y_node` also includes the cell types, we'll use -1 to mask them as ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fe5efc3-97fe-4638-b04b-9eb29d7fb42d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6]),\n",
       " tensor([[ 0,  0,  0,  1, -1, -1],\n",
       "         [ 1, -1,  0,  1, -1, -1]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_node.size(), y_node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986b92da-0888-444a-98c2-df2d45389b48",
   "metadata": {},
   "source": [
    "**y_graph** - per-graph label to determine if an example is cancerous. `0` is for benign and `1` is for cancerous.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e984511-b313-42ac-bc7c-e88cf27399d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2]), tensor([0, 1]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_graph.size(), y_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec818687-4747-4e8c-bb10-a400891d7619",
   "metadata": {},
   "source": [
    "**a_list** - The relationships for each of our cells.  You'll notice here that only the last two rows and columns are used.  In this tensor `+1` is for **upregulated** gene per cell type and `-1` is for **downregulated**. `0` is for not in the network.  We also include here a **Gene_mask** that will act in our loss function as a flag to suppress the gene x gene portions of the matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45348c5b-90d0-481f-bbf4-ed5bb93e94d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ True,  True,  True,  True, False, False]),\n",
       " [tensor([[ 0.,  0.,  0.,  0.,  1., -1.],\n",
       "          [ 0.,  0.,  0.,  0.,  1.,  1.],\n",
       "          [ 0.,  0.,  0.,  0.,  1., -1.],\n",
       "          [ 0.,  0.,  0.,  0., -1.,  1.],\n",
       "          [ 1.,  1.,  1., -1.,  0.,  0.],\n",
       "          [-1.,  1., -1.,  1.,  0.,  0.]]),\n",
       "  tensor([[ 0.,  0.,  0.,  0., -1.,  1.],\n",
       "          [ 0.,  0.,  0.,  0., -1., -1.],\n",
       "          [ 0.,  0.,  0.,  0.,  1., -1.],\n",
       "          [ 0.,  0.,  0.,  0., -1.,  1.],\n",
       "          [-1., -1.,  1., -1.,  0.,  0.],\n",
       "          [ 1., -1., -1.,  1.,  0.,  0.]])])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_mask, a_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727058d6-eafd-4b74-88fd-49d4219d30eb",
   "metadata": {},
   "source": [
    "### Data Loading - Blocked Diagonal Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663c8ee9-64e4-4c90-a9a3-69049cbccafc",
   "metadata": {},
   "source": [
    "Since we are trying to learn from all the examples in our batch, we want a uniform tensor of the network connections to learn from. We do this using a block diagonal join that creates a large tensor out of the inputs by simply sliding each new tensor to start at `[i+1,j+1]`.  In our case this results in a `[12,12]` matrix since we have 2 examples with 6 nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba483d33-38e5-4b3f-9bc1-7514aeb1a85c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12]),\n",
       " tensor([[ 0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 1.,  1.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [-1.,  1., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  1.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1., -1.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  1.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0., -1., -1.,  1., -1.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  1., -1., -1.,  1.,  0.,  0.]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_blk = torch.block_diag(*a_list)\n",
    "a_blk.size(), a_blk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e81275-ae8f-4968-b325-1deab4139fa4",
   "metadata": {},
   "source": [
    "## Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a478cb-44ef-48c6-8363-bca483005b15",
   "metadata": {},
   "source": [
    "<img src=\"explainer_screenshots/gat/full_network.png\" width=\"300\">\n",
    "\n",
    "During training, in the GAT we've built, the forward pass takes in the set of nodes per example and a signed network map and uses this to predict the cell type by node and the cancerous level of the graph. This can be viewed as two different classification tasks at the head level where the head is different for each task. Here the \"batch\" and \"token\" dimensions are one in the same, being that we just have 1 dimension to signify our examples, which we'll call batch.  We could layer in batch based learning but it would make this notebook too complicated and lengthy. \n",
    "\n",
    "Our walkthrough of the forward pass is focused on training where we'll pass in the input nodes `x_tokens` and signed graph `a_blk`, carry that input through the layers, and generate 2 matrices of the probability for the node and graph level prediction. These predictions will be two different sets of `logits`. During the forward pass, after embedding our input nodes we'll pass through two different GAT layers to show how a signed layer and a typical layer behave and to allow the model to learn from 2 hops instead of just 1. \n",
    "\n",
    "At the end of the forward pass we then compare the probability in the logits to the actual next token in `y_node, y_graph` and calculate `loss` based on the difference. You'll see that we calculate the loss on each head, then sum it for a final loss (and so that we can distribute across both pathways in backprop). This difference is what we'll then use in the backprop/training steps.  \n",
    "\n",
    "*Note that we will do some layer initialization to simplify following along.  In reality, layers are often initialized to normal distribution with some adjustments made for parameter sizes, called Xaviar Uniform, to keep the weights properly noisy.  Xaviar Uniform distributes close to 0 and  is designed so that variance of activations and gradients is preserved across layers for linear / tanh / sigmoid-style networks.  We will not cover initialization in this series*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c7257-0fae-48cb-8a5c-193e4e205a48",
   "metadata": {},
   "source": [
    "**Attention Heads**\n",
    "\n",
    "The beauty with attention is the use of multiple heads to allow for learning of different complex concepts.  The more complex concepts you think you want to learn, the more heads you will want to include.  This scaling allows for a broader generalization during training and usage.  \n",
    "\n",
    "Note that the embedding channels need to be cleanly divisible into the heads to allow for flow-through. This is different than in a GPT where each attention head has the same number of channels as the embedding layer.  We're mainly doing it differently to show that both structures work, to make gradient flow easier to aid in explainability, and to reduce the footprint used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37f358c2-7943-4b3e-8ac6-0909ca97e243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_embd= 6\n",
    "heads = 2\n",
    "head_dim = n_embd // heads\n",
    "depth = 1 # just a single hop\n",
    "head_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bddabbf-fbc0-475f-a367-cf7add671d07",
   "metadata": {},
   "source": [
    "### Embedding Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b9e16e-b8b9-4cdf-9248-7887767aeb6a",
   "metadata": {},
   "source": [
    "<img src=\"explainer_screenshots/gat/input_layer.png\" width=\"400\">\n",
    "\n",
    "We'll first create an initial **embedding layer** for our sample level node tokens. Recall that this is the layer that will add the second dimension to our node list. We start with supplying only the nodes. In parallel our adjacency graphs will also be kept on the side as they'll be inputs to the next layer. Generally, by using an embedding on the node, we give the graph a chance to learn how important the different nodes are per example and how to use them, and the node order, in our output prediction task.  We'll also use a small embedding dimension to allow the network to learn a deeper representation of our nodes. After doing the embedding, we'll then remove our batch dimensions for the remaining training to simplify our training.  \n",
    "\n",
    "We'll start by initializing the weight to a sliding scale so that we can quickly see each layer's impact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04abaee3-cd77-4a4f-a165-7918f675ff26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 6]),\n",
       " Parameter containing:\n",
       " tensor([[0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060],\n",
       "         [0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070],\n",
       "         [0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080],\n",
       "         [0.0040, 0.0050, 0.0060, 0.0070, 0.0080, 0.0090],\n",
       "         [0.0050, 0.0060, 0.0070, 0.0080, 0.0090, 0.0100],\n",
       "         [0.0060, 0.0070, 0.0080, 0.0090, 0.0100, 0.0110]], requires_grad=True))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_emb = nn.Embedding(vocab_size, n_embd)\n",
    "with torch.no_grad(): # initilize to W[i,j] = 0.001*(1+i+j) for easy following \n",
    "    vs, d = vocab_size, n_embd\n",
    "    rows = torch.arange(vs).unsqueeze(1)  # (vs,1)\n",
    "    cols = torch.arange(d).unsqueeze(0)  # (1,d)\n",
    "    pattern = 0.001*(1 + rows + cols)  # W[i,j] = 0.001*(1+i+j)\n",
    "    tok_emb.weight.copy_(pattern)\n",
    "tok_emb.weight.size(), tok_emb.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98abc341-ce9a-4d3e-a306-4a8e14bd3444",
   "metadata": {},
   "source": [
    "**Embedding Projection**\n",
    "\n",
    "Remember that each of our samples includes all the nodes so we expect that all the weights will be used repeatedly for each sample.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d211712-b32c-4e63-b194-347c9e116707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 6]),\n",
       " tensor([[[0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060],\n",
       "          [0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070],\n",
       "          [0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080],\n",
       "          [0.0040, 0.0050, 0.0060, 0.0070, 0.0080, 0.0090],\n",
       "          [0.0050, 0.0060, 0.0070, 0.0080, 0.0090, 0.0100],\n",
       "          [0.0060, 0.0070, 0.0080, 0.0090, 0.0100, 0.0110]],\n",
       " \n",
       "         [[0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060],\n",
       "          [0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070],\n",
       "          [0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080],\n",
       "          [0.0040, 0.0050, 0.0060, 0.0070, 0.0080, 0.0090],\n",
       "          [0.0050, 0.0060, 0.0070, 0.0080, 0.0090, 0.0100],\n",
       "          [0.0060, 0.0070, 0.0080, 0.0090, 0.0100, 0.0110]]],\n",
       "        grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tok_emb(x_tokens)\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76222510-e2f3-4ba8-a0a2-62bc5fb4b271",
   "metadata": {},
   "source": [
    "**Remove Batch**\n",
    "\n",
    "Now we'll remove the batch for further training and learning. If you recall we combined our networks using blocked diagonals so the batch removal now aligns the \"sample\"/\"node\" dimension  with the dimension in the network tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3d6ca48-56aa-4d04-babf-2cdc139b0be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 6]),\n",
       " tensor([[0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060],\n",
       "         [0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070],\n",
       "         [0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080],\n",
       "         [0.0040, 0.0050, 0.0060, 0.0070, 0.0080, 0.0090],\n",
       "         [0.0050, 0.0060, 0.0070, 0.0080, 0.0090, 0.0100],\n",
       "         [0.0060, 0.0070, 0.0080, 0.0090, 0.0100, 0.0110],\n",
       "         [0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060],\n",
       "         [0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070],\n",
       "         [0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080],\n",
       "         [0.0040, 0.0050, 0.0060, 0.0070, 0.0080, 0.0090],\n",
       "         [0.0050, 0.0060, 0.0070, 0.0080, 0.0090, 0.0100],\n",
       "         [0.0060, 0.0070, 0.0080, 0.0090, 0.0100, 0.0110]],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.view(B_batch*N_nodes,n_embd) # remove batch\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eb2ba0-e6d1-49ae-ac16-11b54a94ecb3",
   "metadata": {},
   "source": [
    "### GAT Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a60bfd6-b215-4725-b6ae-d49a8901e6e5",
   "metadata": {},
   "source": [
    "<img src=\"explainer_screenshots/gat/block_details.png\" width=\"400\">\n",
    "\n",
    "To support our two hops of learning (Cell type > gene > cell type) we stack two signed graph attention layers with a nonlinearity to learn increasingly expressive node features. We have two different types of attention layers: one that includes the signed graph attention and one closer to a vanilla GAT that only has attention on the nodes. \n",
    "\n",
    "\n",
    "Each graph attention layer first projects node embeddings with a shared linear map per head, then computes edge-wise attention scores that depend on both endpoint features and, if signed, the sign of the interaction. For each head $h$, we form\n",
    "$$\n",
    "e_{ij}^{h}=\\text{LeakyReLU}\\big((a_{src}^{h})^\\top W^{h}x_i + (a_{dst}^{h})^\\top W^{h}x_j\\big),\n",
    "$$\n",
    "If the GAT is signed an addition $a_{sign}^{h} s_{ij}$ is added. \n",
    "\n",
    "After this, we mask non-edges, and apply a softmax over neighbors $j$ to get attention weights $\\alpha_{ij}^{h}$. The updated node state is then a signed, attention-weighted aggregation of neighbor “values”\n",
    "\n",
    "$$\n",
    "h_i^{\\prime(h)}=\\sum_j \\alpha_{ij}^{h} W^{h}x_j\n",
    "$$\n",
    "\n",
    "If the GAT is signed, we also multiply by our signed edges $s_{ij}$ again. We concatenate the multi-head outputs back into a single embedding.  In this structure, the attention Query, Key, and Value can be thought of as\n",
    "* $Query = (a_{src}^{h})^\\top W^{h}x_i$\n",
    "* $Key = (a_{dst}^{h})^\\top W^{h}x_j$\n",
    "* $Value = W^{h}x_j$\n",
    "\n",
    "We apply dropout to node features and attention coefficients to regularize both representations and neighbor selection. We use an LeakyReLU nonlinearity inside each GAT and ELU between GAT to preserve informative negative signals, and add residual connections around stacked attention layers so information can bypass noisy edges, allowing deeper models that remain expressive while still respecting up vs. down regulation patterns encoded by the signed edges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf8de80-8021-458f-aea0-d30a6fa5e7ed",
   "metadata": {},
   "source": [
    "#### GAT Block - Signed First Attention Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c023339-d911-4524-a4fa-3d9f545ad7bc",
   "metadata": {},
   "source": [
    "<img src=\"explainer_screenshots/gat/signed_gat.png\" width=\"400\">\n",
    "\n",
    "Our first GAT layer is signed so it includes the attention for the signed network to the standard GAT structure. This means that we perform the following:\n",
    "$$\n",
    "e_{ij}^{h}=\\text{LeakyReLU}\\big((a_{src}^{h})^\\top W^{h}x_i + (a_{dst}^{h})^\\top W^{h}x_j + a_{sign}^{h} \\odot s_{ij}\\big),\n",
    "$$\n",
    "\n",
    "The addition of the signed edge attention allows the model to use the known graph edges without having to learn them in the attention alone. This allows the model to learn with less data.  \n",
    "\n",
    "For this GAT layer, we'll \n",
    "1. Run feature dropout\n",
    "2. Calculate our linear projection and node attention\n",
    "3. Calculate the signed edge attention\n",
    "4. Run nonlinearity, masking, and softmax\n",
    "5. Calculate the Hadamard product of the attention and the signed network map.\n",
    "6. Take the dot product of the edge attention with the linear projection creating a learned network map.\n",
    "7. A final ELU nonlinearity to support positive and negative values. \n",
    "\n",
    "The inclusion of the Hadamard product allows for gradients to flow down to individual elements of the network map allowing for more precise learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d27d36-de15-451a-a34e-cc45f293abdf",
   "metadata": {},
   "source": [
    "##### Feature Dropout\n",
    "\n",
    "We'll start with feature level dropout in the incoming embedding.  Dropout will randomly zero out any value effectively removing that specific node from impacting prediction. Since this is Bernoulli based dropout, in addition to zeroing out weights the surviving entries are scaled by $1/(1-p)$.  Feature level dropout stops the model from overfitting to very specific feature patterns and encourages robustness in the learned representations before attention even happens. You can think of it as applying the following: “don’t over-trust any single feature dimension.” \n",
    "\n",
    "Since this introduces `0` you can quickly see the dropout's impact on the embeddings.  You'll also see a 10% increase in the value of each row due to the normalization that dropout does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c467044-9de1-434d-9498-4d02f321c74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dropout=0.10\n",
    "gat1_fdrop = nn.Dropout(feat_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a18d23a3-fb31-4b91-9e0b-5fc6d1c2c857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 6]),\n",
       " tensor([[0.0011, 0.0022, 0.0033, 0.0044, 0.0056, 0.0067],\n",
       "         [0.0000, 0.0033, 0.0044, 0.0056, 0.0067, 0.0078],\n",
       "         [0.0033, 0.0044, 0.0056, 0.0067, 0.0078, 0.0089],\n",
       "         [0.0044, 0.0056, 0.0067, 0.0000, 0.0089, 0.0100],\n",
       "         [0.0056, 0.0067, 0.0078, 0.0089, 0.0100, 0.0111],\n",
       "         [0.0067, 0.0078, 0.0089, 0.0000, 0.0111, 0.0122],\n",
       "         [0.0011, 0.0022, 0.0033, 0.0044, 0.0056, 0.0067],\n",
       "         [0.0000, 0.0033, 0.0044, 0.0000, 0.0067, 0.0078],\n",
       "         [0.0000, 0.0044, 0.0056, 0.0067, 0.0078, 0.0089],\n",
       "         [0.0044, 0.0056, 0.0067, 0.0078, 0.0089, 0.0100],\n",
       "         [0.0056, 0.0067, 0.0078, 0.0000, 0.0100, 0.0111],\n",
       "         [0.0067, 0.0078, 0.0089, 0.0100, 0.0111, 0.0000]],\n",
       "        grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_drop = gat1_fdrop(x)\n",
    "x_drop.size(), x_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b018776e-ac01-4119-b2e7-9d583c2ceaed",
   "metadata": {},
   "source": [
    "##### Per-Head Linear Projection\n",
    "\n",
    "Our first step is to a multi-headed linear projection of the node features.  We create a separate weighted matrix per head so that each head can learn concepts independently.  This slices the embedding space into learned sub-spaces for attention to interact with. This subspace is at the node level allowing the attention to interact with different weights of each node when building connections. \n",
    "\n",
    "We'll initialize both heads at the same sliding weight. Because of this you'll see the two heads result in the same product and each channel scales the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80c8d12f-b80a-402e-8e2e-6a92dfb894b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 3]),\n",
       " Parameter containing:\n",
       " tensor([[[0.1000, 0.2000, 0.3000],\n",
       "          [0.1000, 0.2000, 0.3000],\n",
       "          [0.1000, 0.2000, 0.3000],\n",
       "          [0.1000, 0.2000, 0.3000],\n",
       "          [0.1000, 0.2000, 0.3000],\n",
       "          [0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "         [[0.1000, 0.2000, 0.3000],\n",
       "          [0.1000, 0.2000, 0.3000],\n",
       "          [0.1000, 0.2000, 0.3000],\n",
       "          [0.1000, 0.2000, 0.3000],\n",
       "          [0.1000, 0.2000, 0.3000],\n",
       "          [0.1000, 0.2000, 0.3000]]], requires_grad=True))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gat1_attn_w = nn.Parameter(torch.empty(heads, n_embd, head_dim))\n",
    "with torch.no_grad(): \n",
    "    vs, d = n_embd, head_dim\n",
    "    rows = torch.arange(vs).unsqueeze(1)  # (vs,1)\n",
    "    cols = torch.arange(d).unsqueeze(0)  # (1,d)\n",
    "    pattern = 0.1*(1 + cols)  # W[i,j] = 0.001*(1+i+j)\n",
    "    gat1_attn_w.copy_(pattern)\n",
    "gat1_attn_w.size(), gat1_attn_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a44f6030-b424-4489-9001-6d76bc86d832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 12, 3]),\n",
       " tensor([[[0.0023, 0.0047, 0.0070],\n",
       "          [0.0028, 0.0056, 0.0083],\n",
       "          [0.0037, 0.0073, 0.0110],\n",
       "          [0.0036, 0.0071, 0.0107],\n",
       "          [0.0050, 0.0100, 0.0150],\n",
       "          [0.0047, 0.0093, 0.0140],\n",
       "          [0.0023, 0.0047, 0.0070],\n",
       "          [0.0022, 0.0044, 0.0067],\n",
       "          [0.0033, 0.0067, 0.0100],\n",
       "          [0.0043, 0.0087, 0.0130],\n",
       "          [0.0041, 0.0082, 0.0123],\n",
       "          [0.0044, 0.0089, 0.0133]],\n",
       " \n",
       "         [[0.0023, 0.0047, 0.0070],\n",
       "          [0.0028, 0.0056, 0.0083],\n",
       "          [0.0037, 0.0073, 0.0110],\n",
       "          [0.0036, 0.0071, 0.0107],\n",
       "          [0.0050, 0.0100, 0.0150],\n",
       "          [0.0047, 0.0093, 0.0140],\n",
       "          [0.0023, 0.0047, 0.0070],\n",
       "          [0.0022, 0.0044, 0.0067],\n",
       "          [0.0033, 0.0067, 0.0100],\n",
       "          [0.0043, 0.0087, 0.0130],\n",
       "          [0.0041, 0.0082, 0.0123],\n",
       "          [0.0044, 0.0089, 0.0133]]], grad_fn=<CloneBackward0>))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hproj = x_drop @ gat1_attn_w\n",
    "Hproj.size(), Hproj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf228036-14d6-48b1-9da2-b34e29f62b29",
   "metadata": {},
   "source": [
    "#####  Linear Node Attention\n",
    "\n",
    "Our linear Node attention now takes the per-head project and starts creating edge-specific weights between each pair of connected nodes. This is where our GAT layer begins to learn the adjacency importance. We still keep separation by head so that each head can learn its own relationships. For each head, it scores a builds potential edges edge $i \\leftarrow j$ by applying a learned vector to the destination features (thought of as a key) and another to the source features (thought of as a value), then adding them to get a compatibility logit $e_{ij}^{h}$. These learned weights represent “how much should node $i$ listen to node $j$ for this head.” This makes neighbor influence data-driven and asymmetric: different heads can specialize to different patterns (e.g. strong T-cell–like signatures, aberrant cancer patterns), and each node can selectively amplify or suppress specific neighbors rather than averaging them uniformly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8874f1-82f6-4852-9e5b-dfb2e834e4e8",
   "metadata": {},
   "source": [
    "**Source Node (Query)**\n",
    "\n",
    "We'll start with the source first. We'll initialize the heads so that the second head is double the first.  As a result you can see that the output of this shows the second head as double the first. We will also initialize with consistent weights across the head dimension so that we can see in backprop how the model changes the importance of the different dimensions thereby building a network representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85cc0b71-5536-4361-9b31-b3a8f1a0724e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 1]),\n",
       " Parameter containing:\n",
       " tensor([[[0.1000],\n",
       "          [0.1000],\n",
       "          [0.1000]],\n",
       " \n",
       "         [[0.2000],\n",
       "          [0.2000],\n",
       "          [0.2000]]], requires_grad=True))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gat1_attn_src = nn.Parameter(torch.empty(heads, head_dim, 1))\n",
    "with torch.no_grad(): \n",
    "    src_pattern = torch.tensor([[[0.1],[0.1],[0.1]],[[0.2],[0.2],[0.2]]])\n",
    "    gat1_attn_src.copy_(src_pattern)\n",
    "gat1_attn_src.size(), gat1_attn_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d80cb9ed-9564-4fc8-a857-9545411d78d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 12, 1]),\n",
       " tensor([[[0.0014],\n",
       "          [0.0017],\n",
       "          [0.0022],\n",
       "          [0.0021],\n",
       "          [0.0030],\n",
       "          [0.0028],\n",
       "          [0.0014],\n",
       "          [0.0013],\n",
       "          [0.0020],\n",
       "          [0.0026],\n",
       "          [0.0025],\n",
       "          [0.0027]],\n",
       " \n",
       "         [[0.0028],\n",
       "          [0.0033],\n",
       "          [0.0044],\n",
       "          [0.0043],\n",
       "          [0.0060],\n",
       "          [0.0056],\n",
       "          [0.0028],\n",
       "          [0.0027],\n",
       "          [0.0040],\n",
       "          [0.0052],\n",
       "          [0.0049],\n",
       "          [0.0053]]], grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_src = Hproj @ gat1_attn_src\n",
    "e_src.size(), e_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e184327-1563-432e-ba3e-58078762683a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 1, 2]),\n",
       " tensor([[[0.0014, 0.0028]],\n",
       " \n",
       "         [[0.0017, 0.0033]],\n",
       " \n",
       "         [[0.0022, 0.0044]],\n",
       " \n",
       "         [[0.0021, 0.0043]],\n",
       " \n",
       "         [[0.0030, 0.0060]],\n",
       " \n",
       "         [[0.0028, 0.0056]],\n",
       " \n",
       "         [[0.0014, 0.0028]],\n",
       " \n",
       "         [[0.0013, 0.0027]],\n",
       " \n",
       "         [[0.0020, 0.0040]],\n",
       " \n",
       "         [[0.0026, 0.0052]],\n",
       " \n",
       "         [[0.0025, 0.0049]],\n",
       " \n",
       "         [[0.0027, 0.0053]]], grad_fn=<PermuteBackward0>))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_src = e_src.permute(1, 2, 0)\n",
    "e_src.size(), e_src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2767adf-e6e3-4227-ab61-70c6200d9814",
   "metadata": {},
   "source": [
    "**Destination Node (Key)**\n",
    "\n",
    "Next we'll build the destination attention. We'll initialize the heads so that the first head is 1.5x the second and that both heads are negative.  As a result you can see the ratio is maintained for the destination and you'll see that we get negative weights, signaling in our case downregulation being important. We are doing this to help show that both positive and negative weights are useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2d7572c-fa56-417d-8613-3a59bb39c8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 1]),\n",
       " Parameter containing:\n",
       " tensor([[[-0.3000],\n",
       "          [-0.3000],\n",
       "          [-0.3000]],\n",
       " \n",
       "         [[-0.2000],\n",
       "          [-0.2000],\n",
       "          [-0.2000]]], requires_grad=True))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gat1_attn_dst = nn.Parameter(torch.empty(heads, head_dim, 1))\n",
    "with torch.no_grad(): \n",
    "    dst_pattern = torch.tensor([[[-0.3],[-0.3],[-0.3]],[[-0.2],[-0.2],[-0.2]]])\n",
    "    gat1_attn_dst.copy_(dst_pattern)\n",
    "gat1_attn_dst.size(), gat1_attn_dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c746cc1-3415-48ed-a8a1-78234e161735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 12, 1]),\n",
       " tensor([[[-0.0042],\n",
       "          [-0.0050],\n",
       "          [-0.0066],\n",
       "          [-0.0064],\n",
       "          [-0.0090],\n",
       "          [-0.0084],\n",
       "          [-0.0042],\n",
       "          [-0.0040],\n",
       "          [-0.0060],\n",
       "          [-0.0078],\n",
       "          [-0.0074],\n",
       "          [-0.0080]],\n",
       " \n",
       "         [[-0.0028],\n",
       "          [-0.0033],\n",
       "          [-0.0044],\n",
       "          [-0.0043],\n",
       "          [-0.0060],\n",
       "          [-0.0056],\n",
       "          [-0.0028],\n",
       "          [-0.0027],\n",
       "          [-0.0040],\n",
       "          [-0.0052],\n",
       "          [-0.0049],\n",
       "          [-0.0053]]], grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_dst = Hproj @ gat1_attn_dst\n",
    "e_dst.size(), e_dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6eb761a-f640-43a4-ba20-f479a92244cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 12, 2]),\n",
       " tensor([[[-0.0042, -0.0028],\n",
       "          [-0.0050, -0.0033],\n",
       "          [-0.0066, -0.0044],\n",
       "          [-0.0064, -0.0043],\n",
       "          [-0.0090, -0.0060],\n",
       "          [-0.0084, -0.0056],\n",
       "          [-0.0042, -0.0028],\n",
       "          [-0.0040, -0.0027],\n",
       "          [-0.0060, -0.0040],\n",
       "          [-0.0078, -0.0052],\n",
       "          [-0.0074, -0.0049],\n",
       "          [-0.0080, -0.0053]]], grad_fn=<PermuteBackward0>))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_dst = e_dst.permute(2, 1, 0)\n",
    "e_dst.size(), e_dst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46beb74a-a8ac-431d-a876-f6ac7afd3930",
   "metadata": {},
   "source": [
    "**Combine Node Directions Together**\n",
    "\n",
    "Now we'll need to combine our two attention weights together.  This combination creates `[12,12]` weights creating the dimension necessary to learn the network that we have encoded in `a_blk`.  You'll see that this matches up with the dimensions of our adjacency map which is a nice mapping and shows what these weights are attempting to learn.  \n",
    "\n",
    "Because we used a negative destination, with scalers bigger than the source, you'll see that in most cases the values are negative.  This just means that at this current initiation state the model thinks down-regulation is very important. Also, because of how our input `x_token` and embeddings have been initiated, you'll see that we actually have a repetition of two equal `[6,12,2]`.  As we do backprop and learning, this should adjust. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17a09eff-93bb-49d8-abbe-591c456695b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 2]),\n",
       " tensor([[[-0.0028,  0.0000],\n",
       "          [-0.0036, -0.0005],\n",
       "          [-0.0052, -0.0016],\n",
       "          [-0.0050, -0.0015],\n",
       "          [-0.0076, -0.0032],\n",
       "          [-0.0070, -0.0028],\n",
       "          [-0.0028,  0.0000],\n",
       "          [-0.0026,  0.0001],\n",
       "          [-0.0046, -0.0012],\n",
       "          [-0.0064, -0.0024],\n",
       "          [-0.0060, -0.0021],\n",
       "          [-0.0066, -0.0025]],\n",
       " \n",
       "         [[-0.0025,  0.0005],\n",
       "          [-0.0033,  0.0000],\n",
       "          [-0.0049, -0.0011],\n",
       "          [-0.0047, -0.0009],\n",
       "          [-0.0073, -0.0027],\n",
       "          [-0.0067, -0.0023],\n",
       "          [-0.0025,  0.0005],\n",
       "          [-0.0023,  0.0007],\n",
       "          [-0.0043, -0.0007],\n",
       "          [-0.0061, -0.0019],\n",
       "          [-0.0057, -0.0016],\n",
       "          [-0.0063, -0.0020]],\n",
       " \n",
       "         [[-0.0020,  0.0016],\n",
       "          [-0.0028,  0.0011],\n",
       "          [-0.0044,  0.0000],\n",
       "          [-0.0042,  0.0001],\n",
       "          [-0.0068, -0.0016],\n",
       "          [-0.0062, -0.0012],\n",
       "          [-0.0020,  0.0016],\n",
       "          [-0.0018,  0.0017],\n",
       "          [-0.0038,  0.0004],\n",
       "          [-0.0056, -0.0008],\n",
       "          [-0.0052, -0.0005],\n",
       "          [-0.0058, -0.0009]],\n",
       " \n",
       "         [[-0.0021,  0.0015],\n",
       "          [-0.0029,  0.0009],\n",
       "          [-0.0045, -0.0001],\n",
       "          [-0.0043,  0.0000],\n",
       "          [-0.0069, -0.0017],\n",
       "          [-0.0063, -0.0013],\n",
       "          [-0.0021,  0.0015],\n",
       "          [-0.0019,  0.0016],\n",
       "          [-0.0039,  0.0003],\n",
       "          [-0.0057, -0.0009],\n",
       "          [-0.0053, -0.0007],\n",
       "          [-0.0059, -0.0011]],\n",
       " \n",
       "         [[-0.0012,  0.0032],\n",
       "          [-0.0020,  0.0027],\n",
       "          [-0.0036,  0.0016],\n",
       "          [-0.0034,  0.0017],\n",
       "          [-0.0060,  0.0000],\n",
       "          [-0.0054,  0.0004],\n",
       "          [-0.0012,  0.0032],\n",
       "          [-0.0010,  0.0033],\n",
       "          [-0.0030,  0.0020],\n",
       "          [-0.0048,  0.0008],\n",
       "          [-0.0044,  0.0011],\n",
       "          [-0.0050,  0.0007]],\n",
       " \n",
       "         [[-0.0014,  0.0028],\n",
       "          [-0.0022,  0.0023],\n",
       "          [-0.0038,  0.0012],\n",
       "          [-0.0036,  0.0013],\n",
       "          [-0.0062, -0.0004],\n",
       "          [-0.0056,  0.0000],\n",
       "          [-0.0014,  0.0028],\n",
       "          [-0.0012,  0.0029],\n",
       "          [-0.0032,  0.0016],\n",
       "          [-0.0050,  0.0004],\n",
       "          [-0.0046,  0.0007],\n",
       "          [-0.0052,  0.0003]],\n",
       " \n",
       "         [[-0.0028,  0.0000],\n",
       "          [-0.0036, -0.0005],\n",
       "          [-0.0052, -0.0016],\n",
       "          [-0.0050, -0.0015],\n",
       "          [-0.0076, -0.0032],\n",
       "          [-0.0070, -0.0028],\n",
       "          [-0.0028,  0.0000],\n",
       "          [-0.0026,  0.0001],\n",
       "          [-0.0046, -0.0012],\n",
       "          [-0.0064, -0.0024],\n",
       "          [-0.0060, -0.0021],\n",
       "          [-0.0066, -0.0025]],\n",
       " \n",
       "         [[-0.0029, -0.0001],\n",
       "          [-0.0037, -0.0007],\n",
       "          [-0.0053, -0.0017],\n",
       "          [-0.0051, -0.0016],\n",
       "          [-0.0077, -0.0033],\n",
       "          [-0.0071, -0.0029],\n",
       "          [-0.0029, -0.0001],\n",
       "          [-0.0027,  0.0000],\n",
       "          [-0.0047, -0.0013],\n",
       "          [-0.0065, -0.0025],\n",
       "          [-0.0061, -0.0023],\n",
       "          [-0.0067, -0.0027]],\n",
       " \n",
       "         [[-0.0022,  0.0012],\n",
       "          [-0.0030,  0.0007],\n",
       "          [-0.0046, -0.0004],\n",
       "          [-0.0044, -0.0003],\n",
       "          [-0.0070, -0.0020],\n",
       "          [-0.0064, -0.0016],\n",
       "          [-0.0022,  0.0012],\n",
       "          [-0.0020,  0.0013],\n",
       "          [-0.0040,  0.0000],\n",
       "          [-0.0058, -0.0012],\n",
       "          [-0.0054, -0.0009],\n",
       "          [-0.0060, -0.0013]],\n",
       " \n",
       "         [[-0.0016,  0.0024],\n",
       "          [-0.0024,  0.0019],\n",
       "          [-0.0040,  0.0008],\n",
       "          [-0.0038,  0.0009],\n",
       "          [-0.0064, -0.0008],\n",
       "          [-0.0058, -0.0004],\n",
       "          [-0.0016,  0.0024],\n",
       "          [-0.0014,  0.0025],\n",
       "          [-0.0034,  0.0012],\n",
       "          [-0.0052,  0.0000],\n",
       "          [-0.0048,  0.0003],\n",
       "          [-0.0054, -0.0001]],\n",
       " \n",
       "         [[-0.0017,  0.0021],\n",
       "          [-0.0025,  0.0016],\n",
       "          [-0.0041,  0.0005],\n",
       "          [-0.0039,  0.0007],\n",
       "          [-0.0065, -0.0011],\n",
       "          [-0.0059, -0.0007],\n",
       "          [-0.0017,  0.0021],\n",
       "          [-0.0015,  0.0023],\n",
       "          [-0.0035,  0.0009],\n",
       "          [-0.0053, -0.0003],\n",
       "          [-0.0049,  0.0000],\n",
       "          [-0.0055, -0.0004]],\n",
       " \n",
       "         [[-0.0015,  0.0025],\n",
       "          [-0.0023,  0.0020],\n",
       "          [-0.0039,  0.0009],\n",
       "          [-0.0037,  0.0011],\n",
       "          [-0.0063, -0.0007],\n",
       "          [-0.0057, -0.0003],\n",
       "          [-0.0015,  0.0025],\n",
       "          [-0.0013,  0.0027],\n",
       "          [-0.0033,  0.0013],\n",
       "          [-0.0051,  0.0001],\n",
       "          [-0.0047,  0.0004],\n",
       "          [-0.0053,  0.0000]]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = e_src + e_dst\n",
    "e.size(), e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae2ce9d-bda0-431d-82d1-cf92138bc1a9",
   "metadata": {},
   "source": [
    "##### Signed Edge Attention\n",
    "\n",
    "This version of our GAT includes learned attention for the signed edges.  This is non-standard for GAT but I wanted to show how you'd inject it into the Attention block.  Since we want this to be an influence on the learned network attention, we need to learn how much an edge should influence. We achieve this by using a learned scaler and the Hadamard product, or element wise product. This allows each value to be multiplied by a learned scalar.  Additionally, since we have multi-headed attention, we will need two scalares so that each head can learn independently. \n",
    "\n",
    "We'll start by first prepping our adjacency map.  We'll need to do 3 things:\n",
    "1. We want to make sure that our adjacency allows nodes to learn self references, so we first want to fill in the diagonal with a non-0 value.  We'll use `0.5` since `1` is upregulated and `-1` is downregulated. \n",
    "2. Since we have the adjacency and are going to manipulate it, we first will extract the adjacency mask out.  This mask will be used on our network attention to break the back-propogation on node interactions that are not present in our example.  We'll cover how that's done in the masking step.\n",
    "3. Extract the edges out of the adjacency and multiply them by the scalar. This will be our learned Signed Edge Attention where the model can decide how much of the edge is important to the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d30eb41-6191-43b7-9b15-9d675f78bc2f",
   "metadata": {},
   "source": [
    "**Prep Adjacency Map**\n",
    "\n",
    "We'll first start by filling the diagonal and extracting a mask.  The mask will be a boolean tensor that shows all of the entries in our adjacency that are not connected.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a282808-d4e3-466f-a608-e1521595ded6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 1]),\n",
       " tensor([[[False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True]],\n",
       " \n",
       "         [[ True],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True]],\n",
       " \n",
       "         [[False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True]],\n",
       " \n",
       "         [[False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [False]]]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signed_a = a_blk.clone()\n",
    "signed_a.fill_diagonal_(0.5)                       # self-loops are positive\n",
    "mask = (signed_a == 0)  \n",
    "mask = mask.unsqueeze(-1)\n",
    "mask.size(), mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6877cbc7-0ee6-4901-b190-89840c8eb921",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 1]),\n",
       " tensor([[[ 0.5000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 1.0000],\n",
       "          [-1.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000]],\n",
       " \n",
       "         [[ 0.0000],\n",
       "          [ 0.5000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 1.0000],\n",
       "          [ 1.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000]],\n",
       " \n",
       "         [[ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.5000],\n",
       "          [ 0.0000],\n",
       "          [ 1.0000],\n",
       "          [-1.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000]],\n",
       " \n",
       "         [[ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.5000],\n",
       "          [-1.0000],\n",
       "          [ 1.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000]],\n",
       " \n",
       "         [[ 1.0000],\n",
       "          [ 1.0000],\n",
       "          [ 1.0000],\n",
       "          [-1.0000],\n",
       "          [ 0.5000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000]],\n",
       " \n",
       "         [[-1.0000],\n",
       "          [ 1.0000],\n",
       "          [-1.0000],\n",
       "          [ 1.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.5000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000]],\n",
       " \n",
       "         [[ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.5000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [-1.0000],\n",
       "          [ 1.0000]],\n",
       " \n",
       "         [[ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.5000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [-1.0000],\n",
       "          [-1.0000]],\n",
       " \n",
       "         [[ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.5000],\n",
       "          [ 0.0000],\n",
       "          [ 1.0000],\n",
       "          [-1.0000]],\n",
       " \n",
       "         [[ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.5000],\n",
       "          [-1.0000],\n",
       "          [ 1.0000]],\n",
       " \n",
       "         [[ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [-1.0000],\n",
       "          [-1.0000],\n",
       "          [ 1.0000],\n",
       "          [-1.0000],\n",
       "          [ 0.5000],\n",
       "          [ 0.0000]],\n",
       " \n",
       "         [[ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 1.0000],\n",
       "          [-1.0000],\n",
       "          [-1.0000],\n",
       "          [ 1.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.5000]]]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signed_a = signed_a.unsqueeze(-1) # [M,M,1] sign tensor\n",
    "signed_a.size(), signed_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2182fd6-20df-48b0-b6cf-4e9dd85d017c",
   "metadata": {},
   "source": [
    "**Edge Attention**\n",
    "\n",
    "Now we'll calculate our edge attention. We'll initiate our first head to be 2x our second.  Also, because our adjacency map is simply either `[-1,0.5,1]` when it's not 0, you can see how the edge gets projected to the two heads.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d946f0ec-e326-4012-915b-e8f42d4c1ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 2]),\n",
       " Parameter containing:\n",
       " tensor([[[0.2000, 0.1000]]], requires_grad=True))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gat1_attn_sign = nn.Parameter(torch.zeros(1, 1, heads))\n",
    "with torch.no_grad(): \n",
    "    sign_pattern = torch.tensor([[[0.2, 0.1]]])\n",
    "    gat1_attn_sign.copy_(sign_pattern)\n",
    "gat1_attn_sign.size(), gat1_attn_sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ac5ffd7-e677-4295-bcac-f8d0650318d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 2]),\n",
       " tensor([[[ 0.1000,  0.0500],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.2000,  0.1000],\n",
       "          [-0.2000, -0.1000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.0000,  0.0000],\n",
       "          [ 0.1000,  0.0500],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.2000,  0.1000],\n",
       "          [ 0.2000,  0.1000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.1000,  0.0500],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.2000,  0.1000],\n",
       "          [-0.2000, -0.1000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.1000,  0.0500],\n",
       "          [-0.2000, -0.1000],\n",
       "          [ 0.2000,  0.1000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.2000,  0.1000],\n",
       "          [ 0.2000,  0.1000],\n",
       "          [ 0.2000,  0.1000],\n",
       "          [-0.2000, -0.1000],\n",
       "          [ 0.1000,  0.0500],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.2000, -0.1000],\n",
       "          [ 0.2000,  0.1000],\n",
       "          [-0.2000, -0.1000],\n",
       "          [ 0.2000,  0.1000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.1000,  0.0500],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.1000,  0.0500],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [-0.2000, -0.1000],\n",
       "          [ 0.2000,  0.1000]],\n",
       " \n",
       "         [[ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.1000,  0.0500],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [-0.2000, -0.1000],\n",
       "          [-0.2000, -0.1000]],\n",
       " \n",
       "         [[ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.1000,  0.0500],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.2000,  0.1000],\n",
       "          [-0.2000, -0.1000]],\n",
       " \n",
       "         [[ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.1000,  0.0500],\n",
       "          [-0.2000, -0.1000],\n",
       "          [ 0.2000,  0.1000]],\n",
       " \n",
       "         [[ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [-0.2000, -0.1000],\n",
       "          [-0.2000, -0.1000],\n",
       "          [ 0.2000,  0.1000],\n",
       "          [-0.2000, -0.1000],\n",
       "          [ 0.1000,  0.0500],\n",
       "          [ 0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.2000,  0.1000],\n",
       "          [-0.2000, -0.1000],\n",
       "          [-0.2000, -0.1000],\n",
       "          [ 0.2000,  0.1000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.1000,  0.0500]]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_sig_a = signed_a * gat1_attn_sign\n",
    "e_sig_a.size(), e_sig_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96843a4d-3955-4a73-8914-0fdfae2848a1",
   "metadata": {},
   "source": [
    "**Combine Node and Edge Attention**\n",
    "\n",
    "Now to allow our edge attention to have an impact on the learned node attention, we need to combine them.  This is a simple summation of the two weights. You'll see that in some of the edges the values were pulled positive showing how impactful the signed edge attention can be when included.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a874dcc0-5c78-4e16-8bc3-3eaaa998fb69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 2]),\n",
       " tensor([[[ 9.7200e-02,  5.0000e-02],\n",
       "          [-3.6000e-03, -5.3333e-04],\n",
       "          [-5.2000e-03, -1.6000e-03],\n",
       "          [-5.0000e-03, -1.4667e-03],\n",
       "          [ 1.9240e-01,  9.6800e-02],\n",
       "          [-2.0700e-01, -1.0280e-01],\n",
       "          [-2.8000e-03,  0.0000e+00],\n",
       "          [-2.6000e-03,  1.3333e-04],\n",
       "          [-4.6000e-03, -1.2000e-03],\n",
       "          [-6.4000e-03, -2.4000e-03],\n",
       "          [-6.0000e-03, -2.1333e-03],\n",
       "          [-6.6000e-03, -2.5333e-03]],\n",
       " \n",
       "         [[-2.5333e-03,  5.3333e-04],\n",
       "          [ 9.6667e-02,  5.0000e-02],\n",
       "          [-4.9333e-03, -1.0667e-03],\n",
       "          [-4.7333e-03, -9.3333e-04],\n",
       "          [ 1.9267e-01,  9.7333e-02],\n",
       "          [ 1.9327e-01,  9.7733e-02],\n",
       "          [-2.5333e-03,  5.3333e-04],\n",
       "          [-2.3333e-03,  6.6667e-04],\n",
       "          [-4.3333e-03, -6.6667e-04],\n",
       "          [-6.1333e-03, -1.8667e-03],\n",
       "          [-5.7333e-03, -1.6000e-03],\n",
       "          [-6.3333e-03, -2.0000e-03]],\n",
       " \n",
       "         [[-2.0000e-03,  1.6000e-03],\n",
       "          [-2.8000e-03,  1.0667e-03],\n",
       "          [ 9.5600e-02,  5.0000e-02],\n",
       "          [-4.2000e-03,  1.3333e-04],\n",
       "          [ 1.9320e-01,  9.8400e-02],\n",
       "          [-2.0620e-01, -1.0120e-01],\n",
       "          [-2.0000e-03,  1.6000e-03],\n",
       "          [-1.8000e-03,  1.7333e-03],\n",
       "          [-3.8000e-03,  4.0000e-04],\n",
       "          [-5.6000e-03, -8.0000e-04],\n",
       "          [-5.2000e-03, -5.3333e-04],\n",
       "          [-5.8000e-03, -9.3333e-04]],\n",
       " \n",
       "         [[-2.0667e-03,  1.4667e-03],\n",
       "          [-2.8667e-03,  9.3333e-04],\n",
       "          [-4.4667e-03, -1.3333e-04],\n",
       "          [ 9.5733e-02,  5.0000e-02],\n",
       "          [-2.0687e-01, -1.0173e-01],\n",
       "          [ 1.9373e-01,  9.8667e-02],\n",
       "          [-2.0667e-03,  1.4667e-03],\n",
       "          [-1.8667e-03,  1.6000e-03],\n",
       "          [-3.8667e-03,  2.6667e-04],\n",
       "          [-5.6667e-03, -9.3333e-04],\n",
       "          [-5.2667e-03, -6.6667e-04],\n",
       "          [-5.8667e-03, -1.0667e-03]],\n",
       " \n",
       "         [[ 1.9880e-01,  1.0320e-01],\n",
       "          [ 1.9800e-01,  1.0267e-01],\n",
       "          [ 1.9640e-01,  1.0160e-01],\n",
       "          [-2.0340e-01, -9.8267e-02],\n",
       "          [ 9.4000e-02,  5.0000e-02],\n",
       "          [-5.4000e-03,  4.0000e-04],\n",
       "          [-1.2000e-03,  3.2000e-03],\n",
       "          [-1.0000e-03,  3.3333e-03],\n",
       "          [-3.0000e-03,  2.0000e-03],\n",
       "          [-4.8000e-03,  8.0000e-04],\n",
       "          [-4.4000e-03,  1.0667e-03],\n",
       "          [-5.0000e-03,  6.6667e-04]],\n",
       " \n",
       "         [[-2.0140e-01, -9.7200e-02],\n",
       "          [ 1.9780e-01,  1.0227e-01],\n",
       "          [-2.0380e-01, -9.8800e-02],\n",
       "          [ 1.9640e-01,  1.0133e-01],\n",
       "          [-6.2000e-03, -4.0000e-04],\n",
       "          [ 9.4400e-02,  5.0000e-02],\n",
       "          [-1.4000e-03,  2.8000e-03],\n",
       "          [-1.2000e-03,  2.9333e-03],\n",
       "          [-3.2000e-03,  1.6000e-03],\n",
       "          [-5.0000e-03,  4.0000e-04],\n",
       "          [-4.6000e-03,  6.6667e-04],\n",
       "          [-5.2000e-03,  2.6667e-04]],\n",
       " \n",
       "         [[-2.8000e-03,  0.0000e+00],\n",
       "          [-3.6000e-03, -5.3333e-04],\n",
       "          [-5.2000e-03, -1.6000e-03],\n",
       "          [-5.0000e-03, -1.4667e-03],\n",
       "          [-7.6000e-03, -3.2000e-03],\n",
       "          [-7.0000e-03, -2.8000e-03],\n",
       "          [ 9.7200e-02,  5.0000e-02],\n",
       "          [-2.6000e-03,  1.3333e-04],\n",
       "          [-4.6000e-03, -1.2000e-03],\n",
       "          [-6.4000e-03, -2.4000e-03],\n",
       "          [-2.0600e-01, -1.0213e-01],\n",
       "          [ 1.9340e-01,  9.7467e-02]],\n",
       " \n",
       "         [[-2.8667e-03, -1.3333e-04],\n",
       "          [-3.6667e-03, -6.6667e-04],\n",
       "          [-5.2667e-03, -1.7333e-03],\n",
       "          [-5.0667e-03, -1.6000e-03],\n",
       "          [-7.6667e-03, -3.3333e-03],\n",
       "          [-7.0667e-03, -2.9333e-03],\n",
       "          [-2.8667e-03, -1.3333e-04],\n",
       "          [ 9.7333e-02,  5.0000e-02],\n",
       "          [-4.6667e-03, -1.3333e-03],\n",
       "          [-6.4667e-03, -2.5333e-03],\n",
       "          [-2.0607e-01, -1.0227e-01],\n",
       "          [-2.0667e-01, -1.0267e-01]],\n",
       " \n",
       "         [[-2.2000e-03,  1.2000e-03],\n",
       "          [-3.0000e-03,  6.6667e-04],\n",
       "          [-4.6000e-03, -4.0000e-04],\n",
       "          [-4.4000e-03, -2.6667e-04],\n",
       "          [-7.0000e-03, -2.0000e-03],\n",
       "          [-6.4000e-03, -1.6000e-03],\n",
       "          [-2.2000e-03,  1.2000e-03],\n",
       "          [-2.0000e-03,  1.3333e-03],\n",
       "          [ 9.6000e-02,  5.0000e-02],\n",
       "          [-5.8000e-03, -1.2000e-03],\n",
       "          [ 1.9460e-01,  9.9067e-02],\n",
       "          [-2.0600e-01, -1.0133e-01]],\n",
       " \n",
       "         [[-1.6000e-03,  2.4000e-03],\n",
       "          [-2.4000e-03,  1.8667e-03],\n",
       "          [-4.0000e-03,  8.0000e-04],\n",
       "          [-3.8000e-03,  9.3333e-04],\n",
       "          [-6.4000e-03, -8.0000e-04],\n",
       "          [-5.8000e-03, -4.0000e-04],\n",
       "          [-1.6000e-03,  2.4000e-03],\n",
       "          [-1.4000e-03,  2.5333e-03],\n",
       "          [-3.4000e-03,  1.2000e-03],\n",
       "          [ 9.4800e-02,  5.0000e-02],\n",
       "          [-2.0480e-01, -9.9733e-02],\n",
       "          [ 1.9460e-01,  9.9867e-02]],\n",
       " \n",
       "         [[-1.7333e-03,  2.1333e-03],\n",
       "          [-2.5333e-03,  1.6000e-03],\n",
       "          [-4.1333e-03,  5.3333e-04],\n",
       "          [-3.9333e-03,  6.6667e-04],\n",
       "          [-6.5333e-03, -1.0667e-03],\n",
       "          [-5.9333e-03, -6.6667e-04],\n",
       "          [-2.0173e-01, -9.7867e-02],\n",
       "          [-2.0153e-01, -9.7733e-02],\n",
       "          [ 1.9647e-01,  1.0093e-01],\n",
       "          [-2.0533e-01, -1.0027e-01],\n",
       "          [ 9.5067e-02,  5.0000e-02],\n",
       "          [-5.5333e-03, -4.0000e-04]],\n",
       " \n",
       "         [[-1.5333e-03,  2.5333e-03],\n",
       "          [-2.3333e-03,  2.0000e-03],\n",
       "          [-3.9333e-03,  9.3333e-04],\n",
       "          [-3.7333e-03,  1.0667e-03],\n",
       "          [-6.3333e-03, -6.6667e-04],\n",
       "          [-5.7333e-03, -2.6667e-04],\n",
       "          [ 1.9847e-01,  1.0253e-01],\n",
       "          [-2.0133e-01, -9.7333e-02],\n",
       "          [-2.0333e-01, -9.8667e-02],\n",
       "          [ 1.9487e-01,  1.0013e-01],\n",
       "          [-4.7333e-03,  4.0000e-04],\n",
       "          [ 9.4667e-02,  5.0000e-02]]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = e + e_sig_a \n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1504fe05-0514-4125-bd20-d5c1bcfac85b",
   "metadata": {},
   "source": [
    "##### Leaky ReLU\n",
    "\n",
    "Next we apply leaky ReLU (rectified linear units), an element-wise nonlinearity that leaves positive activations unchanged and scales negative activations by a small factor, in our case `0.5`. In graph networks it is commonly used to introduce nonlinearity while preserving gradient flow through negative values and avoiding “dying units.” Additionally, leaky ReLU preserves relative ordering between moderately negative vs strongly negative scores, which still matters once you apply the softmax across neighbors. In our signed setup, it’s especially useful because negative contributions (e.g. down-regulation) don’t get hard-clipped; they remain part of the competition for attention instead of being erased outright like would be the case if using ReLU.\n",
    "\n",
    "The formula applied is:\n",
    "$$\n",
    "y=\\max(\\alpha x,x)=\n",
    "\\begin{cases}\n",
    "x,& x>0\\\\\n",
    "\\alpha x,& x\\leq 0\n",
    "\\end{cases} \\quad\n",
    "\\frac{\\partial y}{\\partial x}=\n",
    "\\begin{cases}\n",
    "1,& x>0 \\\\\n",
    "\\alpha,& x\\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "LeakyReLU is stateless, meaning it has no learnable parameters.  As an output you can see that positive values remained in their state while negative values were cut in half. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bfd85eb6-5308-4df8-be1f-c28694cb080e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 2]),\n",
       " tensor([[[ 9.7200e-02,  5.0000e-02],\n",
       "          [-1.8000e-03, -2.6667e-04],\n",
       "          [-2.6000e-03, -8.0000e-04],\n",
       "          [-2.5000e-03, -7.3333e-04],\n",
       "          [ 1.9240e-01,  9.6800e-02],\n",
       "          [-1.0350e-01, -5.1400e-02],\n",
       "          [-1.4000e-03,  0.0000e+00],\n",
       "          [-1.3000e-03,  1.3333e-04],\n",
       "          [-2.3000e-03, -6.0000e-04],\n",
       "          [-3.2000e-03, -1.2000e-03],\n",
       "          [-3.0000e-03, -1.0667e-03],\n",
       "          [-3.3000e-03, -1.2667e-03]],\n",
       " \n",
       "         [[-1.2667e-03,  5.3333e-04],\n",
       "          [ 9.6667e-02,  5.0000e-02],\n",
       "          [-2.4667e-03, -5.3333e-04],\n",
       "          [-2.3667e-03, -4.6667e-04],\n",
       "          [ 1.9267e-01,  9.7333e-02],\n",
       "          [ 1.9327e-01,  9.7733e-02],\n",
       "          [-1.2667e-03,  5.3333e-04],\n",
       "          [-1.1667e-03,  6.6667e-04],\n",
       "          [-2.1667e-03, -3.3333e-04],\n",
       "          [-3.0667e-03, -9.3333e-04],\n",
       "          [-2.8667e-03, -8.0000e-04],\n",
       "          [-3.1667e-03, -1.0000e-03]],\n",
       " \n",
       "         [[-1.0000e-03,  1.6000e-03],\n",
       "          [-1.4000e-03,  1.0667e-03],\n",
       "          [ 9.5600e-02,  5.0000e-02],\n",
       "          [-2.1000e-03,  1.3333e-04],\n",
       "          [ 1.9320e-01,  9.8400e-02],\n",
       "          [-1.0310e-01, -5.0600e-02],\n",
       "          [-1.0000e-03,  1.6000e-03],\n",
       "          [-9.0000e-04,  1.7333e-03],\n",
       "          [-1.9000e-03,  4.0000e-04],\n",
       "          [-2.8000e-03, -4.0000e-04],\n",
       "          [-2.6000e-03, -2.6667e-04],\n",
       "          [-2.9000e-03, -4.6667e-04]],\n",
       " \n",
       "         [[-1.0333e-03,  1.4667e-03],\n",
       "          [-1.4333e-03,  9.3333e-04],\n",
       "          [-2.2333e-03, -6.6667e-05],\n",
       "          [ 9.5733e-02,  5.0000e-02],\n",
       "          [-1.0343e-01, -5.0867e-02],\n",
       "          [ 1.9373e-01,  9.8667e-02],\n",
       "          [-1.0333e-03,  1.4667e-03],\n",
       "          [-9.3333e-04,  1.6000e-03],\n",
       "          [-1.9333e-03,  2.6667e-04],\n",
       "          [-2.8333e-03, -4.6667e-04],\n",
       "          [-2.6333e-03, -3.3333e-04],\n",
       "          [-2.9333e-03, -5.3333e-04]],\n",
       " \n",
       "         [[ 1.9880e-01,  1.0320e-01],\n",
       "          [ 1.9800e-01,  1.0267e-01],\n",
       "          [ 1.9640e-01,  1.0160e-01],\n",
       "          [-1.0170e-01, -4.9133e-02],\n",
       "          [ 9.4000e-02,  5.0000e-02],\n",
       "          [-2.7000e-03,  4.0000e-04],\n",
       "          [-6.0000e-04,  3.2000e-03],\n",
       "          [-5.0000e-04,  3.3333e-03],\n",
       "          [-1.5000e-03,  2.0000e-03],\n",
       "          [-2.4000e-03,  8.0000e-04],\n",
       "          [-2.2000e-03,  1.0667e-03],\n",
       "          [-2.5000e-03,  6.6667e-04]],\n",
       " \n",
       "         [[-1.0070e-01, -4.8600e-02],\n",
       "          [ 1.9780e-01,  1.0227e-01],\n",
       "          [-1.0190e-01, -4.9400e-02],\n",
       "          [ 1.9640e-01,  1.0133e-01],\n",
       "          [-3.1000e-03, -2.0000e-04],\n",
       "          [ 9.4400e-02,  5.0000e-02],\n",
       "          [-7.0000e-04,  2.8000e-03],\n",
       "          [-6.0000e-04,  2.9333e-03],\n",
       "          [-1.6000e-03,  1.6000e-03],\n",
       "          [-2.5000e-03,  4.0000e-04],\n",
       "          [-2.3000e-03,  6.6667e-04],\n",
       "          [-2.6000e-03,  2.6667e-04]],\n",
       " \n",
       "         [[-1.4000e-03,  0.0000e+00],\n",
       "          [-1.8000e-03, -2.6667e-04],\n",
       "          [-2.6000e-03, -8.0000e-04],\n",
       "          [-2.5000e-03, -7.3333e-04],\n",
       "          [-3.8000e-03, -1.6000e-03],\n",
       "          [-3.5000e-03, -1.4000e-03],\n",
       "          [ 9.7200e-02,  5.0000e-02],\n",
       "          [-1.3000e-03,  1.3333e-04],\n",
       "          [-2.3000e-03, -6.0000e-04],\n",
       "          [-3.2000e-03, -1.2000e-03],\n",
       "          [-1.0300e-01, -5.1067e-02],\n",
       "          [ 1.9340e-01,  9.7467e-02]],\n",
       " \n",
       "         [[-1.4333e-03, -6.6667e-05],\n",
       "          [-1.8333e-03, -3.3333e-04],\n",
       "          [-2.6333e-03, -8.6667e-04],\n",
       "          [-2.5333e-03, -8.0000e-04],\n",
       "          [-3.8333e-03, -1.6667e-03],\n",
       "          [-3.5333e-03, -1.4667e-03],\n",
       "          [-1.4333e-03, -6.6667e-05],\n",
       "          [ 9.7333e-02,  5.0000e-02],\n",
       "          [-2.3333e-03, -6.6667e-04],\n",
       "          [-3.2333e-03, -1.2667e-03],\n",
       "          [-1.0303e-01, -5.1133e-02],\n",
       "          [-1.0333e-01, -5.1333e-02]],\n",
       " \n",
       "         [[-1.1000e-03,  1.2000e-03],\n",
       "          [-1.5000e-03,  6.6667e-04],\n",
       "          [-2.3000e-03, -2.0000e-04],\n",
       "          [-2.2000e-03, -1.3333e-04],\n",
       "          [-3.5000e-03, -1.0000e-03],\n",
       "          [-3.2000e-03, -8.0000e-04],\n",
       "          [-1.1000e-03,  1.2000e-03],\n",
       "          [-1.0000e-03,  1.3333e-03],\n",
       "          [ 9.6000e-02,  5.0000e-02],\n",
       "          [-2.9000e-03, -6.0000e-04],\n",
       "          [ 1.9460e-01,  9.9067e-02],\n",
       "          [-1.0300e-01, -5.0667e-02]],\n",
       " \n",
       "         [[-8.0000e-04,  2.4000e-03],\n",
       "          [-1.2000e-03,  1.8667e-03],\n",
       "          [-2.0000e-03,  8.0000e-04],\n",
       "          [-1.9000e-03,  9.3333e-04],\n",
       "          [-3.2000e-03, -4.0000e-04],\n",
       "          [-2.9000e-03, -2.0000e-04],\n",
       "          [-8.0000e-04,  2.4000e-03],\n",
       "          [-7.0000e-04,  2.5333e-03],\n",
       "          [-1.7000e-03,  1.2000e-03],\n",
       "          [ 9.4800e-02,  5.0000e-02],\n",
       "          [-1.0240e-01, -4.9867e-02],\n",
       "          [ 1.9460e-01,  9.9867e-02]],\n",
       " \n",
       "         [[-8.6667e-04,  2.1333e-03],\n",
       "          [-1.2667e-03,  1.6000e-03],\n",
       "          [-2.0667e-03,  5.3333e-04],\n",
       "          [-1.9667e-03,  6.6667e-04],\n",
       "          [-3.2667e-03, -5.3333e-04],\n",
       "          [-2.9667e-03, -3.3333e-04],\n",
       "          [-1.0087e-01, -4.8933e-02],\n",
       "          [-1.0077e-01, -4.8867e-02],\n",
       "          [ 1.9647e-01,  1.0093e-01],\n",
       "          [-1.0267e-01, -5.0133e-02],\n",
       "          [ 9.5067e-02,  5.0000e-02],\n",
       "          [-2.7667e-03, -2.0000e-04]],\n",
       " \n",
       "         [[-7.6667e-04,  2.5333e-03],\n",
       "          [-1.1667e-03,  2.0000e-03],\n",
       "          [-1.9667e-03,  9.3333e-04],\n",
       "          [-1.8667e-03,  1.0667e-03],\n",
       "          [-3.1667e-03, -3.3333e-04],\n",
       "          [-2.8667e-03, -1.3333e-04],\n",
       "          [ 1.9847e-01,  1.0253e-01],\n",
       "          [-1.0067e-01, -4.8667e-02],\n",
       "          [-1.0167e-01, -4.9333e-02],\n",
       "          [ 1.9487e-01,  1.0013e-01],\n",
       "          [-2.3667e-03,  4.0000e-04],\n",
       "          [ 9.4667e-02,  5.0000e-02]]], grad_fn=<LeakyReluBackward1>))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gat1_attn_lrelu = nn.LeakyReLU(0.5, inplace=True)\n",
    "out = gat1_attn_lrelu(out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8c0ad9-bef6-434a-9b40-b03577b28902",
   "metadata": {},
   "source": [
    "##### Masking\n",
    "\n",
    "Our next step is to mask the edges that we know are not present in our example.  Recall that we built the boolean based mask from our adjacency map when building the signed edge attention. In this mask, a `true` value represented a connection that needed to be suppressed. \n",
    "\n",
    "Since this is the branch for node connections , we need to mute the weights for where there is no edge so that gradients can't flow backwards there. We'll do this in 2 steps.  First we'll replace the values with $-\\infty$ and then during `softmax` those values will become 0. \n",
    "\n",
    "Similar to other steps, we'll need to unravel our mask to a representation of each edge and then our masked fill will apply the value across both heads. You'll see that after masking the weight sparsity significantly jumps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6fdfc7ac-688b-41af-8e0d-570dc1234ce1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 2]),\n",
       " tensor([[[ 0.0972,  0.0500],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.1924,  0.0968],\n",
       "          [-0.1035, -0.0514],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [ 0.0967,  0.0500],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.1927,  0.0973],\n",
       "          [ 0.1933,  0.0977],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.0956,  0.0500],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.1932,  0.0984],\n",
       "          [-0.1031, -0.0506],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.0957,  0.0500],\n",
       "          [-0.1034, -0.0509],\n",
       "          [ 0.1937,  0.0987],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf]],\n",
       " \n",
       "         [[ 0.1988,  0.1032],\n",
       "          [ 0.1980,  0.1027],\n",
       "          [ 0.1964,  0.1016],\n",
       "          [-0.1017, -0.0491],\n",
       "          [ 0.0940,  0.0500],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf]],\n",
       " \n",
       "         [[-0.1007, -0.0486],\n",
       "          [ 0.1978,  0.1023],\n",
       "          [-0.1019, -0.0494],\n",
       "          [ 0.1964,  0.1013],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.0944,  0.0500],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.0972,  0.0500],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [-0.1030, -0.0511],\n",
       "          [ 0.1934,  0.0975]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.0973,  0.0500],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [-0.1030, -0.0511],\n",
       "          [-0.1033, -0.0513]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.0960,  0.0500],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.1946,  0.0991],\n",
       "          [-0.1030, -0.0507]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.0948,  0.0500],\n",
       "          [-0.1024, -0.0499],\n",
       "          [ 0.1946,  0.0999]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [-0.1009, -0.0489],\n",
       "          [-0.1008, -0.0489],\n",
       "          [ 0.1965,  0.1009],\n",
       "          [-0.1027, -0.0501],\n",
       "          [ 0.0951,  0.0500],\n",
       "          [   -inf,    -inf]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.1985,  0.1025],\n",
       "          [-0.1007, -0.0487],\n",
       "          [-0.1017, -0.0493],\n",
       "          [ 0.1949,  0.1001],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.0947,  0.0500]]], grad_fn=<MaskedFillBackward0>))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out.masked_fill(mask, float('-inf'))\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9299fa2a-cbd6-4abe-9d87-51c619f7f0f1",
   "metadata": {},
   "source": [
    "##### SoftMax\n",
    "\n",
    "Now we'll apply the softmax. The softmax turns the attention this far into a connection strength measure. In essence this encodes “how strongly each head trusts neighbors\".  Softmax turns a real-valued vector into a probability distribution by exponentiating each component and normalizing by the sum so all outputs are nonnegative and sum to 1.  It does this by applying the following: \n",
    "\n",
    "$\\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}$\n",
    "\n",
    "Because of this calculation, we can expect that the values we masked with $-\\infty$ will become 0 due to their extreme negativeness, showing the power of the mask.  Also, now you'll see that each row changes in value since you have `n+1` elements in each row to distribute across. Finally, we'll now see that all of our negative values are removed.  We are not worried about that though since we'll add the sign back in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f6d0c43-e939-4009-8dd0-728ab4f59b55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 2]),\n",
       " tensor([[[0.3427, 0.3388],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3769, 0.3550],\n",
       "          [0.2804, 0.3061],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.3123, 0.3229],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3438, 0.3385],\n",
       "          [0.3440, 0.3386],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3422, 0.3385],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3773, 0.3553],\n",
       "          [0.2805, 0.3061],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3422, 0.3385],\n",
       "          [0.2804, 0.3061],\n",
       "          [0.3774, 0.3554],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.2156, 0.2081],\n",
       "          [0.2154, 0.2080],\n",
       "          [0.2151, 0.2078],\n",
       "          [0.1597, 0.1787],\n",
       "          [0.1942, 0.1973],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.1693, 0.1842],\n",
       "          [0.2281, 0.2143],\n",
       "          [0.1691, 0.1841],\n",
       "          [0.2278, 0.2141],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2057, 0.2033],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3425, 0.3387],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2804, 0.3061],\n",
       "          [0.3771, 0.3552]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3793, 0.3562],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3104, 0.3219],\n",
       "          [0.3103, 0.3219]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3421, 0.3385],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3775, 0.3555],\n",
       "          [0.2804, 0.3061]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3418, 0.3383],\n",
       "          [0.2806, 0.3061],\n",
       "          [0.3776, 0.3556]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.1798, 0.1900],\n",
       "          [0.1798, 0.1900],\n",
       "          [0.2421, 0.2207],\n",
       "          [0.1795, 0.1897],\n",
       "          [0.2187, 0.2097],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2283, 0.2144],\n",
       "          [0.1693, 0.1843],\n",
       "          [0.1691, 0.1842],\n",
       "          [0.2275, 0.2138],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2058, 0.2034]]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = torch.softmax(out, dim=1)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4cdf5c-fc8f-4613-b698-f6c53257b313",
   "metadata": {},
   "source": [
    "##### Attention Dropout\n",
    "\n",
    "Now that we have our attention probabilities calculated, we'll do another round of dropout. Attention dropout is an edge-level / neighbor-level regularization. Because it randomly drops some attention coefficients during training it prevents the model from collapsing onto a single neighbor or a tiny subset of edges, forces heads to consider multiple neighbors and not overfit to one “perfect” regulatory edge and, in our signed version, it’s acts as stochastic pruning of both up- and down-regulation edges during training. The best way to think of it is “don’t over-trust any single edge/neighbor.”  Again, this is Bernoulli based dropout, in addition to zeroing out weights the surviving entries are scaled by $1/(1-p)$.  We'll use a p of 0.05 so you'll see the non-0 values increase by that scalar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e77e16fd-eaed-448e-b57d-a76314ba4516",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_dropout = 0.05\n",
    "gat1_attn_drop = nn.Dropout(attn_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "60cb4a03-178f-45fc-b630-612ca113b4e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 2]),\n",
       " tensor([[[0.3607, 0.3566],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3968, 0.3737],\n",
       "          [0.2951, 0.3223],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.3287, 0.3398],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3618, 0.3563],\n",
       "          [0.3621, 0.3565],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.3564],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3971, 0.3740],\n",
       "          [0.2953, 0.3222],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3602, 0.3564],\n",
       "          [0.2952, 0.0000],\n",
       "          [0.3973, 0.3741],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.2270, 0.2191],\n",
       "          [0.2268, 0.2190],\n",
       "          [0.2264, 0.2187],\n",
       "          [0.1681, 0.1881],\n",
       "          [0.2044, 0.2077],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.1782, 0.1939],\n",
       "          [0.2401, 0.2255],\n",
       "          [0.1780, 0.1938],\n",
       "          [0.2398, 0.2253],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2166, 0.2140],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3605, 0.3565],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2951, 0.3223],\n",
       "          [0.3970, 0.3739]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3992, 0.3749],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3267, 0.3389],\n",
       "          [0.3266, 0.3388]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3601, 0.3563],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3974, 0.3742],\n",
       "          [0.2951, 0.3222]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3598, 0.3561],\n",
       "          [0.2954, 0.3222],\n",
       "          [0.3975, 0.3743]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.1893, 0.1999],\n",
       "          [0.1893, 0.2000],\n",
       "          [0.2548, 0.2323],\n",
       "          [0.1889, 0.1997],\n",
       "          [0.2303, 0.2207],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2403, 0.2256],\n",
       "          [0.1782, 0.1940],\n",
       "          [0.1780, 0.1938],\n",
       "          [0.2395, 0.2251],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2166, 0.2141]]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = gat1_attn_drop(out) \n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40131fd7-2182-4b1f-bc42-7bff26384dea",
   "metadata": {},
   "source": [
    "##### Sign Aware Attention\n",
    "\n",
    "Now that we calculated the attention we'll add in our sign again. The previous softmax gave us the connection strength but now we need the direction to encode up and down regulation. By taking the Hadamard product we allow each edge to be scaled by the adjacency value making inhibitory edges contribute with a negative weight and excitatory edges with a positive weight, so the same attention pattern can either reinforce or oppose a signal depending on biology. \n",
    "\n",
    "We'll first start by reshaping our current weight and the adjacency map to match dimensions per head. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a1a5a99d-6afb-43ff-b3b7-4bc314203006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 12, 12]),\n",
       " tensor([[[0.3607, 0.0000, 0.0000, 0.0000, 0.3968, 0.2951, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.3287, 0.0000, 0.0000, 0.3618, 0.3621, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.3971, 0.2953, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.3602, 0.2952, 0.3973, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.2270, 0.2268, 0.2264, 0.1681, 0.2044, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.1782, 0.2401, 0.1780, 0.2398, 0.0000, 0.2166, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3605, 0.0000,\n",
       "           0.0000, 0.0000, 0.2951, 0.3970],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3992,\n",
       "           0.0000, 0.0000, 0.3267, 0.3266],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.3601, 0.0000, 0.3974, 0.2951],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.3598, 0.2954, 0.3975],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1893, 0.1893,\n",
       "           0.2548, 0.1889, 0.2303, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2403, 0.1782,\n",
       "           0.1780, 0.2395, 0.0000, 0.2166]],\n",
       " \n",
       "         [[0.3566, 0.0000, 0.0000, 0.0000, 0.3737, 0.3223, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.3398, 0.0000, 0.0000, 0.3563, 0.3565, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.3564, 0.0000, 0.3740, 0.3222, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.3564, 0.0000, 0.3741, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.2191, 0.2190, 0.2187, 0.1881, 0.2077, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.1939, 0.2255, 0.1938, 0.2253, 0.0000, 0.2140, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3565, 0.0000,\n",
       "           0.0000, 0.0000, 0.3223, 0.3739],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3749,\n",
       "           0.0000, 0.0000, 0.3389, 0.3388],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.3563, 0.0000, 0.3742, 0.3222],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.3561, 0.3222, 0.3743],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1999, 0.2000,\n",
       "           0.2323, 0.1997, 0.2207, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2256, 0.1940,\n",
       "           0.1938, 0.2251, 0.0000, 0.2141]]], grad_fn=<PermuteBackward0>))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out.permute(2, 0, 1) # [H,M,M]\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cdcab328-cd4c-4eed-bc16-bb4e64b1fa17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 12, 12]),\n",
       " tensor([[[ 0.5000,  0.0000,  0.0000,  0.0000,  1.0000, -1.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.5000,  0.0000,  0.0000,  1.0000,  1.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.5000,  0.0000,  1.0000, -1.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.5000, -1.0000,  1.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000, -1.0000,  0.5000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.0000,  1.0000, -1.0000,  1.0000,  0.0000,  0.5000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.5000,\n",
       "            0.0000,  0.0000,  0.0000, -1.0000,  1.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.5000,  0.0000,  0.0000, -1.0000, -1.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.5000,  0.0000,  1.0000, -1.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.5000, -1.0000,  1.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.0000,\n",
       "           -1.0000,  1.0000, -1.0000,  0.5000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,\n",
       "           -1.0000, -1.0000,  1.0000,  0.0000,  0.5000]],\n",
       " \n",
       "         [[ 0.5000,  0.0000,  0.0000,  0.0000,  1.0000, -1.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.5000,  0.0000,  0.0000,  1.0000,  1.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.5000,  0.0000,  1.0000, -1.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.5000, -1.0000,  1.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000, -1.0000,  0.5000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.0000,  1.0000, -1.0000,  1.0000,  0.0000,  0.5000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.5000,\n",
       "            0.0000,  0.0000,  0.0000, -1.0000,  1.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.5000,  0.0000,  0.0000, -1.0000, -1.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.5000,  0.0000,  1.0000, -1.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.5000, -1.0000,  1.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.0000,\n",
       "           -1.0000,  1.0000, -1.0000,  0.5000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,\n",
       "           -1.0000, -1.0000,  1.0000,  0.0000,  0.5000]]]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_heads = signed_a.squeeze(-1).unsqueeze(0).expand(heads, -1, -1) # [H,M,M]\n",
    "s_heads.size(), s_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcaec31-8caa-40c6-8a94-04c3e7caab89",
   "metadata": {},
   "source": [
    "**Direction Scaled Attention**\n",
    "\n",
    "Now that we have the dimensions aligned we'll run our scaled multiple.  You'll see that the softmax weights now adopt the direction of the adjacency map and the diagonals divide in half so that they're not quite as impactful as the up-regulated edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "37fcdc96-aa3d-433c-a4d2-bca520a4ca92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 12, 12]),\n",
       " tensor([[[ 0.1804,  0.0000,  0.0000,  0.0000,  0.3968, -0.2951,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.1644,  0.0000,  0.0000,  0.3618,  0.3621,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.3971, -0.2953,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.1801, -0.2952,  0.3973,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.2270,  0.2268,  0.2264, -0.1681,  0.1022,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.1782,  0.2401, -0.1780,  0.2398,  0.0000,  0.1083,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1803,\n",
       "            0.0000,  0.0000,  0.0000, -0.2951,  0.3970],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.1996,  0.0000,  0.0000, -0.3267, -0.3266],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.1800,  0.0000,  0.3974, -0.2951],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.1799, -0.2954,  0.3975],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.1893,\n",
       "           -0.1893,  0.2548, -0.1889,  0.1151,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.2403,\n",
       "           -0.1782, -0.1780,  0.2395,  0.0000,  0.1083]],\n",
       " \n",
       "         [[ 0.1783,  0.0000,  0.0000,  0.0000,  0.3737, -0.3223,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.1699,  0.0000,  0.0000,  0.3563,  0.3565,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.1782,  0.0000,  0.3740, -0.3222,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.1782, -0.0000,  0.3741,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.2191,  0.2190,  0.2187, -0.1881,  0.1039,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.1939,  0.2255, -0.1938,  0.2253,  0.0000,  0.1070,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1783,\n",
       "            0.0000,  0.0000,  0.0000, -0.3223,  0.3739],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.1875,  0.0000,  0.0000, -0.3389, -0.3388],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.1781,  0.0000,  0.3742, -0.3222],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.1780, -0.3222,  0.3743],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.1999,\n",
       "           -0.2000,  0.2323, -0.1997,  0.1104,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.2256,\n",
       "           -0.1940, -0.1938,  0.2251,  0.0000,  0.1070]]],\n",
       "        grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out * s_heads # [H,M,M]\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57801ef-e400-48bb-a4a3-bae79aa262d4",
   "metadata": {},
   "source": [
    "##### Message Aggregation\n",
    "\n",
    "Now, for each head, we have to combine the neighbor features into updated node embeddings with the learned attention weights.  To do this we'll take the dot product of the the learned attention by the node projection to perform \n",
    "$$\n",
    "text{out}_h[h, i, :] = \\sum_j out_h[h, i, j] Hproj[h, j, :]\n",
    "$$\n",
    "This results in us taking a weighted sum over all neighbors’ features per node and per head. This turns attention scores into new node representations. This representation is how nodes aggregate information mostly from neighbors with high attention weights, and different heads learn different aggregation patterns over the same graph.\n",
    "\n",
    "Despite starting out with the linear node projection being the same across heads, the different attention per head will now pull the nodes and channels to differing directions.  You can also see the value of the negative edges showing up with node projections now being negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d8ec34e2-ad6d-416c-b4d3-793303f2ccc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 12, 3]),\n",
       " tensor([[[ 0.0010,  0.0021,  0.0031],\n",
       "          [ 0.0040,  0.0079,  0.0119],\n",
       "          [ 0.0006,  0.0012,  0.0018],\n",
       "          [ 0.0010,  0.0020,  0.0031],\n",
       "          [ 0.0019,  0.0038,  0.0057],\n",
       "          [ 0.0010,  0.0019,  0.0029],\n",
       "          [ 0.0010,  0.0019,  0.0029],\n",
       "          [-0.0024, -0.0047, -0.0071],\n",
       "          [ 0.0009,  0.0018,  0.0028],\n",
       "          [ 0.0013,  0.0027,  0.0040],\n",
       "          [-0.0004, -0.0007, -0.0011],\n",
       "          [ 0.0011,  0.0022,  0.0033]],\n",
       " \n",
       "         [[ 0.0008,  0.0016,  0.0023],\n",
       "          [ 0.0039,  0.0078,  0.0118],\n",
       "          [ 0.0010,  0.0020,  0.0031],\n",
       "          [ 0.0024,  0.0048,  0.0071],\n",
       "          [ 0.0018,  0.0035,  0.0053],\n",
       "          [ 0.0008,  0.0015,  0.0023],\n",
       "          [ 0.0008,  0.0015,  0.0023],\n",
       "          [-0.0025, -0.0050, -0.0074],\n",
       "          [ 0.0007,  0.0014,  0.0021],\n",
       "          [ 0.0011,  0.0022,  0.0033],\n",
       "          [-0.0005, -0.0011, -0.0016],\n",
       "          [ 0.0009,  0.0018,  0.0027]]], grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out @ Hproj # [H,M,O]\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457ac89d-d7f0-41dd-ad6d-b4e236d74f69",
   "metadata": {},
   "source": [
    "##### Concatenate Heads\n",
    "\n",
    "Now that we've calculated our node level weights, we'll collapse our heads together by concatenating them.  This concatenation allows the gradients to maintain the flow through the head while flowing forward to the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "72fc3ae5-a289-4bac-9e8a-c646d6ae14cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 2, 3]),\n",
       " tensor([[[ 0.0010,  0.0021,  0.0031],\n",
       "          [ 0.0008,  0.0016,  0.0023]],\n",
       " \n",
       "         [[ 0.0040,  0.0079,  0.0119],\n",
       "          [ 0.0039,  0.0078,  0.0118]],\n",
       " \n",
       "         [[ 0.0006,  0.0012,  0.0018],\n",
       "          [ 0.0010,  0.0020,  0.0031]],\n",
       " \n",
       "         [[ 0.0010,  0.0020,  0.0031],\n",
       "          [ 0.0024,  0.0048,  0.0071]],\n",
       " \n",
       "         [[ 0.0019,  0.0038,  0.0057],\n",
       "          [ 0.0018,  0.0035,  0.0053]],\n",
       " \n",
       "         [[ 0.0010,  0.0019,  0.0029],\n",
       "          [ 0.0008,  0.0015,  0.0023]],\n",
       " \n",
       "         [[ 0.0010,  0.0019,  0.0029],\n",
       "          [ 0.0008,  0.0015,  0.0023]],\n",
       " \n",
       "         [[-0.0024, -0.0047, -0.0071],\n",
       "          [-0.0025, -0.0050, -0.0074]],\n",
       " \n",
       "         [[ 0.0009,  0.0018,  0.0028],\n",
       "          [ 0.0007,  0.0014,  0.0021]],\n",
       " \n",
       "         [[ 0.0013,  0.0027,  0.0040],\n",
       "          [ 0.0011,  0.0022,  0.0033]],\n",
       " \n",
       "         [[-0.0004, -0.0007, -0.0011],\n",
       "          [-0.0005, -0.0011, -0.0016]],\n",
       " \n",
       "         [[ 0.0011,  0.0022,  0.0033],\n",
       "          [ 0.0009,  0.0018,  0.0027]]], grad_fn=<PermuteBackward0>))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out.permute(1, 0, 2) # [M,H,O]\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "53177d08-6b0f-4133-92a7-4a57e1ff874a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 6]),\n",
       " tensor([[ 0.0010,  0.0021,  0.0031,  0.0008,  0.0016,  0.0023],\n",
       "         [ 0.0040,  0.0079,  0.0119,  0.0039,  0.0078,  0.0118],\n",
       "         [ 0.0006,  0.0012,  0.0018,  0.0010,  0.0020,  0.0031],\n",
       "         [ 0.0010,  0.0020,  0.0031,  0.0024,  0.0048,  0.0071],\n",
       "         [ 0.0019,  0.0038,  0.0057,  0.0018,  0.0035,  0.0053],\n",
       "         [ 0.0010,  0.0019,  0.0029,  0.0008,  0.0015,  0.0023],\n",
       "         [ 0.0010,  0.0019,  0.0029,  0.0008,  0.0015,  0.0023],\n",
       "         [-0.0024, -0.0047, -0.0071, -0.0025, -0.0050, -0.0074],\n",
       "         [ 0.0009,  0.0018,  0.0028,  0.0007,  0.0014,  0.0021],\n",
       "         [ 0.0013,  0.0027,  0.0040,  0.0011,  0.0022,  0.0033],\n",
       "         [-0.0004, -0.0007, -0.0011, -0.0005, -0.0011, -0.0016],\n",
       "         [ 0.0011,  0.0022,  0.0033,  0.0009,  0.0018,  0.0027]],\n",
       "        grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out.reshape(B_batch*N_nodes, heads * head_dim)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c80e6ca-764d-4a0e-a1e5-0c143639f8a4",
   "metadata": {},
   "source": [
    "##### Add Bias\n",
    "\n",
    "Finally, the layer will add in a learned bias. The bias at the end lets each output channel learn its own baseline activation independent of the neighbor messages. Without it, every feature would be forced to be purely a weighted combination of neighbors, which can make it harder to represent simple shifts like “always on” or “always slightly suppressed” for certain dimensions. Adding the bias is a cheap way to increase expressiveness and gives the model flexibility to recenter or offset aggregated signals, especially important when stacking layers where small systematic shifts can accumulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "11dd3732-a2a3-409a-a8a9-c2af7cd3c9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6]),\n",
       " Parameter containing:\n",
       " tensor([1.0000e-06, 1.0000e-06, 1.0000e-06, 1.0000e-06, 1.0000e-06, 1.0000e-06],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gat1_bias = nn.Parameter(torch.zeros(heads*head_dim))\n",
    "with torch.no_grad(): \n",
    "    sign_pattern = torch.tensor([1e-6])\n",
    "    gat1_bias.copy_(sign_pattern)\n",
    "gat1_bias.size(), gat1_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f5cded6-55ac-4ee0-a96c-f0d9f875ace7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 6]),\n",
       " tensor([[ 0.0010,  0.0021,  0.0031,  0.0008,  0.0016,  0.0023],\n",
       "         [ 0.0040,  0.0079,  0.0119,  0.0039,  0.0078,  0.0118],\n",
       "         [ 0.0006,  0.0012,  0.0018,  0.0010,  0.0020,  0.0031],\n",
       "         [ 0.0010,  0.0020,  0.0031,  0.0024,  0.0048,  0.0071],\n",
       "         [ 0.0019,  0.0038,  0.0057,  0.0018,  0.0035,  0.0053],\n",
       "         [ 0.0010,  0.0019,  0.0029,  0.0008,  0.0015,  0.0023],\n",
       "         [ 0.0010,  0.0019,  0.0029,  0.0008,  0.0015,  0.0023],\n",
       "         [-0.0024, -0.0047, -0.0071, -0.0025, -0.0050, -0.0074],\n",
       "         [ 0.0009,  0.0018,  0.0028,  0.0007,  0.0014,  0.0021],\n",
       "         [ 0.0013,  0.0027,  0.0040,  0.0011,  0.0022,  0.0033],\n",
       "         [-0.0004, -0.0007, -0.0011, -0.0005, -0.0011, -0.0016],\n",
       "         [ 0.0011,  0.0022,  0.0033,  0.0009,  0.0018,  0.0027]],\n",
       "        grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out + gat1_bias\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca31eb10-229a-4011-871f-b4243d705037",
   "metadata": {},
   "source": [
    "#### GAT Block - ELU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7622f00-b47d-4cf4-a286-e9ab9acfb9fd",
   "metadata": {},
   "source": [
    "Now we'll add in a Exponential Linear Unit (ELU) based non-linearity. ELU provides a smooth, non-saturating nonlinearity that preserves negative information which is useful in attention-based and signed-graph settings. For an input (x), ELU is defined as\n",
    "$$\n",
    "\\mathrm{ELU}(x) =\n",
    "\\begin{cases}\n",
    "x & x > 0\\\\\n",
    "\\alpha(\\exp(x) - 1) & x \\le 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Unlike ReLU, which hard-clips all negative values to zero and Leaky ReLU that scales negatives, ELU maps them to bounded negative outputs in a smooth way, maintaining gradient flow and allowing the model to represent “suppressed” or inhibitory signals without erasing them. Between our GAT layer, this helps stabilize training after attention and aggregation, keeps features well-behaved around zero, and works naturally with both positive and negative contributions coming from signed edges and attention weights.\n",
    "\n",
    "Since our values at this point are all relatively small, even the negatives don't significantly change.  If our values were more spread out we'd see a larger shift. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "372f23d6-d418-4e05-b689-1be01986eb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "gat_elu1 = nn.ELU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "72bb919e-cb9c-49df-b953-bb4ba68a6ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 6]),\n",
       " tensor([[ 0.0010,  0.0021,  0.0031,  0.0008,  0.0016,  0.0023],\n",
       "         [ 0.0040,  0.0079,  0.0119,  0.0039,  0.0078,  0.0118],\n",
       "         [ 0.0006,  0.0012,  0.0018,  0.0010,  0.0020,  0.0031],\n",
       "         [ 0.0010,  0.0020,  0.0031,  0.0024,  0.0048,  0.0071],\n",
       "         [ 0.0019,  0.0038,  0.0057,  0.0018,  0.0035,  0.0053],\n",
       "         [ 0.0010,  0.0019,  0.0029,  0.0008,  0.0015,  0.0023],\n",
       "         [ 0.0010,  0.0019,  0.0029,  0.0008,  0.0015,  0.0023],\n",
       "         [-0.0023, -0.0047, -0.0070, -0.0025, -0.0050, -0.0074],\n",
       "         [ 0.0009,  0.0018,  0.0028,  0.0007,  0.0014,  0.0021],\n",
       "         [ 0.0013,  0.0027,  0.0040,  0.0011,  0.0022,  0.0033],\n",
       "         [-0.0004, -0.0007, -0.0011, -0.0005, -0.0011, -0.0016],\n",
       "         [ 0.0011,  0.0022,  0.0033,  0.0009,  0.0018,  0.0027]],\n",
       "        grad_fn=<EluBackward0>))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = gat_elu1(out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59824bf4-3443-48a8-a23f-6ae6ab069fd8",
   "metadata": {},
   "source": [
    "#### GAT Block - Second Attention Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebfc62c-98b6-48a7-9f5e-6268e4b74c4b",
   "metadata": {},
   "source": [
    "<img src=\"explainer_screenshots/gat/default_gat.png\" width=\"400\">\n",
    "\n",
    "Our second GAT layer does not use learned adjacency attention and is closer to a default GAT layer.  The only adjustment is the inclusion of a signed multiplier to account for negative edges. This means that our attention is:\n",
    "$$\n",
    "e_{ij}^{h}=\\text{LeakyReLU}\\big((a_{src}^{h})^\\top W^{h}x_i + (a_{dst}^{h})^\\top W^{h}x_j\\big),\n",
    "$$\n",
    "\n",
    "By not including the signed edge attention this layer's attention must learn the adjacency weights without any aid. The benefit is that it allows the attention layer to decide how much to mix each neighbor via the non-negative weights via softmax. By still including the message transform we allow for the positive or negative components to be included. \n",
    "\n",
    "For this GAT layer, we'll have the same layers as the previous with the only exclusion being the signed edge attention. As a reminder we'll do the same regularization, dropout and other layers in the same order as this is learning the \"second-hop\" of the graph.  We'll still include multiple heads so that the model can learn different concepts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23313e04-bbea-4999-ad65-00d024e30948",
   "metadata": {},
   "source": [
    "##### Feature Dropout\n",
    "\n",
    "We'll again perform feature level dropout for the output of the ELU layer. As a reminder, this dropout will randomly zero out any value effectively removing that specific node from impacting prediction. Since this is Bernoulli based dropout, in addition to zeroing out weights the surviving entries are scaled by $1/(1-p)$.  We continue to maintain the benefits of feature level dropout  being “don’t over-trust any single feature dimension.” \n",
    "\n",
    "You'll again see the zeroing out of random values and a 10% increase in the value of each row due to the normalization that dropout does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fa6dfbd0-a558-4b71-be21-d8e15afe7de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 6]),\n",
       " tensor([[ 0.0011,  0.0023,  0.0034,  0.0009,  0.0017,  0.0026],\n",
       "         [ 0.0044,  0.0088,  0.0132,  0.0044,  0.0087,  0.0131],\n",
       "         [ 0.0007,  0.0014,  0.0020,  0.0011,  0.0023,  0.0000],\n",
       "         [ 0.0011,  0.0023,  0.0034,  0.0026,  0.0053,  0.0079],\n",
       "         [ 0.0021,  0.0042,  0.0063,  0.0020,  0.0039,  0.0059],\n",
       "         [ 0.0011,  0.0021,  0.0032,  0.0008,  0.0017,  0.0025],\n",
       "         [ 0.0011,  0.0022,  0.0032,  0.0008,  0.0017,  0.0025],\n",
       "         [-0.0026, -0.0052, -0.0078, -0.0028, -0.0055, -0.0000],\n",
       "         [ 0.0010,  0.0021,  0.0000,  0.0008,  0.0016,  0.0023],\n",
       "         [ 0.0015,  0.0030,  0.0044,  0.0012,  0.0000,  0.0037],\n",
       "         [-0.0004, -0.0008, -0.0012, -0.0006, -0.0012, -0.0018],\n",
       "         [ 0.0012,  0.0000,  0.0036,  0.0010,  0.0020,  0.0030]],\n",
       "        grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropout on features (standard GAT)\n",
    "gat2_fdrop = nn.Dropout(feat_dropout)\n",
    "out_drop = gat2_fdrop(out)\n",
    "out_drop.size(), out_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7fbce1-d511-4d21-af2b-d88c2d50f712",
   "metadata": {},
   "source": [
    "##### Per-Head Linear Projection\n",
    "\n",
    "Again, the first step will be a multi-headed linear projection of the node features.  This projection is the same as in the first GAT layer where we create a separate weighted matrix per head so that each head can learn concepts independently. This slices the embedding space into learned sub-spaces for attention to interact with. \n",
    "\n",
    "We'll initialize both heads at the same sliding weights at 2x the scale of the first GAT layer.  You'll see that because we initialize the heads with the same values, the output of this layer is the same for both heads and the channels scale by 2x. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c0e4cddd-33a6-4a4e-9664-a38a8ce63407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 3]),\n",
       " Parameter containing:\n",
       " tensor([[[0.2000, 0.4000, 0.6000],\n",
       "          [0.2000, 0.4000, 0.6000],\n",
       "          [0.2000, 0.4000, 0.6000],\n",
       "          [0.2000, 0.4000, 0.6000],\n",
       "          [0.2000, 0.4000, 0.6000],\n",
       "          [0.2000, 0.4000, 0.6000]],\n",
       " \n",
       "         [[0.2000, 0.4000, 0.6000],\n",
       "          [0.2000, 0.4000, 0.6000],\n",
       "          [0.2000, 0.4000, 0.6000],\n",
       "          [0.2000, 0.4000, 0.6000],\n",
       "          [0.2000, 0.4000, 0.6000],\n",
       "          [0.2000, 0.4000, 0.6000]]], requires_grad=True))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gat2_attn_w = nn.Parameter(torch.empty(heads, n_embd, head_dim))\n",
    "with torch.no_grad(): \n",
    "    vs, d = n_embd, head_dim\n",
    "    rows = torch.arange(vs).unsqueeze(1)  # (vs,1)\n",
    "    cols = torch.arange(d).unsqueeze(0)  # (1,d)\n",
    "    pattern = 0.2*(1 + cols)  # W[i,j] = 0.001*(1+i+j)\n",
    "    gat2_attn_w.copy_(pattern)\n",
    "gat2_attn_w.size(), gat2_attn_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "36f7f3bf-87df-423d-b41a-9c622651c024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 12, 3]),\n",
       " tensor([[[ 0.0024,  0.0048,  0.0072],\n",
       "          [ 0.0105,  0.0210,  0.0315],\n",
       "          [ 0.0015,  0.0030,  0.0045],\n",
       "          [ 0.0045,  0.0091,  0.0136],\n",
       "          [ 0.0049,  0.0098,  0.0147],\n",
       "          [ 0.0023,  0.0046,  0.0069],\n",
       "          [ 0.0023,  0.0046,  0.0069],\n",
       "          [-0.0048, -0.0096, -0.0143],\n",
       "          [ 0.0015,  0.0031,  0.0046],\n",
       "          [ 0.0028,  0.0055,  0.0083],\n",
       "          [-0.0012, -0.0024, -0.0036],\n",
       "          [ 0.0022,  0.0043,  0.0065]],\n",
       " \n",
       "         [[ 0.0024,  0.0048,  0.0072],\n",
       "          [ 0.0105,  0.0210,  0.0315],\n",
       "          [ 0.0015,  0.0030,  0.0045],\n",
       "          [ 0.0045,  0.0091,  0.0136],\n",
       "          [ 0.0049,  0.0098,  0.0147],\n",
       "          [ 0.0023,  0.0046,  0.0069],\n",
       "          [ 0.0023,  0.0046,  0.0069],\n",
       "          [-0.0048, -0.0096, -0.0143],\n",
       "          [ 0.0015,  0.0031,  0.0046],\n",
       "          [ 0.0028,  0.0055,  0.0083],\n",
       "          [-0.0012, -0.0024, -0.0036],\n",
       "          [ 0.0022,  0.0043,  0.0065]]], grad_fn=<CloneBackward0>))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hproj = out_drop @ gat2_attn_w\n",
    "Hproj.size(), Hproj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ef1cc7-b4ed-4893-a0aa-a74d8f904f7c",
   "metadata": {},
   "source": [
    "#####  Linear Node Attention\n",
    "\n",
    "We'll again build out the linear node attention. This attention takes the per-head project and starts creating edge-specific weights between each pair of connected nodes. This is where our GAT layer begins to learn the adjacency importance. We still keep separation by head so that each head can learn its own relationships. This performs the same calculation that we did for linear node attention in the first GAT layer. As a reminder, these learned weights represent “how much should node $i$ listen to node $j$ for this head.” This makes neighbor influence data-driven and asymmetric: different heads can specialize to different patterns (e.g. strong T-cell–like signatures, aberrant cancer patterns), and each node can selectively amplify or suppress specific neighbors rather than averaging them uniformly.\n",
    "\n",
    "The biggest difference is that this will be the only network attention we'll learn in this layer so it will have to do more of the heavy lifting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3f0d0a-7290-4cce-909f-bab9f54bf4db",
   "metadata": {},
   "source": [
    "**Source Node (Query)**\n",
    "\n",
    "We'll again start with the source first. We'll initialize the heads so that the first head is triple the first.  As a result you can see that the output of this shows the first head as triple the second. We will also initialize with consistent weights across the head dimension so that we can see in backprop how the model changes the importance of the different dimensions thereby building a network representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e09a9bb4-3fde-4b35-a8e5-8b8b8c12273a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 1]),\n",
       " Parameter containing:\n",
       " tensor([[[0.3000],\n",
       "          [0.3000],\n",
       "          [0.3000]],\n",
       " \n",
       "         [[0.1000],\n",
       "          [0.1000],\n",
       "          [0.1000]]], requires_grad=True))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gat2_attn_src = nn.Parameter(torch.empty(heads, head_dim, 1))\n",
    "with torch.no_grad(): \n",
    "    src_pattern = torch.tensor([[[0.3],[0.3],[0.3]],[[0.1],[0.1],[0.1]]])\n",
    "    gat2_attn_src.copy_(src_pattern)\n",
    "gat2_attn_src.size(), gat2_attn_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e2e0ced6-7f17-470c-ab0a-8d80fcb73cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 12, 1]),\n",
       " tensor([[[ 0.0043],\n",
       "          [ 0.0189],\n",
       "          [ 0.0027],\n",
       "          [ 0.0082],\n",
       "          [ 0.0088],\n",
       "          [ 0.0041],\n",
       "          [ 0.0041],\n",
       "          [-0.0086],\n",
       "          [ 0.0028],\n",
       "          [ 0.0050],\n",
       "          [-0.0022],\n",
       "          [ 0.0039]],\n",
       " \n",
       "         [[ 0.0014],\n",
       "          [ 0.0063],\n",
       "          [ 0.0009],\n",
       "          [ 0.0027],\n",
       "          [ 0.0029],\n",
       "          [ 0.0014],\n",
       "          [ 0.0014],\n",
       "          [-0.0029],\n",
       "          [ 0.0009],\n",
       "          [ 0.0017],\n",
       "          [-0.0007],\n",
       "          [ 0.0013]]], grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_src = Hproj @ gat2_attn_src\n",
    "e_src.size(), e_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0fab3c08-8cd6-4224-a7c9-f7f7026f24d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 1, 2]),\n",
       " tensor([[[ 0.0043,  0.0014]],\n",
       " \n",
       "         [[ 0.0189,  0.0063]],\n",
       " \n",
       "         [[ 0.0027,  0.0009]],\n",
       " \n",
       "         [[ 0.0082,  0.0027]],\n",
       " \n",
       "         [[ 0.0088,  0.0029]],\n",
       " \n",
       "         [[ 0.0041,  0.0014]],\n",
       " \n",
       "         [[ 0.0041,  0.0014]],\n",
       " \n",
       "         [[-0.0086, -0.0029]],\n",
       " \n",
       "         [[ 0.0028,  0.0009]],\n",
       " \n",
       "         [[ 0.0050,  0.0017]],\n",
       " \n",
       "         [[-0.0022, -0.0007]],\n",
       " \n",
       "         [[ 0.0039,  0.0013]]], grad_fn=<PermuteBackward0>))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_src = e_src.permute(1, 2, 0)  # [M,1,H]\n",
    "e_src.size(), e_src\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc88fc34-0b04-4eb1-8ae8-cabd3571a661",
   "metadata": {},
   "source": [
    "**Destination Node (Key)**\n",
    "\n",
    "Next we'll again build the destination attention. We'll initialize the heads so that the second head is 2x the first and that both heads are positive.  As a result you can see the ratio is maintained for the destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cba8c334-3e6a-4086-936b-af60944b1774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 1]),\n",
       " Parameter containing:\n",
       " tensor([[[0.1000],\n",
       "          [0.1000],\n",
       "          [0.1000]],\n",
       " \n",
       "         [[0.2000],\n",
       "          [0.2000],\n",
       "          [0.2000]]], requires_grad=True))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gat2_attn_dst = nn.Parameter(torch.empty(heads, head_dim, 1))\n",
    "with torch.no_grad(): \n",
    "    dst_pattern = torch.tensor([[[0.1],[0.1],[0.1]],[[0.2],[0.2],[0.2]]])\n",
    "    gat2_attn_dst.copy_(dst_pattern)\n",
    "gat2_attn_dst.size(), gat2_attn_dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e075d366-cb9f-49a9-b236-1023cd1e8695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 12, 1]),\n",
       " tensor([[[ 0.0014],\n",
       "          [ 0.0063],\n",
       "          [ 0.0009],\n",
       "          [ 0.0027],\n",
       "          [ 0.0029],\n",
       "          [ 0.0014],\n",
       "          [ 0.0014],\n",
       "          [-0.0029],\n",
       "          [ 0.0009],\n",
       "          [ 0.0017],\n",
       "          [-0.0007],\n",
       "          [ 0.0013]],\n",
       " \n",
       "         [[ 0.0029],\n",
       "          [ 0.0126],\n",
       "          [ 0.0018],\n",
       "          [ 0.0054],\n",
       "          [ 0.0059],\n",
       "          [ 0.0028],\n",
       "          [ 0.0028],\n",
       "          [-0.0057],\n",
       "          [ 0.0019],\n",
       "          [ 0.0033],\n",
       "          [-0.0014],\n",
       "          [ 0.0026]]], grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_dst = Hproj @ gat2_attn_dst\n",
    "e_dst.size(), e_dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6d308531-7425-4c18-9494-e39328d4e3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 12, 2]),\n",
       " tensor([[[ 0.0014,  0.0029],\n",
       "          [ 0.0063,  0.0126],\n",
       "          [ 0.0009,  0.0018],\n",
       "          [ 0.0027,  0.0054],\n",
       "          [ 0.0029,  0.0059],\n",
       "          [ 0.0014,  0.0028],\n",
       "          [ 0.0014,  0.0028],\n",
       "          [-0.0029, -0.0057],\n",
       "          [ 0.0009,  0.0019],\n",
       "          [ 0.0017,  0.0033],\n",
       "          [-0.0007, -0.0014],\n",
       "          [ 0.0013,  0.0026]]], grad_fn=<PermuteBackward0>))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_dst = e_dst.permute(2, 1, 0)\n",
    "e_dst.size(), e_dst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f96c52-d1a9-43a1-938f-cdeb79da809d",
   "metadata": {},
   "source": [
    "**Combine Node Directions Together**\n",
    "\n",
    "Again we'll combine our two attention weights together.  This combination creates `[12,12]` weights creating the dimension necessary to learn the network that we have encoded in `a_blk`.  You'll see that this matches up with the dimensions of our adjacency map which is a nice mapping and shows what these weights are attempting to learn.  \n",
    "\n",
    "Because we used positive weights for both source and destination, with scalers bigger than the source, you'll see that the weights are additive and many negatives are removed.  This is not a worry since we'll add in direction after the softmax. Also, because of how our input `x_token` and embeddings have been initiated, you'll see that we actually have a repetition of two equal `[6,12,2]`.  As we do backprop and learning, this should adjust. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a3afb520-f5c9-49bc-b32d-66a1dbf18456",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 2]),\n",
       " tensor([[[ 5.7896e-03,  4.3422e-03],\n",
       "          [ 1.0641e-02,  1.4045e-02],\n",
       "          [ 5.2368e-03,  3.2366e-03],\n",
       "          [ 7.0614e-03,  6.8858e-03],\n",
       "          [ 7.2831e-03,  7.3292e-03],\n",
       "          [ 5.7196e-03,  4.2021e-03],\n",
       "          [ 5.7224e-03,  4.2078e-03],\n",
       "          [ 1.4760e-03, -4.2851e-03],\n",
       "          [ 5.2721e-03,  3.3072e-03],\n",
       "          [ 6.0006e-03,  4.7641e-03],\n",
       "          [ 3.6181e-03, -8.9093e-07],\n",
       "          [ 5.6448e-03,  4.0526e-03]],\n",
       " \n",
       "         [[ 2.0344e-02,  9.1936e-03],\n",
       "          [ 2.5195e-02,  1.8896e-02],\n",
       "          [ 1.9791e-02,  8.0881e-03],\n",
       "          [ 2.1616e-02,  1.1737e-02],\n",
       "          [ 2.1837e-02,  1.2181e-02],\n",
       "          [ 2.0274e-02,  9.0535e-03],\n",
       "          [ 2.0277e-02,  9.0592e-03],\n",
       "          [ 1.6030e-02,  5.6629e-04],\n",
       "          [ 1.9826e-02,  8.1586e-03],\n",
       "          [ 2.0555e-02,  9.6155e-03],\n",
       "          [ 1.8172e-02,  4.8505e-03],\n",
       "          [ 2.0199e-02,  8.9040e-03]],\n",
       " \n",
       "         [[ 4.1313e-03,  3.7894e-03],\n",
       "          [ 8.9827e-03,  1.3492e-02],\n",
       "          [ 3.5785e-03,  2.6838e-03],\n",
       "          [ 5.4031e-03,  6.3330e-03],\n",
       "          [ 5.6247e-03,  6.7764e-03],\n",
       "          [ 4.0612e-03,  3.6493e-03],\n",
       "          [ 4.0641e-03,  3.6550e-03],\n",
       "          [-1.8242e-04, -4.8379e-03],\n",
       "          [ 3.6137e-03,  2.7544e-03],\n",
       "          [ 4.3422e-03,  4.2113e-03],\n",
       "          [ 1.9597e-03, -5.5368e-04],\n",
       "          [ 3.9865e-03,  3.4998e-03]],\n",
       " \n",
       "         [[ 9.6050e-03,  5.6140e-03],\n",
       "          [ 1.4456e-02,  1.5317e-02],\n",
       "          [ 9.0522e-03,  4.5084e-03],\n",
       "          [ 1.0877e-02,  8.1576e-03],\n",
       "          [ 1.1098e-02,  8.6009e-03],\n",
       "          [ 9.5350e-03,  5.4739e-03],\n",
       "          [ 9.5378e-03,  5.4796e-03],\n",
       "          [ 5.2913e-03, -3.0133e-03],\n",
       "          [ 9.0875e-03,  4.5789e-03],\n",
       "          [ 9.8159e-03,  6.0359e-03],\n",
       "          [ 7.4335e-03,  1.2709e-03],\n",
       "          [ 9.4602e-03,  5.3244e-03]],\n",
       " \n",
       "         [[ 1.0270e-02,  5.8357e-03],\n",
       "          [ 1.5121e-02,  1.5539e-02],\n",
       "          [ 9.7172e-03,  4.7301e-03],\n",
       "          [ 1.1542e-02,  8.3793e-03],\n",
       "          [ 1.1763e-02,  8.8226e-03],\n",
       "          [ 1.0200e-02,  5.6956e-03],\n",
       "          [ 1.0203e-02,  5.7013e-03],\n",
       "          [ 5.9563e-03, -2.7917e-03],\n",
       "          [ 9.7525e-03,  4.8006e-03],\n",
       "          [ 1.0481e-02,  6.2575e-03],\n",
       "          [ 8.0985e-03,  1.4926e-03],\n",
       "          [ 1.0125e-02,  5.5461e-03]],\n",
       " \n",
       "         [[ 5.5795e-03,  4.2722e-03],\n",
       "          [ 1.0431e-02,  1.3975e-02],\n",
       "          [ 5.0267e-03,  3.1666e-03],\n",
       "          [ 6.8513e-03,  6.8158e-03],\n",
       "          [ 7.0730e-03,  7.2591e-03],\n",
       "          [ 5.5094e-03,  4.1321e-03],\n",
       "          [ 5.5123e-03,  4.1378e-03],\n",
       "          [ 1.2658e-03, -4.3552e-03],\n",
       "          [ 5.0620e-03,  3.2371e-03],\n",
       "          [ 5.7904e-03,  4.6940e-03],\n",
       "          [ 3.4079e-03, -7.0937e-05],\n",
       "          [ 5.4347e-03,  3.9826e-03]],\n",
       " \n",
       "         [[ 5.5881e-03,  4.2750e-03],\n",
       "          [ 1.0439e-02,  1.3978e-02],\n",
       "          [ 5.0353e-03,  3.1694e-03],\n",
       "          [ 6.8598e-03,  6.8186e-03],\n",
       "          [ 7.0815e-03,  7.2620e-03],\n",
       "          [ 5.5180e-03,  4.1349e-03],\n",
       "          [ 5.5209e-03,  4.1406e-03],\n",
       "          [ 1.2744e-03, -4.3523e-03],\n",
       "          [ 5.0705e-03,  3.2400e-03],\n",
       "          [ 5.7990e-03,  4.6969e-03],\n",
       "          [ 3.4165e-03, -6.8085e-05],\n",
       "          [ 5.4433e-03,  3.9854e-03]],\n",
       " \n",
       "         [[-7.1514e-03,  2.8550e-05],\n",
       "          [-2.3000e-03,  9.7314e-03],\n",
       "          [-7.7042e-03, -1.0770e-03],\n",
       "          [-5.8796e-03,  2.5721e-03],\n",
       "          [-5.6579e-03,  3.0155e-03],\n",
       "          [-7.2214e-03, -1.1154e-04],\n",
       "          [-7.2186e-03, -1.0584e-04],\n",
       "          [-1.1465e-02, -8.5988e-03],\n",
       "          [-7.6689e-03, -1.0065e-03],\n",
       "          [-6.9405e-03,  4.5039e-04],\n",
       "          [-9.3229e-03, -4.3146e-03],\n",
       "          [-7.2962e-03, -2.6105e-04]],\n",
       " \n",
       "         [[ 4.2370e-03,  3.8247e-03],\n",
       "          [ 9.0884e-03,  1.3528e-02],\n",
       "          [ 3.6842e-03,  2.7191e-03],\n",
       "          [ 5.5088e-03,  6.3683e-03],\n",
       "          [ 5.7305e-03,  6.8116e-03],\n",
       "          [ 4.1670e-03,  3.6846e-03],\n",
       "          [ 4.1698e-03,  3.6903e-03],\n",
       "          [-7.6648e-05, -4.8027e-03],\n",
       "          [ 3.7195e-03,  2.7896e-03],\n",
       "          [ 4.4479e-03,  4.2465e-03],\n",
       "          [ 2.0655e-03, -5.1843e-04],\n",
       "          [ 4.0922e-03,  3.5351e-03]],\n",
       " \n",
       "         [[ 6.4224e-03,  4.5531e-03],\n",
       "          [ 1.1274e-02,  1.4256e-02],\n",
       "          [ 5.8696e-03,  3.4476e-03],\n",
       "          [ 7.6942e-03,  7.0967e-03],\n",
       "          [ 7.9159e-03,  7.5401e-03],\n",
       "          [ 6.3523e-03,  4.4131e-03],\n",
       "          [ 6.3552e-03,  4.4188e-03],\n",
       "          [ 2.1087e-03, -4.0742e-03],\n",
       "          [ 5.9049e-03,  3.5181e-03],\n",
       "          [ 6.6333e-03,  4.9750e-03],\n",
       "          [ 4.2508e-03,  2.1003e-04],\n",
       "          [ 6.2776e-03,  4.2635e-03]],\n",
       " \n",
       "         [[-7.2504e-04,  2.1707e-03],\n",
       "          [ 4.1264e-03,  1.1873e-02],\n",
       "          [-1.2778e-03,  1.0651e-03],\n",
       "          [ 5.4675e-04,  4.7143e-03],\n",
       "          [ 7.6842e-04,  5.1576e-03],\n",
       "          [-7.9509e-04,  2.0306e-03],\n",
       "          [-7.9223e-04,  2.0363e-03],\n",
       "          [-5.0387e-03, -6.4567e-03],\n",
       "          [-1.2426e-03,  1.1356e-03],\n",
       "          [-5.1412e-04,  2.5925e-03],\n",
       "          [-2.8966e-03, -2.1724e-03],\n",
       "          [-8.6984e-04,  1.8811e-03]],\n",
       " \n",
       "         [[ 5.3552e-03,  4.1974e-03],\n",
       "          [ 1.0207e-02,  1.3900e-02],\n",
       "          [ 4.8024e-03,  3.0918e-03],\n",
       "          [ 6.6270e-03,  6.7410e-03],\n",
       "          [ 6.8487e-03,  7.1844e-03],\n",
       "          [ 5.2852e-03,  4.0573e-03],\n",
       "          [ 5.2880e-03,  4.0630e-03],\n",
       "          [ 1.0416e-03, -4.4299e-03],\n",
       "          [ 4.8377e-03,  3.1624e-03],\n",
       "          [ 5.5662e-03,  4.6193e-03],\n",
       "          [ 3.1837e-03, -1.4569e-04],\n",
       "          [ 5.2104e-03,  3.9078e-03]]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = e_src + e_dst            # [M,M,H\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7de4d3d-6291-434e-ad37-8150ec5fc584",
   "metadata": {},
   "source": [
    "##### Leaky ReLU\n",
    "\n",
    "Next we again apply leaky ReLU (rectified linear units), an element-wise nonlinearity that leaves positive activations unchanged and scales negative activations by a small factor, in our case `0.5`. In graph networks it is commonly used to introduce nonlinearity while preserving gradient flow through negative values and avoiding “dying units.” Additionally, leaky ReLU preserves relative ordering between moderately negative vs strongly negative scores, which still matters once you apply the softmax across neighbors. We apply the same formula as in the first GAT layer. \n",
    "\n",
    "As a reminder, Leaky ReLU is stateless, meaning it has no learnable parameters.  As an output you can see that positive values remained in their state while negative values were cut in half. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c1b6438e-21b1-490c-b178-c4371d3ebf69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 2]),\n",
       " tensor([[[ 5.7896e-03,  4.3422e-03],\n",
       "          [ 1.0641e-02,  1.4045e-02],\n",
       "          [ 5.2368e-03,  3.2366e-03],\n",
       "          [ 7.0614e-03,  6.8858e-03],\n",
       "          [ 7.2831e-03,  7.3292e-03],\n",
       "          [ 5.7196e-03,  4.2021e-03],\n",
       "          [ 5.7224e-03,  4.2078e-03],\n",
       "          [ 1.4760e-03, -2.1426e-03],\n",
       "          [ 5.2721e-03,  3.3072e-03],\n",
       "          [ 6.0006e-03,  4.7641e-03],\n",
       "          [ 3.6181e-03, -4.4546e-07],\n",
       "          [ 5.6448e-03,  4.0526e-03]],\n",
       " \n",
       "         [[ 2.0344e-02,  9.1936e-03],\n",
       "          [ 2.5195e-02,  1.8896e-02],\n",
       "          [ 1.9791e-02,  8.0881e-03],\n",
       "          [ 2.1616e-02,  1.1737e-02],\n",
       "          [ 2.1837e-02,  1.2181e-02],\n",
       "          [ 2.0274e-02,  9.0535e-03],\n",
       "          [ 2.0277e-02,  9.0592e-03],\n",
       "          [ 1.6030e-02,  5.6629e-04],\n",
       "          [ 1.9826e-02,  8.1586e-03],\n",
       "          [ 2.0555e-02,  9.6155e-03],\n",
       "          [ 1.8172e-02,  4.8505e-03],\n",
       "          [ 2.0199e-02,  8.9040e-03]],\n",
       " \n",
       "         [[ 4.1313e-03,  3.7894e-03],\n",
       "          [ 8.9827e-03,  1.3492e-02],\n",
       "          [ 3.5785e-03,  2.6838e-03],\n",
       "          [ 5.4031e-03,  6.3330e-03],\n",
       "          [ 5.6247e-03,  6.7764e-03],\n",
       "          [ 4.0612e-03,  3.6493e-03],\n",
       "          [ 4.0641e-03,  3.6550e-03],\n",
       "          [-9.1210e-05, -2.4190e-03],\n",
       "          [ 3.6137e-03,  2.7544e-03],\n",
       "          [ 4.3422e-03,  4.2113e-03],\n",
       "          [ 1.9597e-03, -2.7684e-04],\n",
       "          [ 3.9865e-03,  3.4998e-03]],\n",
       " \n",
       "         [[ 9.6050e-03,  5.6140e-03],\n",
       "          [ 1.4456e-02,  1.5317e-02],\n",
       "          [ 9.0522e-03,  4.5084e-03],\n",
       "          [ 1.0877e-02,  8.1576e-03],\n",
       "          [ 1.1098e-02,  8.6009e-03],\n",
       "          [ 9.5350e-03,  5.4739e-03],\n",
       "          [ 9.5378e-03,  5.4796e-03],\n",
       "          [ 5.2913e-03, -1.5067e-03],\n",
       "          [ 9.0875e-03,  4.5789e-03],\n",
       "          [ 9.8159e-03,  6.0359e-03],\n",
       "          [ 7.4335e-03,  1.2709e-03],\n",
       "          [ 9.4602e-03,  5.3244e-03]],\n",
       " \n",
       "         [[ 1.0270e-02,  5.8357e-03],\n",
       "          [ 1.5121e-02,  1.5539e-02],\n",
       "          [ 9.7172e-03,  4.7301e-03],\n",
       "          [ 1.1542e-02,  8.3793e-03],\n",
       "          [ 1.1763e-02,  8.8226e-03],\n",
       "          [ 1.0200e-02,  5.6956e-03],\n",
       "          [ 1.0203e-02,  5.7013e-03],\n",
       "          [ 5.9563e-03, -1.3958e-03],\n",
       "          [ 9.7525e-03,  4.8006e-03],\n",
       "          [ 1.0481e-02,  6.2575e-03],\n",
       "          [ 8.0985e-03,  1.4926e-03],\n",
       "          [ 1.0125e-02,  5.5461e-03]],\n",
       " \n",
       "         [[ 5.5795e-03,  4.2722e-03],\n",
       "          [ 1.0431e-02,  1.3975e-02],\n",
       "          [ 5.0267e-03,  3.1666e-03],\n",
       "          [ 6.8513e-03,  6.8158e-03],\n",
       "          [ 7.0730e-03,  7.2591e-03],\n",
       "          [ 5.5094e-03,  4.1321e-03],\n",
       "          [ 5.5123e-03,  4.1378e-03],\n",
       "          [ 1.2658e-03, -2.1776e-03],\n",
       "          [ 5.0620e-03,  3.2371e-03],\n",
       "          [ 5.7904e-03,  4.6940e-03],\n",
       "          [ 3.4079e-03, -3.5469e-05],\n",
       "          [ 5.4347e-03,  3.9826e-03]],\n",
       " \n",
       "         [[ 5.5881e-03,  4.2750e-03],\n",
       "          [ 1.0439e-02,  1.3978e-02],\n",
       "          [ 5.0353e-03,  3.1694e-03],\n",
       "          [ 6.8598e-03,  6.8186e-03],\n",
       "          [ 7.0815e-03,  7.2620e-03],\n",
       "          [ 5.5180e-03,  4.1349e-03],\n",
       "          [ 5.5209e-03,  4.1406e-03],\n",
       "          [ 1.2744e-03, -2.1762e-03],\n",
       "          [ 5.0705e-03,  3.2400e-03],\n",
       "          [ 5.7990e-03,  4.6969e-03],\n",
       "          [ 3.4165e-03, -3.4042e-05],\n",
       "          [ 5.4433e-03,  3.9854e-03]],\n",
       " \n",
       "         [[-3.5757e-03,  2.8550e-05],\n",
       "          [-1.1500e-03,  9.7314e-03],\n",
       "          [-3.8521e-03, -5.3852e-04],\n",
       "          [-2.9398e-03,  2.5721e-03],\n",
       "          [-2.8290e-03,  3.0155e-03],\n",
       "          [-3.6107e-03, -5.5771e-05],\n",
       "          [-3.6093e-03, -5.2919e-05],\n",
       "          [-5.7325e-03, -4.2994e-03],\n",
       "          [-3.8345e-03, -5.0326e-04],\n",
       "          [-3.4702e-03,  4.5039e-04],\n",
       "          [-4.6615e-03, -2.1573e-03],\n",
       "          [-3.6481e-03, -1.3053e-04]],\n",
       " \n",
       "         [[ 4.2370e-03,  3.8247e-03],\n",
       "          [ 9.0884e-03,  1.3528e-02],\n",
       "          [ 3.6842e-03,  2.7191e-03],\n",
       "          [ 5.5088e-03,  6.3683e-03],\n",
       "          [ 5.7305e-03,  6.8116e-03],\n",
       "          [ 4.1670e-03,  3.6846e-03],\n",
       "          [ 4.1698e-03,  3.6903e-03],\n",
       "          [-3.8324e-05, -2.4013e-03],\n",
       "          [ 3.7195e-03,  2.7896e-03],\n",
       "          [ 4.4479e-03,  4.2465e-03],\n",
       "          [ 2.0655e-03, -2.5921e-04],\n",
       "          [ 4.0922e-03,  3.5351e-03]],\n",
       " \n",
       "         [[ 6.4224e-03,  4.5531e-03],\n",
       "          [ 1.1274e-02,  1.4256e-02],\n",
       "          [ 5.8696e-03,  3.4476e-03],\n",
       "          [ 7.6942e-03,  7.0967e-03],\n",
       "          [ 7.9159e-03,  7.5401e-03],\n",
       "          [ 6.3523e-03,  4.4131e-03],\n",
       "          [ 6.3552e-03,  4.4188e-03],\n",
       "          [ 2.1087e-03, -2.0371e-03],\n",
       "          [ 5.9049e-03,  3.5181e-03],\n",
       "          [ 6.6333e-03,  4.9750e-03],\n",
       "          [ 4.2508e-03,  2.1003e-04],\n",
       "          [ 6.2776e-03,  4.2635e-03]],\n",
       " \n",
       "         [[-3.6252e-04,  2.1707e-03],\n",
       "          [ 4.1264e-03,  1.1873e-02],\n",
       "          [-6.3892e-04,  1.0651e-03],\n",
       "          [ 5.4675e-04,  4.7143e-03],\n",
       "          [ 7.6842e-04,  5.1576e-03],\n",
       "          [-3.9754e-04,  2.0306e-03],\n",
       "          [-3.9612e-04,  2.0363e-03],\n",
       "          [-2.5194e-03, -3.2283e-03],\n",
       "          [-6.2129e-04,  1.1356e-03],\n",
       "          [-2.5706e-04,  2.5925e-03],\n",
       "          [-1.4483e-03, -1.0862e-03],\n",
       "          [-4.3492e-04,  1.8811e-03]],\n",
       " \n",
       "         [[ 5.3552e-03,  4.1974e-03],\n",
       "          [ 1.0207e-02,  1.3900e-02],\n",
       "          [ 4.8024e-03,  3.0918e-03],\n",
       "          [ 6.6270e-03,  6.7410e-03],\n",
       "          [ 6.8487e-03,  7.1844e-03],\n",
       "          [ 5.2852e-03,  4.0573e-03],\n",
       "          [ 5.2880e-03,  4.0630e-03],\n",
       "          [ 1.0416e-03, -2.2150e-03],\n",
       "          [ 4.8377e-03,  3.1624e-03],\n",
       "          [ 5.5662e-03,  4.6193e-03],\n",
       "          [ 3.1837e-03, -7.2846e-05],\n",
       "          [ 5.2104e-03,  3.9078e-03]]], grad_fn=<LeakyReluBackward1>))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### regularization\n",
    "gat2_attn_lrelu = nn.LeakyReLU(0.5, inplace=True)\n",
    "out = gat2_attn_lrelu(out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f6c671-5f7e-4d77-be71-1278d5dfe24c",
   "metadata": {},
   "source": [
    "##### Masking\n",
    "\n",
    "Next, we'll do masking again.  Since we did not do signed edge attention we first have to build or mask. In this mask, a `true` value represented a connection that needed to be suppressed. \n",
    "\n",
    "Since this is the branch for learning node connections, we need to mute the weights for where there is no edge so that gradients can't flow backwards there. We'll do this in 2 steps.  First we'll replace the values with $-\\infty$ and then during `softmax` those values will become 0. \n",
    "\n",
    "Similar to other steps, we'll need to unravel our mask to a representation of each edge and then our masked fill will apply the value across both heads. You'll see that after masking the weight sparsity significantly jumps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "72dca296-d11f-4929-a356-032209a99c0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 1]),\n",
       " tensor([[[False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True]],\n",
       " \n",
       "         [[ True],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True]],\n",
       " \n",
       "         [[False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True]],\n",
       " \n",
       "         [[False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [False]]]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signed_a = a_blk.clone()\n",
    "signed_a.fill_diagonal_(0.5)                       # self-loops are positive\n",
    "mask = (signed_a == 0)  \n",
    "mask = mask.unsqueeze(-1)\n",
    "mask.size(), mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4993eedb-9c24-43e6-909a-ac22f88e3156",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 2]),\n",
       " tensor([[[ 5.7896e-03,  4.3422e-03],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [ 7.2831e-03,  7.3292e-03],\n",
       "          [ 5.7196e-03,  4.2021e-03],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf]],\n",
       " \n",
       "         [[       -inf,        -inf],\n",
       "          [ 2.5195e-02,  1.8896e-02],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [ 2.1837e-02,  1.2181e-02],\n",
       "          [ 2.0274e-02,  9.0535e-03],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf]],\n",
       " \n",
       "         [[       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [ 3.5785e-03,  2.6838e-03],\n",
       "          [       -inf,        -inf],\n",
       "          [ 5.6247e-03,  6.7764e-03],\n",
       "          [ 4.0612e-03,  3.6493e-03],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf]],\n",
       " \n",
       "         [[       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [ 1.0877e-02,  8.1576e-03],\n",
       "          [ 1.1098e-02,  8.6009e-03],\n",
       "          [ 9.5350e-03,  5.4739e-03],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf]],\n",
       " \n",
       "         [[ 1.0270e-02,  5.8357e-03],\n",
       "          [ 1.5121e-02,  1.5539e-02],\n",
       "          [ 9.7172e-03,  4.7301e-03],\n",
       "          [ 1.1542e-02,  8.3793e-03],\n",
       "          [ 1.1763e-02,  8.8226e-03],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf]],\n",
       " \n",
       "         [[ 5.5795e-03,  4.2722e-03],\n",
       "          [ 1.0431e-02,  1.3975e-02],\n",
       "          [ 5.0267e-03,  3.1666e-03],\n",
       "          [ 6.8513e-03,  6.8158e-03],\n",
       "          [       -inf,        -inf],\n",
       "          [ 5.5094e-03,  4.1321e-03],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf]],\n",
       " \n",
       "         [[       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [ 5.5209e-03,  4.1406e-03],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [ 3.4165e-03, -3.4042e-05],\n",
       "          [ 5.4433e-03,  3.9854e-03]],\n",
       " \n",
       "         [[       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [-5.7325e-03, -4.2994e-03],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [-4.6615e-03, -2.1573e-03],\n",
       "          [-3.6481e-03, -1.3053e-04]],\n",
       " \n",
       "         [[       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [ 3.7195e-03,  2.7896e-03],\n",
       "          [       -inf,        -inf],\n",
       "          [ 2.0655e-03, -2.5921e-04],\n",
       "          [ 4.0922e-03,  3.5351e-03]],\n",
       " \n",
       "         [[       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [ 6.6333e-03,  4.9750e-03],\n",
       "          [ 4.2508e-03,  2.1003e-04],\n",
       "          [ 6.2776e-03,  4.2635e-03]],\n",
       " \n",
       "         [[       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [-3.9612e-04,  2.0363e-03],\n",
       "          [-2.5194e-03, -3.2283e-03],\n",
       "          [-6.2129e-04,  1.1356e-03],\n",
       "          [-2.5706e-04,  2.5925e-03],\n",
       "          [-1.4483e-03, -1.0862e-03],\n",
       "          [       -inf,        -inf]],\n",
       " \n",
       "         [[       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [       -inf,        -inf],\n",
       "          [ 5.2880e-03,  4.0630e-03],\n",
       "          [ 1.0416e-03, -2.2150e-03],\n",
       "          [ 4.8377e-03,  3.1624e-03],\n",
       "          [ 5.5662e-03,  4.6193e-03],\n",
       "          [       -inf,        -inf],\n",
       "          [ 5.2104e-03,  3.9078e-03]]], grad_fn=<MaskedFillBackward0>))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out.masked_fill(mask, float('-inf'))\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc77e2cf-b902-4ecd-b4a7-c16c76036402",
   "metadata": {},
   "source": [
    "##### SoftMax\n",
    "\n",
    "Now we'll apply the softmax. The softmax turns the attention output into a connection strength measure. In essence this encodes “how strongly each head trusts neighbors\".  Softmax turns a real-valued vector into a probability distribution by exponentiating each component and normalizing by the sum so all outputs are nonnegative and sum to 1. Similar to the first GAT layer, we again apply the following: \n",
    "\n",
    "$\\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}$\n",
    "\n",
    "Similar to the first GAT layer, you'll see that  $-\\infty$ becomes 0 and negatives become positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4e89a2bc-d0ae-4a76-b32b-d66261d43b84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 2]),\n",
       " tensor([[[0.3332, 0.3330],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3337, 0.3340],\n",
       "          [0.3332, 0.3330],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.3343, 0.3352],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3331, 0.3329],\n",
       "          [0.3326, 0.3319],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3331, 0.3328],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3337, 0.3341],\n",
       "          [0.3332, 0.3331],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3335, 0.3336],\n",
       "          [0.3335, 0.3337],\n",
       "          [0.3330, 0.3327],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.1997, 0.1994],\n",
       "          [0.2007, 0.2014],\n",
       "          [0.1996, 0.1992],\n",
       "          [0.2000, 0.1999],\n",
       "          [0.2000, 0.2000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.1998, 0.1996],\n",
       "          [0.2008, 0.2015],\n",
       "          [0.1997, 0.1993],\n",
       "          [0.2000, 0.2001],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.1998, 0.1995],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3336, 0.3338],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3329, 0.3324],\n",
       "          [0.3335, 0.3338]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3330, 0.3326],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3333, 0.3333],\n",
       "          [0.3337, 0.3340]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3335, 0.3336],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3329, 0.3326],\n",
       "          [0.3336, 0.3338]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3336, 0.3339],\n",
       "          [0.3328, 0.3324],\n",
       "          [0.3335, 0.3337]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2001, 0.2003],\n",
       "          [0.1997, 0.1993],\n",
       "          [0.2001, 0.2002],\n",
       "          [0.2002, 0.2005],\n",
       "          [0.1999, 0.1997],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2002, 0.2003],\n",
       "          [0.1993, 0.1990],\n",
       "          [0.2001, 0.2001],\n",
       "          [0.2002, 0.2004],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2002, 0.2002]]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = torch.softmax(out, dim=1)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf7209f-0f07-4a52-b09d-9447ee05389a",
   "metadata": {},
   "source": [
    "##### Attention Dropout\n",
    "\n",
    "We'll run another round of attention dropout. As a reminder, attention dropout is an edge-level / neighbor-level regularization. This dropout reinforces “don’t over-trust any single edge/neighbor.”  \n",
    "\n",
    "Again, this is Bernoulli based dropout, in addition to zeroing out weights the surviving entries are scaled by $1/(1-p)$.  We'll use a p of 0.05 so you'll see the non-0 values increase by that scalar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f64c01da-4c18-4491-b107-75a0a9503db3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 2]),\n",
       " tensor([[[0.3507, 0.3505],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3512, 0.3516],\n",
       "          [0.3507, 0.3505],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.3518, 0.3528],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3507, 0.3505],\n",
       "          [0.3501, 0.3494],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3506, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3513, 0.3517],\n",
       "          [0.3508, 0.3506],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3510, 0.3511],\n",
       "          [0.3511, 0.3513],\n",
       "          [0.3505, 0.3502],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.2102, 0.2099],\n",
       "          [0.2113, 0.2120],\n",
       "          [0.2101, 0.2097],\n",
       "          [0.2105, 0.0000],\n",
       "          [0.2105, 0.2106],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.2103, 0.2101],\n",
       "          [0.2113, 0.2121],\n",
       "          [0.2102, 0.2098],\n",
       "          [0.2106, 0.2106],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2103, 0.2100],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3511, 0.3514],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3504, 0.0000],\n",
       "          [0.3511, 0.3513]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3505, 0.3501],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.3509],\n",
       "          [0.3512, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3510, 0.3511],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3504, 0.3501],\n",
       "          [0.3512, 0.3514]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3512, 0.3515],\n",
       "          [0.3504, 0.3498],\n",
       "          [0.3511, 0.3513]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2107, 0.2109],\n",
       "          [0.2102, 0.2098],\n",
       "          [0.2106, 0.2107],\n",
       "          [0.2107, 0.2110],\n",
       "          [0.0000, 0.2102],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2107, 0.2108],\n",
       "          [0.2098, 0.2095],\n",
       "          [0.2106, 0.2106],\n",
       "          [0.2108, 0.2109],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2107, 0.2108]]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gat2_attn_drop = nn.Dropout(attn_dropout)\n",
    "out = gat2_attn_drop(out) \n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad42fa9-54ab-45e1-a108-59c0d7ce3924",
   "metadata": {},
   "source": [
    "##### Sign Aware Attention\n",
    "\n",
    "Now that we calculated the attention we'll add back in the edge sign. The previous softmax gave us the connection strength but now we need the direction to encode up and down regulation. By taking the Hadamard product we allow each edge to be scaled by the adjacency value making inhibitory edges contribute with a negative weight and excitatory edges with a positive weight, so the same attention pattern can either reinforce or oppose a signal depending on biology. \n",
    "\n",
    "We'll first start by reshaping our current weight and the adjacency map to match dimensions per head. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fa7d5c61-3db2-4555-9643-4ebbfda4f41a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 12, 12]),\n",
       " tensor([[[0.3507, 0.0000, 0.0000, 0.0000, 0.3512, 0.3507, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.3518, 0.0000, 0.0000, 0.3507, 0.3501, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.3506, 0.0000, 0.3513, 0.3508, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.3510, 0.3511, 0.3505, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.2102, 0.2113, 0.2101, 0.2105, 0.2105, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.2103, 0.2113, 0.2102, 0.2106, 0.0000, 0.2103, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3511, 0.0000,\n",
       "           0.0000, 0.0000, 0.3504, 0.3511],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3505,\n",
       "           0.0000, 0.0000, 0.0000, 0.3512],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.3510, 0.0000, 0.3504, 0.3512],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.3512, 0.3504, 0.3511],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2107, 0.2102,\n",
       "           0.2106, 0.2107, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2107, 0.2098,\n",
       "           0.2106, 0.2108, 0.0000, 0.2107]],\n",
       " \n",
       "         [[0.3505, 0.0000, 0.0000, 0.0000, 0.3516, 0.3505, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.3528, 0.0000, 0.0000, 0.3505, 0.3494, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.3517, 0.3506, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.3511, 0.3513, 0.3502, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.2099, 0.2120, 0.2097, 0.0000, 0.2106, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.2101, 0.2121, 0.2098, 0.2106, 0.0000, 0.2100, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3514, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.3513],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3501,\n",
       "           0.0000, 0.0000, 0.3509, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.3511, 0.0000, 0.3501, 0.3514],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.3515, 0.3498, 0.3513],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2109, 0.2098,\n",
       "           0.2107, 0.2110, 0.2102, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2108, 0.2095,\n",
       "           0.2106, 0.2109, 0.0000, 0.2108]]], grad_fn=<PermuteBackward0>))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out.permute(2, 0, 1) # [H,M,M]\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7f019cb6-eeb9-4ff7-bedf-1bb2744b208f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 12, 12]),\n",
       " tensor([[[ 0.5000,  0.0000,  0.0000,  0.0000,  1.0000, -1.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.5000,  0.0000,  0.0000,  1.0000,  1.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.5000,  0.0000,  1.0000, -1.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.5000, -1.0000,  1.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000, -1.0000,  0.5000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.0000,  1.0000, -1.0000,  1.0000,  0.0000,  0.5000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.5000,\n",
       "            0.0000,  0.0000,  0.0000, -1.0000,  1.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.5000,  0.0000,  0.0000, -1.0000, -1.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.5000,  0.0000,  1.0000, -1.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.5000, -1.0000,  1.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.0000,\n",
       "           -1.0000,  1.0000, -1.0000,  0.5000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,\n",
       "           -1.0000, -1.0000,  1.0000,  0.0000,  0.5000]],\n",
       " \n",
       "         [[ 0.5000,  0.0000,  0.0000,  0.0000,  1.0000, -1.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.5000,  0.0000,  0.0000,  1.0000,  1.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.5000,  0.0000,  1.0000, -1.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.5000, -1.0000,  1.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000, -1.0000,  0.5000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.0000,  1.0000, -1.0000,  1.0000,  0.0000,  0.5000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.5000,\n",
       "            0.0000,  0.0000,  0.0000, -1.0000,  1.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.5000,  0.0000,  0.0000, -1.0000, -1.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.5000,  0.0000,  1.0000, -1.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.5000, -1.0000,  1.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.0000,\n",
       "           -1.0000,  1.0000, -1.0000,  0.5000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,\n",
       "           -1.0000, -1.0000,  1.0000,  0.0000,  0.5000]]]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_heads = signed_a.squeeze(-1).unsqueeze(0).expand(heads, -1, -1) # [H,M,M]\n",
    "s_heads.size(), s_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84204c5b-6700-4779-ad19-8f96d4621ad9",
   "metadata": {},
   "source": [
    "**Direction Scaled Attention**\n",
    "\n",
    "Now that we have the dimensions aligned we'll run our scaled multiple.  You'll see that the softmax weights now adopt the direction of the adjacency map and the diagonals divide in half so that they're not quite as impactful as the up-regulated edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4bf639f8-8963-4eac-ada6-bd2e224c14e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 12, 12]),\n",
       " tensor([[[ 0.1754,  0.0000,  0.0000,  0.0000,  0.3512, -0.3507,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.1759,  0.0000,  0.0000,  0.3507,  0.3501,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.1753,  0.0000,  0.3513, -0.3508,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.1755, -0.3511,  0.3505,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.2102,  0.2113,  0.2101, -0.2105,  0.1053,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.2103,  0.2113, -0.2102,  0.2106,  0.0000,  0.1051,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1756,\n",
       "            0.0000,  0.0000,  0.0000, -0.3504,  0.3511],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.1753,  0.0000,  0.0000, -0.0000, -0.3512],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.1755,  0.0000,  0.3504, -0.3512],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.1756, -0.3504,  0.3511],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.2107,\n",
       "           -0.2102,  0.2106, -0.2107,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.2107,\n",
       "           -0.2098, -0.2106,  0.2108,  0.0000,  0.1053]],\n",
       " \n",
       "         [[ 0.1753,  0.0000,  0.0000,  0.0000,  0.3516, -0.3505,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.1764,  0.0000,  0.0000,  0.3505,  0.3494,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.3517, -0.3506,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.1756, -0.3513,  0.3502,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.2099,  0.2120,  0.2097, -0.0000,  0.1053,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.2101,  0.2121, -0.2098,  0.2106,  0.0000,  0.1050,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1757,\n",
       "            0.0000,  0.0000,  0.0000, -0.0000,  0.3513],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.1751,  0.0000,  0.0000, -0.3509, -0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.1756,  0.0000,  0.3501, -0.3514],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.1758, -0.3498,  0.3513],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.2109,\n",
       "           -0.2098,  0.2107, -0.2110,  0.1051,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.2108,\n",
       "           -0.2095, -0.2106,  0.2109,  0.0000,  0.1054]]],\n",
       "        grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out * s_heads # [H,M,M]\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654ea9b4-1d9d-41b3-8665-d213c8048dd4",
   "metadata": {},
   "source": [
    "##### Message Aggregation\n",
    "\n",
    "Now, for each head, we have to combine the neighbor features into updated node embeddings with the learned attention weights.  As a reminder we'll take the dot product of the the learned attention by the node projection to perform \n",
    "$$\n",
    "text{out}_h[h, i, :] = \\sum_j out_h[h, i, j] Hproj[h, j, :]\n",
    "$$\n",
    "\n",
    "This results in us taking a weighted sum over all neighbors’ features per node and per head. This turns attention scores into new node representations. This representation is how nodes aggregate information mostly from neighbors with high attention weights, and different heads learn different aggregation patterns over the same graph.\n",
    "\n",
    "You'll again see that we end up with different values per head because the attention per head now pulls the nodes and channels of the linear embeddings to differing directions.  You can also see the value of the negative edges showing up with node projections now being negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9a43c2b2-ca16-4ffc-94e1-9ce60864ec77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 12, 3]),\n",
       " tensor([[[ 0.0013,  0.0027,  0.0040],\n",
       "          [ 0.0044,  0.0087,  0.0131],\n",
       "          [ 0.0012,  0.0024,  0.0035],\n",
       "          [-0.0001, -0.0002, -0.0004],\n",
       "          [ 0.0026,  0.0052,  0.0078],\n",
       "          [ 0.0026,  0.0052,  0.0078],\n",
       "          [ 0.0016,  0.0032,  0.0048],\n",
       "          [-0.0016, -0.0032, -0.0048],\n",
       "          [-0.0009, -0.0018, -0.0027],\n",
       "          [ 0.0017,  0.0033,  0.0050],\n",
       "          [ 0.0003,  0.0005,  0.0008],\n",
       "          [ 0.0020,  0.0039,  0.0059]],\n",
       " \n",
       "         [[ 0.0013,  0.0027,  0.0040],\n",
       "          [ 0.0044,  0.0087,  0.0131],\n",
       "          [ 0.0009,  0.0018,  0.0028],\n",
       "          [-0.0001, -0.0002, -0.0004],\n",
       "          [ 0.0036,  0.0071,  0.0107],\n",
       "          [ 0.0026,  0.0052,  0.0078],\n",
       "          [ 0.0012,  0.0023,  0.0035],\n",
       "          [-0.0004, -0.0008, -0.0012],\n",
       "          [-0.0009, -0.0018, -0.0027],\n",
       "          [ 0.0017,  0.0033,  0.0050],\n",
       "          [ 0.0001,  0.0003,  0.0004],\n",
       "          [ 0.0020,  0.0039,  0.0059]]], grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out @ Hproj # [H,M,O]\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2ac74d-4043-4440-8a55-6b2a1edab178",
   "metadata": {},
   "source": [
    "##### Concatenate Heads\n",
    "\n",
    "Now that we've calculated our node level weights, we'll collapse our heads together by concatenating them.  This concatenation allows the gradients to maintain the flow through the head while flowing forward to the next layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c068f6a5-0bc4-4912-a66c-c80a14f1dca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 2, 3]),\n",
       " tensor([[[ 0.0013,  0.0027,  0.0040],\n",
       "          [ 0.0013,  0.0027,  0.0040]],\n",
       " \n",
       "         [[ 0.0044,  0.0087,  0.0131],\n",
       "          [ 0.0044,  0.0087,  0.0131]],\n",
       " \n",
       "         [[ 0.0012,  0.0024,  0.0035],\n",
       "          [ 0.0009,  0.0018,  0.0028]],\n",
       " \n",
       "         [[-0.0001, -0.0002, -0.0004],\n",
       "          [-0.0001, -0.0002, -0.0004]],\n",
       " \n",
       "         [[ 0.0026,  0.0052,  0.0078],\n",
       "          [ 0.0036,  0.0071,  0.0107]],\n",
       " \n",
       "         [[ 0.0026,  0.0052,  0.0078],\n",
       "          [ 0.0026,  0.0052,  0.0078]],\n",
       " \n",
       "         [[ 0.0016,  0.0032,  0.0048],\n",
       "          [ 0.0012,  0.0023,  0.0035]],\n",
       " \n",
       "         [[-0.0016, -0.0032, -0.0048],\n",
       "          [-0.0004, -0.0008, -0.0012]],\n",
       " \n",
       "         [[-0.0009, -0.0018, -0.0027],\n",
       "          [-0.0009, -0.0018, -0.0027]],\n",
       " \n",
       "         [[ 0.0017,  0.0033,  0.0050],\n",
       "          [ 0.0017,  0.0033,  0.0050]],\n",
       " \n",
       "         [[ 0.0003,  0.0005,  0.0008],\n",
       "          [ 0.0001,  0.0003,  0.0004]],\n",
       " \n",
       "         [[ 0.0020,  0.0039,  0.0059],\n",
       "          [ 0.0020,  0.0039,  0.0059]]], grad_fn=<PermuteBackward0>))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out.permute(1, 0, 2) # [M,H,O]\n",
    "out.size(), out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0e9f73b4-6958-4c92-bfa3-a96a81aef33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 6]),\n",
       " tensor([[ 0.0013,  0.0027,  0.0040,  0.0013,  0.0027,  0.0040],\n",
       "         [ 0.0044,  0.0087,  0.0131,  0.0044,  0.0087,  0.0131],\n",
       "         [ 0.0012,  0.0024,  0.0035,  0.0009,  0.0018,  0.0028],\n",
       "         [-0.0001, -0.0002, -0.0004, -0.0001, -0.0002, -0.0004],\n",
       "         [ 0.0026,  0.0052,  0.0078,  0.0036,  0.0071,  0.0107],\n",
       "         [ 0.0026,  0.0052,  0.0078,  0.0026,  0.0052,  0.0078],\n",
       "         [ 0.0016,  0.0032,  0.0048,  0.0012,  0.0023,  0.0035],\n",
       "         [-0.0016, -0.0032, -0.0048, -0.0004, -0.0008, -0.0012],\n",
       "         [-0.0009, -0.0018, -0.0027, -0.0009, -0.0018, -0.0027],\n",
       "         [ 0.0017,  0.0033,  0.0050,  0.0017,  0.0033,  0.0050],\n",
       "         [ 0.0003,  0.0005,  0.0008,  0.0001,  0.0003,  0.0004],\n",
       "         [ 0.0020,  0.0039,  0.0059,  0.0020,  0.0039,  0.0059]],\n",
       "        grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out.reshape(B_batch*N_nodes, heads * head_dim)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e159af6-3735-4139-8328-736eed347b08",
   "metadata": {},
   "source": [
    "##### Add Bias\n",
    "\n",
    "Finally we again add bias.  This bias gives this GAT layer a similar benefit that lets each output channel learn its own baseline activation independent of the neighbor messages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "86cdfae9-cf47-4d5d-919d-e335fa4911fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6]),\n",
       " Parameter containing:\n",
       " tensor([1.0000e-06, 1.0000e-06, 1.0000e-06, 1.0000e-06, 1.0000e-06, 1.0000e-06],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Bias\n",
    "gat2_bias = nn.Parameter(torch.zeros(heads*head_dim))\n",
    "with torch.no_grad(): \n",
    "    sign_pattern = torch.tensor([1e-6])\n",
    "    gat2_bias.copy_(sign_pattern)\n",
    "gat2_bias.size(), gat2_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "be1b986b-1101-402a-a393-d831db0c24a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 6]),\n",
       " tensor([[ 0.0013,  0.0027,  0.0040,  0.0013,  0.0027,  0.0040],\n",
       "         [ 0.0044,  0.0087,  0.0131,  0.0044,  0.0087,  0.0131],\n",
       "         [ 0.0012,  0.0024,  0.0035,  0.0009,  0.0018,  0.0028],\n",
       "         [-0.0001, -0.0002, -0.0004, -0.0001, -0.0002, -0.0004],\n",
       "         [ 0.0026,  0.0052,  0.0078,  0.0036,  0.0071,  0.0107],\n",
       "         [ 0.0026,  0.0052,  0.0078,  0.0026,  0.0052,  0.0078],\n",
       "         [ 0.0016,  0.0032,  0.0048,  0.0012,  0.0023,  0.0035],\n",
       "         [-0.0016, -0.0032, -0.0048, -0.0004, -0.0008, -0.0012],\n",
       "         [-0.0009, -0.0018, -0.0027, -0.0009, -0.0018, -0.0027],\n",
       "         [ 0.0017,  0.0033,  0.0050,  0.0017,  0.0033,  0.0050],\n",
       "         [ 0.0003,  0.0005,  0.0008,  0.0001,  0.0003,  0.0004],\n",
       "         [ 0.0020,  0.0039,  0.0059,  0.0020,  0.0039,  0.0059]],\n",
       "        grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out + gat2_bias\n",
    "out.size(), out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8200c22-c120-4f8b-9609-b0c4b64f7bff",
   "metadata": {},
   "source": [
    "### GAT Block - Residual Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f949b931-bd31-461c-9997-15b5c0adf26b",
   "metadata": {},
   "source": [
    "In GATs, residual connections create a clean path for each node’s own features and gradients to combat dead/vanishing gradients that can come from the attention layers and makes deeper message-passing stacks trainable. The residual connection also counters oversmoothing by re-injecting the identity at every layer so the node embeddings are less likely to collapse toward a common subspace, especially under strong degree normalization. Functionally this is represented as\n",
    "\n",
    "$y = f(x) + x$\n",
    "\n",
    "To achieve this we simply sum the projection matrix `x` with the convolution layer outputs `out`.  As a reminder our input embedding was initiated using a sliding scale per channel and node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "514df85f-f04f-4cc7-83a6-58c21eb2fc77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 6]),\n",
       " tensor([[ 0.0023,  0.0047,  0.0070,  0.0053,  0.0077,  0.0100],\n",
       "         [ 0.0064,  0.0117,  0.0171,  0.0094,  0.0147,  0.0201],\n",
       "         [ 0.0042,  0.0064,  0.0085,  0.0069,  0.0088,  0.0108],\n",
       "         [ 0.0039,  0.0048,  0.0056,  0.0069,  0.0078,  0.0086],\n",
       "         [ 0.0076,  0.0112,  0.0148,  0.0116,  0.0161,  0.0207],\n",
       "         [ 0.0086,  0.0122,  0.0158,  0.0116,  0.0152,  0.0188],\n",
       "         [ 0.0026,  0.0052,  0.0078,  0.0052,  0.0073,  0.0095],\n",
       "         [ 0.0004, -0.0002, -0.0008,  0.0046,  0.0052,  0.0058],\n",
       "         [ 0.0021,  0.0022,  0.0023,  0.0051,  0.0052,  0.0053],\n",
       "         [ 0.0057,  0.0083,  0.0110,  0.0087,  0.0113,  0.0140],\n",
       "         [ 0.0053,  0.0065,  0.0078,  0.0081,  0.0093,  0.0104],\n",
       "         [ 0.0080,  0.0109,  0.0139,  0.0110,  0.0139,  0.0169]],\n",
       "        grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out + x\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967de571-bdb2-4fc4-97ae-47e70e136830",
   "metadata": {},
   "source": [
    "### GAT Block - Post Residual ELU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dbef23-7e91-4052-83e5-d884f10559c3",
   "metadata": {},
   "source": [
    "Now we'll run another round of Exponential Linear Unit (ELU) based non-linearity. As a reminder, ELU provides a smooth, non-saturating nonlinearity that preserves negative information which is useful in attention-based and signed-graph settings. \n",
    "\n",
    "Since our values got pushed positive in the residual connection we shouldn't expect changes in values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b4fbd42a-7ad4-4c0a-9236-e2d1f4396839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 6]),\n",
       " tensor([[ 0.0023,  0.0047,  0.0070,  0.0053,  0.0077,  0.0100],\n",
       "         [ 0.0064,  0.0117,  0.0171,  0.0094,  0.0147,  0.0201],\n",
       "         [ 0.0042,  0.0064,  0.0085,  0.0069,  0.0088,  0.0108],\n",
       "         [ 0.0039,  0.0048,  0.0056,  0.0069,  0.0078,  0.0086],\n",
       "         [ 0.0076,  0.0112,  0.0148,  0.0116,  0.0161,  0.0207],\n",
       "         [ 0.0086,  0.0122,  0.0158,  0.0116,  0.0152,  0.0188],\n",
       "         [ 0.0026,  0.0052,  0.0078,  0.0052,  0.0073,  0.0095],\n",
       "         [ 0.0004, -0.0002, -0.0008,  0.0046,  0.0052,  0.0058],\n",
       "         [ 0.0021,  0.0022,  0.0023,  0.0051,  0.0052,  0.0053],\n",
       "         [ 0.0057,  0.0083,  0.0110,  0.0087,  0.0113,  0.0140],\n",
       "         [ 0.0053,  0.0065,  0.0078,  0.0081,  0.0093,  0.0104],\n",
       "         [ 0.0080,  0.0109,  0.0139,  0.0110,  0.0139,  0.0169]],\n",
       "        grad_fn=<EluBackward0>))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elu2 = nn.ELU()\n",
    "out = elu2(out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e63a426-0fd7-4962-b289-3b30a51c6558",
   "metadata": {},
   "source": [
    "### GAT Block - Reinsert The Batch Dimension\n",
    "\n",
    "Finally now that we have our graph attention complete, we'll reinsert our batch dimension.  This will not change values, simply split the incoming tensor down into each batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eba7e631-e020-4637-80c0-a2669463668d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 6]),\n",
       " tensor([[[ 0.0023,  0.0047,  0.0070,  0.0053,  0.0077,  0.0100],\n",
       "          [ 0.0064,  0.0117,  0.0171,  0.0094,  0.0147,  0.0201],\n",
       "          [ 0.0042,  0.0064,  0.0085,  0.0069,  0.0088,  0.0108],\n",
       "          [ 0.0039,  0.0048,  0.0056,  0.0069,  0.0078,  0.0086],\n",
       "          [ 0.0076,  0.0112,  0.0148,  0.0116,  0.0161,  0.0207],\n",
       "          [ 0.0086,  0.0122,  0.0158,  0.0116,  0.0152,  0.0188]],\n",
       " \n",
       "         [[ 0.0026,  0.0052,  0.0078,  0.0052,  0.0073,  0.0095],\n",
       "          [ 0.0004, -0.0002, -0.0008,  0.0046,  0.0052,  0.0058],\n",
       "          [ 0.0021,  0.0022,  0.0023,  0.0051,  0.0052,  0.0053],\n",
       "          [ 0.0057,  0.0083,  0.0110,  0.0087,  0.0113,  0.0140],\n",
       "          [ 0.0053,  0.0065,  0.0078,  0.0081,  0.0093,  0.0104],\n",
       "          [ 0.0080,  0.0109,  0.0139,  0.0110,  0.0139,  0.0169]]],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#insert batch back\n",
    "out = out.view(B_batch, N_nodes,n_embd)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c87789-b608-4646-9e79-bb1cd57f1879",
   "metadata": {},
   "source": [
    "### Output Layers AKA Model Heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb123f0d-ad1b-4fdb-8153-5a632324f14d",
   "metadata": {},
   "source": [
    "We've now shown a common pattern for a GAT block including a layer that handles our signed graph, a more typical layer, and the residual connection. Once those layers are complete during the forward pass we then start the output process that results in our node level and graph level logits which represent the probability of the different classes we're predicting.  \n",
    "\n",
    "<img src=\"explainer_screenshots/gat/output_layer.png\" width=\"300\">\n",
    "\n",
    "This layer is also known as the model **head**, or in this case \"heads\" and the weights learned are more task specific than the general model itself. What we'll uniquely do in this example is train two heads at once, \n",
    "\n",
    "Once a model is trained, there are processes to swap out the \"head\" and learn new tasks, e.g. go from next token prediction to classification.  In our example case, this is a linear layer mapping the backbone to vocab logits.\n",
    "\n",
    "Recall in the beginning that we discussed that we have 2 goals: predict the cell type and predict if a network is cancerous.  Because we have 2 goals we actually need to train 2 separate heads.  Given GATs are graphs at the core, there's 2 ways to look at the probabilities, per node level or for a graph.  Per node level creates logit predictions at each node while the graph uses pooling to create graph level logits.  In our case we'll actually create a head for each type.  Our cell type will predict at the node level and cancerous level will predict at the graph level. \n",
    "\n",
    "One thing you'll notice is that our head for GNNs includes bias, something we haven't done in other models. In a GNN, the bias in the heads acts as a learnable intercept that soaks up systematic offsets coming from graph degree normalized aggregation, variable graph sizes, and, in signed graphs, partial cancellation of positive/negative messages. In other models where you have sequence/vision heads, those heads often use weight tying or the head sits after a normalization layer that does similar levels of offset and normalization that bias does. GCN heads often follow pooling or raw node features without a final centering step so the bias therefore captures class priors and size/structure-dependent baselines, improving calibration when labels are sparse and graphs are small.\n",
    "\n",
    "We'll first start with creating our **Node Head** or our cell type logits through a linear projection on the graph convolution output.  From there we'll then create a parallel **Graph Head** using pooling, masking, and then finally a linear projection.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34732a51-d3a4-45d3-8c8a-2f0f10e746d8",
   "metadata": {},
   "source": [
    "#### Output Layer - Node Head\n",
    "\n",
    "Our node level head will predict if a node is more commonly associated with a \"T cell\" or \"B cell\".  We can think of this task as a classification task where the model has to pick for each sample the association.  Since this is classification, we need 2 dimensions for the cell types, `0` for T cell, and `1` for B cell.  This node level head will be shared across all examples and be  the `[classes,n_embed]` dimensionality.  We'll initialize with an incremental stepping of `0.005` similar to how we initialized the embedding layer. This will create a degree of bias to the second class initially but training should help adjust these weights. \n",
    "\n",
    "Also remember that since this is a GNN we also need to initialize bias in our head, in this case we'll start with our epsilon value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "09f0fad2-469e-4061-a419-e30acae59754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[0.0050, 0.0100, 0.0150, 0.0200, 0.0250, 0.0300],\n",
       "         [0.0100, 0.0150, 0.0200, 0.0250, 0.0300, 0.0350]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1.0000e-06, 1.0000e-06], requires_grad=True))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## nodes logits\n",
    "node_head = nn.Linear(n_embd, 2, bias=True)   # cell-type logits per node\n",
    "with torch.no_grad(): # initilize to W[i,j] = 0.001*(1+i+j) for easy following \n",
    "    vs, d = 2, n_embd\n",
    "    rows = torch.arange(vs).unsqueeze(1)  # (vs,1)\n",
    "    cols = torch.arange(d).unsqueeze(0)  # (1,d)\n",
    "    pattern = 0.005*(1 + rows + cols)  # W[i,j] = 0.001*(1+i+j)\n",
    "    node_head.weight.copy_(pattern)\n",
    "nn.init.constant_(node_head.bias, 1e-6)\n",
    "node_head.weight, node_head.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db422e41-ff1e-444e-9556-f5a6fff57844",
   "metadata": {},
   "source": [
    "**Node Head - Output Projection**\n",
    "\n",
    "Now we're ready for our final projection. Since our head is initialized having the incremental values we're automatically skewing towards class `1` instead of `0`, meaning it might be shit. Luckily backpropagation has a way of updating this so that with enough data and time the probabilities change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7c97b3eb-b370-4475-a27b-08fc6125a34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 2]),\n",
       " tensor([[[0.0008, 0.0009],\n",
       "          [0.0016, 0.0020],\n",
       "          [0.0009, 0.0011],\n",
       "          [0.0007, 0.0009],\n",
       "          [0.0016, 0.0020],\n",
       "          [0.0016, 0.0020]],\n",
       " \n",
       "         [[0.0008, 0.0009],\n",
       "          [0.0004, 0.0005],\n",
       "          [0.0005, 0.0006],\n",
       "          [0.0012, 0.0015],\n",
       "          [0.0009, 0.0012],\n",
       "          [0.0014, 0.0018]]], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_logits = node_head(out)\n",
    "node_logits.size(), node_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0710fb10-e272-44c3-b662-146a3eb96684",
   "metadata": {},
   "source": [
    "#### Output Layer - Graph Head\n",
    "\n",
    "Our graph level head will predict if a network (connection of nodes) is more benign or cancerous.  We can think of this task also as a classification task where the model has to pick for each sample the association.  Since this is classification, we need 2 dimensions for the cell types, `0` for Benign, and `1` for Cancerous.  This node level head will be shared across all examples and be  the `[classes,n_embed]` dimensionality.\n",
    "\n",
    "Before we can run the graph level prediction though, we have to collapse our current graph convolution output. Currently the output `x` is `[2,6,6]` representing our batch, nodes, and embeddings.  Recall that our nodes are all not genes, there's actually two nodes that are cell types. To do our graph level predictions and only focus on the parts of the network focused on the gene edges, we'll use a mask to zero-out the cell dimensions.  That will then be applied to our graph convolution output, then we'll get the node level average, or sum of the embedding channels divided by the number of genes. This is computed by applying the following:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}=\\frac{1}{|G|}\\sum_{i\\in G}\\mathbf{h}_i\n",
    "$$\n",
    "\n",
    "This results in an output of embeddings for each example, or a `[b_batch,n_embd]` tensor.  We then do a final linear projection against our class dimension head to get a final `logits` output. Also remember that since this is a GNN we also need to initialize bias in our head, in this case we'll start with our epsilon value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca83cf7b-5c3e-4fb0-879f-f5847dc79ee7",
   "metadata": {},
   "source": [
    "**Graph Head - Creating The Mask**  \n",
    "\n",
    "We first create a simple mask that shows only our first 4 dimensions on our tensor pertaining to the gene nodes.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0e8de88e-4911-43aa-ac38-e9ba06bd912f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6]), tensor([[1., 1., 1., 1., 0., 0.]]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = gene_mask.float().unsqueeze(0)\n",
    "mask.size(), mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4647c84a-a7d6-4cfd-abbc-f24d76be9baf",
   "metadata": {},
   "source": [
    "**Graph Head - Masked Sum**  \n",
    "\n",
    "We now apply the mask to the graph convolution output.  You can see that this zeros out the tensor entries pertaining to the cell type dimension. After we have that zeroed out we then sum our node dimensions resulting in embeddings for each of our cell types. You can see that at this point we have differing values across all three embedding channels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3d2ac7bd-c49f-44bb-bed7-a105a0dd410a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 6]),\n",
       " tensor([[[ 0.0023,  0.0047,  0.0070,  0.0053,  0.0077,  0.0100],\n",
       "          [ 0.0064,  0.0117,  0.0171,  0.0094,  0.0147,  0.0201],\n",
       "          [ 0.0042,  0.0064,  0.0085,  0.0069,  0.0088,  0.0108],\n",
       "          [ 0.0039,  0.0048,  0.0056,  0.0069,  0.0078,  0.0086],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.0026,  0.0052,  0.0078,  0.0052,  0.0073,  0.0095],\n",
       "          [ 0.0004, -0.0002, -0.0008,  0.0046,  0.0052,  0.0058],\n",
       "          [ 0.0021,  0.0022,  0.0023,  0.0051,  0.0052,  0.0053],\n",
       "          [ 0.0057,  0.0083,  0.0110,  0.0087,  0.0113,  0.0140],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
       "        grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_graph_sum = (out * mask.unsqueeze(-1))\n",
    "out_graph_sum.size(), out_graph_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cdf3f0b2-6a4b-413e-8cbb-ef03baacb559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6]),\n",
       " tensor([[0.0168, 0.0275, 0.0383, 0.0285, 0.0390, 0.0495],\n",
       "         [0.0108, 0.0155, 0.0202, 0.0235, 0.0290, 0.0345]],\n",
       "        grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_graph_sum = out_graph_sum.sum(1)\n",
    "out_graph_sum.size(), out_graph_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de8e0f2-83c1-48c3-a03d-d9b0ac332a6f",
   "metadata": {},
   "source": [
    "**Graph Head - Average Pooling**  \n",
    "\n",
    "To finish our pooling we now have to divide our sums to get a node average per example. This simply requires dividing each row by the number of nodes.  Lucky for us all our entries have the same number so it's a simple division across all entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e7fbb9d8-7fbb-4bdb-a143-8c2d58a2dc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denom = mask.sum(1, keepdim=True).clamp_min(1.0) \n",
    "denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b762ab9e-5972-4c1e-8727-d51944ccbfe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6]),\n",
       " tensor([[0.0042, 0.0069, 0.0096, 0.0071, 0.0098, 0.0124],\n",
       "         [0.0027, 0.0039, 0.0051, 0.0059, 0.0073, 0.0086]],\n",
       "        grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled = out_graph_sum / denom   \n",
    "pooled.size(), pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c96444-9b2e-426a-a320-40b42cdf9468",
   "metadata": {},
   "source": [
    "**Graph Head - Output Projection**\n",
    "\n",
    "Now we're ready for our final projection. Since our head is initialized having the incremental values we're automatically skewing towards class `1` instead of `0`, meaning it might be shit. Luckily backpropagation has a way of updating this so that with enough data and time the probabilities change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5bf19414-0272-4890-830c-555c8d7a203a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[0.0050, 0.0100, 0.0150, 0.0200, 0.0250, 0.0300],\n",
       "         [0.0100, 0.0150, 0.0200, 0.0250, 0.0300, 0.0350]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1.0000e-06, 1.0000e-06], requires_grad=True))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_head = nn.Linear(n_embd, 2, bias=True)  # cancer logits per graph\n",
    "with torch.no_grad(): # initilize to W[i,j] = 0.001*(1+i+j) for easy following \n",
    "    vs, d = 2, n_embd\n",
    "    rows = torch.arange(vs).unsqueeze(1)  # (vs,1)\n",
    "    cols = torch.arange(d).unsqueeze(0)  # (1,d)\n",
    "    pattern = 0.005*(1 + rows + cols)  # W[i,j] = 0.001*(1+i+j)\n",
    "    graph_head.weight.copy_(pattern)\n",
    "nn.init.constant_(graph_head.bias, 1e-6) \n",
    "graph_head.weight, graph_head.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4acbdbe4-07f7-4376-ade3-af0c459dea08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2]),\n",
       " tensor([[0.0010, 0.0012],\n",
       "         [0.0007, 0.0009]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_logits = graph_head(pooled)  # [B,2]\n",
    "graph_logits.size(), graph_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b3a736-9cd3-425a-8c06-824a3e2089bb",
   "metadata": {},
   "source": [
    "## Loss Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09803619-0f07-4401-a4cc-cef29758519c",
   "metadata": {},
   "source": [
    "Now we have to see how good our ~shit~ prediction is.  Since we haven't done training and we saw that regardless of class, for an example we had the same exact logit values, so we can expect it's bad, basically random. That said, we need to know how bad. For this example we'll use cross entropy, also known as the negative log likelihood of the softmax.  Our loss calculates\n",
    "\n",
    "$$\n",
    "\\ell_i=-\\log\\big(\\mathrm{argmax}(z_i)\\_{y_i}\\big)\n",
    "= -z_{i,y_i}+\\log\\!\\sum_{c=1}^C e^{z_{i,c}},\n",
    "$$\n",
    "\n",
    "What's unique in this case is that we actually have 2 heads.  You might wonder how we'll do backprop, but it's simple, we'll just calculate the loss on each head.  For now we'll make it easy and assume that the both losses are equally important to us.  This is tunable though depending on what you value in a graph.  If you value one of the losses more, you can upweight it before summing and, during backprop, that will impact the gradient flow to prioritize that loss. \n",
    "\n",
    "$$\n",
    "\\ell_{total}= \\ell_{node}+\\alpha \\ell_{graph}\n",
    "$$\n",
    "\n",
    "We'll first calculate our node loss and then our graph loss and sum them together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a6f2f2ce-03d1-4e78-a49c-ef7f0b4995d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238b6b25-2be6-4853-8750-bfb0a5b8c78e",
   "metadata": {},
   "source": [
    "**Node Loss**\n",
    "\n",
    "For our node loss, we again need to employ a mask.  This mask helps with our loss to avoid including the cell type level nodes in our prediction as we primarily want to understand which node, or gene, predicts which cell type.  After we create the mask, we slice our node logits (nodes for each example) and label `y_node` for those and then calculate the loss. You'll see because our logits are equal, our loss is random. You'll also notice that the valid masking removes the batch dimension to simplify the loss calculation as it does not care about batches. Finally we also append our loss to `losses` so we can do the final summation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "05bbb946-d1e3-4075-a8db-976449e0e946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True, False, False],\n",
       "        [ True, False,  True,  True, False, False]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid = (gene_mask & (y_node>=0))\n",
    "valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "234bf14d-107c-422c-b2fe-5e9675e40b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0008, 0.0009],\n",
       "         [0.0016, 0.0020],\n",
       "         [0.0009, 0.0011],\n",
       "         [0.0007, 0.0009],\n",
       "         [0.0008, 0.0009],\n",
       "         [0.0005, 0.0006],\n",
       "         [0.0012, 0.0015]], grad_fn=<IndexBackward0>),\n",
       " tensor([0, 0, 0, 1, 1, 0, 1]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_node_logits = node_logits[valid]\n",
    "valid_y_node = y_node[valid]\n",
    "valid_node_logits, valid_y_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "11d951dc-52e2-48d5-ae87-6832b7e0f8c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6932, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nl = F.cross_entropy(valid_node_logits, valid_y_node)\n",
    "losses.append(nl)\n",
    "nl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6b4542-08cc-44fe-8aa4-22b6e087c15a",
   "metadata": {},
   "source": [
    "**Graph Loss**\n",
    "\n",
    "For our graph loss, we do not need a mask since we included it during pooling and we've already collapsed our dimensions down.  At this point we  just pass in the graph logits and our labels stored in `y_graph`.  We'll also append the loss to `losses`. Notice that our current logits are equal per example so we will expect our loss to be close to random.  \n",
    "\n",
    "We also introduce alpha here. Alpha will scale how much to weigh the graph loss in total loss. For now we'll keep it at `1.0` so that it's weighed the same amount as the node loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "881d8400-a313-4f6d-a539-6c207814bf29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0010, 0.0012],\n",
       "         [0.0007, 0.0009]], grad_fn=<AddmmBackward0>),\n",
       " tensor([0, 1]),\n",
       " 1.0)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 1.0\n",
    "graph_logits, y_graph, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "48e94eec-73dc-4225-9f7d-13eb346db493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6932, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gl = F.cross_entropy(graph_logits, y_graph)\n",
    "losses.append(gl)\n",
    "gl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7798365a-92eb-4aaf-85cd-9ec11f8c73d5",
   "metadata": {},
   "source": [
    "**Sum Loss**\n",
    "\n",
    "Now that we have each loss we finally have to complete the sum:\n",
    "\n",
    "$$\n",
    "\\ell_{total}= \\ell_{node}+\\alpha \\ell_{graph}\n",
    "$$\n",
    "\n",
    "For this example we'll weigh each loss equally so we can ignore the alpha. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2e3531e3-7026-4b97-9acc-a86f1b394c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3863, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = sum(losses) \n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60396340-0006-4024-9d5b-3338dbaf1d68",
   "metadata": {},
   "source": [
    "## Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8340cb32-33de-42e2-8035-349795e87267",
   "metadata": {},
   "source": [
    "We now know just how ~well~ terribly the current model, with its weights and biases, predicts the next token given the input context. We now need to know how to change the different weights and biases to improve the formula.  We could do this by guessing through making minor changes and seeing what improves, or we can think through this more critically.\n",
    "\n",
    "If you review the chain of layers above, you can see that it's a series of formulas.  We can think of this as $f(g(x))$, except with many many more layers and complexities.  Since this is a formula, we can dig into our math toolbox and find a better way to determine what parts need to update.  Recall that in our calculus we learned that differentiation tells us the rate of change in a graph.  So if we treat the loss function $\\mathcal{L}$ as $\\mathcal{L}(f(g(x)))$ taking the partial differential \n",
    "\n",
    "$\\delta=\\partial \\mathcal{L}/\\partial h$\n",
    "\n",
    "at each layer will give us the impact of each weight/bias on our final out (albeit the inverse since our loss function is the negative log likelihood). \n",
    "\n",
    "Lucky for us, each layer of our model already has a placeholder for the partial differential called the **Gradient**. We'll use this field to store it.  We'll start by first zeroing out the gradients. We do this because of the nature of handling partial differentials for multiple dependencies. Recall that in multiple places we had a formula structure of \n",
    "\n",
    "$a+b=c ; a+c= d$\n",
    "\n",
    "In this case $a$ has 2 dependencies and determining the partial derivative of $\\partial d / \\partial a$ requires understanding both the path from $d$ and $c$.  To determine the true impact of a we would sum both partial derivatives together.  Because of this property, the tool we use, the built in `.backwards()` automatically sums gradients, `+=`, so if we do not set the gradient to `0` we then end up with erroneous gradients. \n",
    "\n",
    "Finally, we start `.backwards` from the `loss`, not `logits` as our goal is to minimize loss, we need to ensure we are looking at the calculations that impact loss which requires the whole forward pass to be able to generate the prediction `logits_flat`.  If we think of it as $\\mathcal{L}(f(x))$ where $f(x)$ is the forward pass to generate logits, then a simple chain rule is applied:\n",
    "\n",
    "${\\partial}/{\\partial x} =  \\mathcal{L}'(f(x)) f'(x)$\n",
    "\n",
    "Lets start by zeroing the gradients and leaning on pytorch to calculate the gradients for us. We'll also validate the gradients were `none`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb97194-98d0-444c-a7e9-18eaf7a742cb",
   "metadata": {},
   "source": [
    "### Back Propagation - Zero out gradients\n",
    "\n",
    "Before we start we have to zero our gradients to make sure there's nothing in them.  Recall that when we run  `.backwards()` it automatically sums gradients if there are multiple paths so zeroing out ensures no erroneous measures.  \n",
    "\n",
    "Two things to notice: \n",
    "1. Our pooling step is not included.  While pooling does pass through gradients, it is stateless, meaning there are no learnable parameters, so the layer does not have gradient buffers to clear. The step itself is differentiable though so gradients flow through it back to upstream parameters.\n",
    "2. Our convolutional layers use a special reset.  Since we manually built the layers using `nn.Parameter` they do not have the typical gradient and weight functions that layers have, but the layers are learnable.  Because of this they do have gradient buffers but no magic function to clear them so we have to clear them manually.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a39f9da4-0ad8-41ae-a20b-7a9c2b854979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output layers\n",
    "graph_head.zero_grad()\n",
    "node_head.zero_grad()\n",
    "\n",
    "#gat block\n",
    "gat2_bias.grad = None # slightly different given it's manually built and not a torch layer. \n",
    "gat2_attn_dst.grad = None\n",
    "gat2_attn_src.grad = None\n",
    "gat2_attn_w.grad = None\n",
    "gat1_bias.grad = None # slightly different given it's manually built and not a torch layer. \n",
    "gat1_attn_sign.grad = None\n",
    "gat1_attn_dst.grad = None\n",
    "gat1_attn_src.grad = None\n",
    "gat1_attn_w.grad = None\n",
    "\n",
    "#input layer\n",
    "tok_emb.zero_grad()\n",
    "\n",
    "# validate gradients\n",
    "graph_head.weight.grad,node_head.weight.grad,tok_emb.weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012ab08d-9791-461f-a183-72eb755582a1",
   "metadata": {},
   "source": [
    "### Back Propagation - Auto Diff\n",
    "\n",
    "Now let's see the magic of the gradients populate.  This magic is called auto-differentiation, or auto-diff for short. This allows us to not have to write many layers of nasty code to do the differentiation for us, but, if you're a sadist, you can surely find people who have written out that code (it's not too bad since you just do one layer at a time). \n",
    "\n",
    "Recall that our loss is actually the sum of the loss across multiple heads. The beauty here is that because we used a sum, the gradient will be distributed across both and join as we reach each layer, helping ensure that the graph is optimizing both heads at the same time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "aa163c79-eabe-4e2a-b251-b552ff0ccdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2d8f6e-1f48-4bf0-8a34-48bbffcbfac2",
   "metadata": {},
   "source": [
    "Now we will revisualize our gradients and see that they contain  unique values, just like we wanted. If you wanted, you could deeply evaluate each graident and layer to better understand specifically what is causing errors for a specific example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "feb423b4-efdb-42cc-a0f6-cc189e23a9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0004, -0.0008, -0.0011, -0.0003, -0.0006, -0.0009],\n",
       "         [ 0.0004,  0.0008,  0.0011,  0.0003,  0.0006,  0.0009]]),\n",
       " tensor([[-0.0002, -0.0005, -0.0008, -0.0004, -0.0007, -0.0010],\n",
       "         [ 0.0002,  0.0005,  0.0008,  0.0004,  0.0007,  0.0010]]),\n",
       " tensor([[ 0.0002,  0.0002,  0.0002,  0.0002,  0.0002,  0.0002],\n",
       "         [ 0.0004,  0.0007,  0.0007,  0.0008,  0.0007,  0.0007],\n",
       "         [ 0.0015,  0.0021,  0.0021,  0.0021,  0.0021,  0.0021],\n",
       "         [-0.0020, -0.0020, -0.0020, -0.0014, -0.0020, -0.0020],\n",
       "         [ 0.0011,  0.0011,  0.0011,  0.0006,  0.0011,  0.0011],\n",
       "         [-0.0005, -0.0005, -0.0005, -0.0004, -0.0005, -0.0001]]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_head.weight.grad,node_head.weight.grad,tok_emb.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d8aab446-5d59-4c83-8763-3a35abfb9634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-7.2151e-07,  1.6400e-06, -7.2894e-07],\n",
       "          [ 2.4691e-06,  3.9886e-06,  2.4422e-06],\n",
       "          [ 3.3706e-06,  5.1865e-06,  3.3370e-06],\n",
       "          [ 6.0363e-06,  9.2883e-06,  6.0713e-06],\n",
       "          [ 5.1735e-06,  7.5822e-06,  5.1265e-06],\n",
       "          [ 1.0121e-05,  1.1304e-05,  9.9941e-06]],\n",
       " \n",
       "         [[-7.3632e-07, -2.6831e-07, -1.4194e-06],\n",
       "          [ 2.4405e-06,  2.9886e-06,  1.6989e-06],\n",
       "          [ 3.3413e-06,  3.9696e-06,  2.4810e-06],\n",
       "          [ 6.2068e-06,  7.8140e-06,  5.0109e-06],\n",
       "          [ 5.1429e-06,  5.9315e-06,  4.0452e-06],\n",
       "          [ 9.8943e-06,  9.3300e-06,  9.2797e-06]]]),\n",
       " tensor([[[1.4737e-08],\n",
       "          [2.9474e-08],\n",
       "          [4.4212e-08]],\n",
       " \n",
       "         [[1.0550e-08],\n",
       "          [2.1101e-08],\n",
       "          [3.1651e-08]]]),\n",
       " tensor([[[1.2265e-08],\n",
       "          [2.4530e-08],\n",
       "          [3.6795e-08]],\n",
       " \n",
       "         [[5.2377e-09],\n",
       "          [1.0475e-08],\n",
       "          [1.5713e-08]]]),\n",
       " tensor([[[9.4320e-06, 6.5421e-06]]]),\n",
       " tensor([0.0012, 0.0024, 0.0012, 0.0012, 0.0016, 0.0012]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gat1_attn_w.grad,gat1_attn_src.grad,gat1_attn_dst.grad,gat1_attn_sign.grad, gat1_bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2478d1a8-46f7-43eb-801b-e6ef22467bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1.1839e-06, 1.1839e-06, 1.1837e-06],\n",
       "          [3.2791e-06, 3.2790e-06, 3.2788e-06],\n",
       "          [3.5254e-06, 3.5252e-06, 3.5247e-06],\n",
       "          [1.1779e-06, 1.1779e-06, 1.1777e-06],\n",
       "          [2.6450e-06, 2.6449e-06, 2.6446e-06],\n",
       "          [2.6816e-06, 2.6815e-06, 2.6813e-06]],\n",
       " \n",
       "         [[1.0234e-06, 1.0234e-06, 1.0234e-06],\n",
       "          [3.2261e-06, 3.2261e-06, 3.2261e-06],\n",
       "          [3.0445e-06, 3.0445e-06, 3.0445e-06],\n",
       "          [1.0134e-06, 1.0134e-06, 1.0134e-06],\n",
       "          [2.3165e-06, 2.3165e-06, 2.3165e-06],\n",
       "          [2.5898e-06, 2.5898e-06, 2.5898e-06]]]),\n",
       " tensor([[[-4.1016e-15],\n",
       "          [-8.2031e-15],\n",
       "          [-1.2305e-14]],\n",
       " \n",
       "         [[-1.7678e-09],\n",
       "          [-3.5355e-09],\n",
       "          [-5.3033e-09]]]),\n",
       " tensor([[[4.0543e-08],\n",
       "          [8.1087e-08],\n",
       "          [1.2163e-07]],\n",
       " \n",
       "         [[3.6581e-08],\n",
       "          [7.3162e-08],\n",
       "          [1.0974e-07]]]),\n",
       " tensor([0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004]))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gat2_attn_w.grad,gat2_attn_src.grad,gat2_attn_dst.grad, gat2_bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0656835-47ca-425e-96e7-bf07297c45f9",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30915151-ddcb-4d69-86f9-accc4ea07865",
   "metadata": {},
   "source": [
    "The process of learning now requires us to update our weights based on this gradient. To really feel the \"back propagation\" we'll start with the last layer and work backwards, though, since we have all of the gradients calculated already, the order does not matter. Recall that our loss function is the negative log likelihood ratio so our gradient signs are flipped.  If a parameter is important, the gradient will be more negative, and vice versa. The gradients are a ratio of importance of each parameter and we need to know how much of that gradient to apply to our weights. This \"how much\" is referred to as the *learning rate*. In modern training learning rate schedulers and optimizers are used to vary the rate and application by layer and by training round with learning rates that are small (e.g. 1e-3) and decaying. \n",
    "\n",
    "We however are trying to learn and if you look at the gradient above in most layers it's tiny (~1e-4 or smaller).  If we used a typical learning rate scheduler, with our batch size, and just 1 pass, the second pass would just have the same values and we wouldn't learn anything new.  Because of this we'll use very high learning rates that are also different by layer so that we amplify the learning from a single pass and can see the weights change. As a warning, DO NOT DO THIS IN REAL TRAINING. If you did your model would most likely not converge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e4248322-d8f4-4c7f-9d26-69c88bacdbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Huge learning rate to emphasize\n",
    "lr = 1e2\n",
    "value_lr = 1e5\n",
    "qk_lr =  1e7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2af494e-d35d-452b-aaad-953e1a886392",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "\n",
    "Let's start with our output layer.  Recall that we initialized the node_head and graph_head weights to increments of  `0.005` so we can quickly see the impact of the gradient update on the weights. This layer has some of the largest gradients so using the high learning rate radically shifts the values.  You can see also that the model is adjusting to downweight the second class and has adjusted the channels so that they are no longer increasing across the channels.  If you look closely though you can see that if you split the channel into 2 separate buckets, mirroring the heads, there is an increase from channel 0 to 2 inside the head maintaining that incremental increase that we saw in the weight initiation. \n",
    "\n",
    "On the bias though we're seeing interesting symmetry where the values are the same with opposite directionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2bc5015c-6586-4206-a4e9-d931335e1b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[ 0.0426,  0.0853,  0.1279,  0.0513,  0.0875,  0.1238],\n",
       "         [-0.0276, -0.0603, -0.0929, -0.0063, -0.0325, -0.0588]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 5.3080e-05, -5.1065e-05], requires_grad=True))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    graph_head.weight -= lr * graph_head.weight.grad\n",
    "    graph_head.bias -= graph_head.bias.grad\n",
    "graph_head.weight, graph_head.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c3630314-36ff-461c-982f-e9d5efe18f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[ 0.0253,  0.0577,  0.0901,  0.0629,  0.0965,  0.1302],\n",
       "         [-0.0103, -0.0327, -0.0551, -0.0179, -0.0415, -0.0652]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0715, -0.0715], requires_grad=True))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    node_head.weight -= lr * node_head.weight.grad\n",
    "    node_head.bias -= node_head.bias.grad\n",
    "node_head.weight, node_head.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c453ffb-b1e0-4037-8487-84a03bcd1fb0",
   "metadata": {},
   "source": [
    "### GAT Unit - Main Pass\n",
    "\n",
    "Now we'll update our graph attention block. Our bias was uniformly initiated to `1e-6`. You'll see that the bias for the second layer is uniformly initiated while the bias in the first increases across the channels. This shows some baseline learning. \n",
    "\n",
    "For our edge attention, we initiated them consistently across the heads. You'll see that during learning they mostly adjust fairly significantly away from their initiation with the heads shifting also.  We can see heavily negative values in the first GAT layer for the edge attention.  This shows the model learning some of the graph due to the very high learning rates.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a1d740a9-b6a7-474a-83a8-2c150e0f3e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0004, -0.0004, -0.0004, -0.0004, -0.0004, -0.0004],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    gat2_bias -= gat2_bias.grad\n",
    "gat2_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3585bdb3-4553-43c2-8ea8-7c490ebb36cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[[-0.3054],\n",
       "          [-0.7109],\n",
       "          [-1.1163]],\n",
       " \n",
       "         [[-0.1658],\n",
       "          [-0.5316],\n",
       "          [-0.8974]]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[0.3000],\n",
       "          [0.3000],\n",
       "          [0.3000]],\n",
       " \n",
       "         [[0.1177],\n",
       "          [0.1354],\n",
       "          [0.1530]]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[ 0.0816,  0.2816,  0.4816],\n",
       "          [-0.1279,  0.0721,  0.2721],\n",
       "          [-0.1525,  0.0475,  0.2475],\n",
       "          [ 0.0822,  0.2822,  0.4822],\n",
       "          [-0.0645,  0.1355,  0.3355],\n",
       "          [-0.0682,  0.1319,  0.3319]],\n",
       " \n",
       "         [[ 0.0977,  0.2977,  0.4977],\n",
       "          [-0.1226,  0.0774,  0.2774],\n",
       "          [-0.1045,  0.0955,  0.2955],\n",
       "          [ 0.0987,  0.2987,  0.4987],\n",
       "          [-0.0317,  0.1683,  0.3683],\n",
       "          [-0.0590,  0.1410,  0.3410]]], requires_grad=True))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    gat2_attn_dst -= qk_lr * gat2_attn_dst.grad\n",
    "    gat2_attn_src -= qk_lr * gat2_attn_src.grad\n",
    "    gat2_attn_w -= value_lr * gat2_attn_w.grad\n",
    "gat2_attn_dst, gat2_attn_src, gat2_attn_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "af242aba-5c7d-43ee-8215-1746a48f207e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0012, -0.0024, -0.0012, -0.0012, -0.0016, -0.0012],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    gat1_bias -= gat1_bias.grad\n",
    "gat1_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cd30bb4b-830f-4d8c-b591-70e1df49127d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[[-0.7432, -0.5542]]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[-0.4226],\n",
       "          [-0.5453],\n",
       "          [-0.6679]],\n",
       " \n",
       "         [[-0.2524],\n",
       "          [-0.3048],\n",
       "          [-0.3571]]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[-0.0474],\n",
       "          [-0.1947],\n",
       "          [-0.3421]],\n",
       " \n",
       "         [[ 0.0945],\n",
       "          [-0.0110],\n",
       "          [-0.1165]]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[ 0.0816,  0.2816,  0.4816],\n",
       "          [-0.1279,  0.0721,  0.2721],\n",
       "          [-0.1525,  0.0475,  0.2475],\n",
       "          [ 0.0822,  0.2822,  0.4822],\n",
       "          [-0.0645,  0.1355,  0.3355],\n",
       "          [-0.0682,  0.1319,  0.3319]],\n",
       " \n",
       "         [[ 0.0977,  0.2977,  0.4977],\n",
       "          [-0.1226,  0.0774,  0.2774],\n",
       "          [-0.1045,  0.0955,  0.2955],\n",
       "          [ 0.0987,  0.2987,  0.4987],\n",
       "          [-0.0317,  0.1683,  0.3683],\n",
       "          [-0.0590,  0.1410,  0.3410]]], requires_grad=True))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    gat1_attn_sign -= value_lr * gat1_attn_sign.grad\n",
    "    gat1_attn_dst -= qk_lr * gat1_attn_dst.grad\n",
    "    gat1_attn_src -= qk_lr * gat1_attn_src.grad\n",
    "    gat1_attn_w -= value_lr * gat1_attn_w.grad\n",
    "gat1_attn_sign, gat1_attn_dst, gat1_attn_src, gat2_attn_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c0566-b240-4e07-9849-4162e6a3a4e8",
   "metadata": {},
   "source": [
    "### Input Layer\n",
    "\n",
    "Finally we'll update our input layer.  Recall that this was incremented with a sliding increment as well and has no bias.  Here we can again see that the layer seems to be learning with a heavy push negative across the board.  Also the channel weights have been pulled closer together removing the incremental sliding values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ebb27c2d-071a-4e5f-a9aa-5345b12579f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0143, -0.0133, -0.0123, -0.0113, -0.0103, -0.0093],\n",
       "        [-0.0337, -0.0681, -0.0671, -0.0741, -0.0651, -0.0641],\n",
       "        [-0.1431, -0.2027, -0.2017, -0.2007, -0.1997, -0.1987],\n",
       "        [ 0.2070,  0.2080,  0.2090,  0.1431,  0.2110,  0.2120],\n",
       "        [-0.1050, -0.1040, -0.1030, -0.0560, -0.1010, -0.1000],\n",
       "        [ 0.0584,  0.0594,  0.0604,  0.0481,  0.0624,  0.0242]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    tok_emb.weight -= lr * tok_emb.weight.grad\n",
    "tok_emb.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b348bae-1a6e-40c6-a7dc-99d690948688",
   "metadata": {},
   "source": [
    "## Forward Pass With Updated Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bfca7b-09a0-4bc4-ac8a-f7ad131d74a2",
   "metadata": {},
   "source": [
    "Now that we have the updated weights for each layer, let's do another forward pass and compare the loss. Since each layer was previously explained we will instead focus on just showing the outputs of the different layers and the final loss. If you want, you can check the previous outputs in the cached cell outputs above and compare them to see how the weight changes impacted the values at each layer. \n",
    "\n",
    "You'll notice that because of our high learning rate we're able to see how each layer now shifts the embedding values as the input passes through them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51085ada-c3d2-41e0-9e9b-a4fb540952a4",
   "metadata": {},
   "source": [
    "### Data Re-loading\n",
    "\n",
    "Since we didn't reuse any input variables, we'll just recall them and use them again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "be149e9c-dc80-47e5-8bf9-78c64601ac0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2, 3, 4, 5],\n",
       "         [0, 1, 2, 3, 4, 5]]),\n",
       " tensor([[ 0,  0,  0,  1, -1, -1],\n",
       "         [ 1, -1,  0,  1, -1, -1]]),\n",
       " tensor([0, 1]),\n",
       " tensor([ True,  True,  True,  True, False, False]))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tokens, y_node, y_graph, gene_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2c2639f6-45be-47f9-8168-8795f2783dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  1.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [-1.,  1., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1., -1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0., -1., -1.,  1., -1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  1., -1., -1.,  1.,  0.,  0.]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_blk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af26df0e-31c7-4d58-aabe-65edd247b2ec",
   "metadata": {},
   "source": [
    "### Input Layer\n",
    "\n",
    "Note that in `tok_embedding` we ended up with a large shift away from our initial values. Because of this we'll see a shift in our embedding layer though it will still reflect the weights since our input is the iteration of nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "361d1b02-09df-4164-8a08-1dfd32323832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 6]),\n",
       " tensor([[-0.0143, -0.0133, -0.0123, -0.0113, -0.0103, -0.0093],\n",
       "         [-0.0337, -0.0681, -0.0671, -0.0741, -0.0651, -0.0641],\n",
       "         [-0.1431, -0.2027, -0.2017, -0.2007, -0.1997, -0.1987],\n",
       "         [ 0.2070,  0.2080,  0.2090,  0.1431,  0.2110,  0.2120],\n",
       "         [-0.1050, -0.1040, -0.1030, -0.0560, -0.1010, -0.1000],\n",
       "         [ 0.0584,  0.0594,  0.0604,  0.0481,  0.0624,  0.0242],\n",
       "         [-0.0143, -0.0133, -0.0123, -0.0113, -0.0103, -0.0093],\n",
       "         [-0.0337, -0.0681, -0.0671, -0.0741, -0.0651, -0.0641],\n",
       "         [-0.1431, -0.2027, -0.2017, -0.2007, -0.1997, -0.1987],\n",
       "         [ 0.2070,  0.2080,  0.2090,  0.1431,  0.2110,  0.2120],\n",
       "         [-0.1050, -0.1040, -0.1030, -0.0560, -0.1010, -0.1000],\n",
       "         [ 0.0584,  0.0594,  0.0604,  0.0481,  0.0624,  0.0242]],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tok_emb(x_tokens)\n",
    "x = x.view(B_batch*N_nodes,n_embd) # remove batch\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea21ec5-f665-45ed-a0d4-8f6b48c6c171",
   "metadata": {},
   "source": [
    "### GAT Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b844a71f-1db5-436b-8c0d-9ea5841dec39",
   "metadata": {},
   "source": [
    "#### GAT Block - Signed First Attention Layer\n",
    "\n",
    "Now we'll dive into our attention layers.  Recall that this layer had significant changes away from the scaled initiation by head including the introduction of positive and negative weights by node and by channel.  We'll discuss the impact as we go through. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057f6981-ae3d-4b12-8bb1-aa8b5e4431f7",
   "metadata": {},
   "source": [
    "##### Feature Dropout\n",
    "\n",
    "Dorpout is not learnable so we see the same impact we've seen before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f5938cbe-9786-486b-9d9b-4eaaff5c3c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 6]),\n",
       " tensor([[-0.0000, -0.0148, -0.0137, -0.0126, -0.0114, -0.0103],\n",
       "         [-0.0375, -0.0757, -0.0746, -0.0824, -0.0723, -0.0712],\n",
       "         [-0.1590, -0.2252, -0.2241, -0.0000, -0.2219, -0.2208],\n",
       "         [ 0.2300,  0.2311,  0.2322,  0.1590,  0.2345,  0.2356],\n",
       "         [-0.1167, -0.1155, -0.1144, -0.0623, -0.1122, -0.1111],\n",
       "         [ 0.0649,  0.0660,  0.0671,  0.0535,  0.0693,  0.0269],\n",
       "         [-0.0159, -0.0148, -0.0137, -0.0126, -0.0114, -0.0103],\n",
       "         [-0.0375, -0.0757, -0.0000, -0.0824, -0.0723, -0.0712],\n",
       "         [-0.1590, -0.2252, -0.2241, -0.0000, -0.0000, -0.2208],\n",
       "         [ 0.2300,  0.2311,  0.2322,  0.1590,  0.2345,  0.2356],\n",
       "         [-0.1167, -0.1155, -0.0000, -0.0623, -0.1122, -0.1111],\n",
       "         [ 0.0649,  0.0660,  0.0671,  0.0535,  0.0693,  0.0269]],\n",
       "        grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_drop = gat1_fdrop(x)\n",
    "x_drop.size(), x_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0c5a19-d149-4734-8aec-22691c20d957",
   "metadata": {},
   "source": [
    "##### Per-Head Linear Projection\n",
    "\n",
    "The node level linear weights had a mix of positives and negatives along with different scalers by head. Because of this we now see differing values across the heads and channels and some nodes positive weighted vs others negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "924fc34a-0b61-41c7-8e25-073a02408764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 12, 3]),\n",
       " tensor([[[ 2.5923e-02,  3.2432e-02,  1.3147e-02],\n",
       "          [ 1.5900e-01,  2.0416e-01,  7.4820e-02],\n",
       "          [ 3.5281e-01,  4.3974e-01,  1.3729e-01],\n",
       "          [-4.4221e-01, -5.7763e-01, -1.7261e-01],\n",
       "          [ 2.0353e-01,  2.6662e-01,  7.4596e-02],\n",
       "          [-9.4877e-02, -1.3492e-01, -2.4402e-02],\n",
       "          [ 2.3189e-02,  3.1861e-02,  7.2239e-03],\n",
       "          [ 1.4131e-01,  1.8039e-01,  7.2307e-02],\n",
       "          [ 2.6020e-01,  3.1588e-01,  9.0112e-02],\n",
       "          [-4.4221e-01, -5.7763e-01, -1.7261e-01],\n",
       "          [ 1.7641e-01,  2.3015e-01,  7.0740e-02],\n",
       "          [-9.4877e-02, -1.3492e-01, -2.4402e-02]],\n",
       " \n",
       "         [[ 2.5786e-02,  2.3515e-02,  7.5738e-03],\n",
       "          [ 1.5808e-01,  1.4222e-01,  3.8576e-02],\n",
       "          [ 3.4559e-01,  2.7940e-01,  5.0628e-02],\n",
       "          [-4.3718e-01, -3.7371e-01, -6.0630e-02],\n",
       "          [ 2.0090e-01,  1.6924e-01,  2.1487e-02],\n",
       "          [-9.4486e-02, -8.3126e-02,  5.8191e-03],\n",
       "          [ 2.3028e-02,  1.9913e-02,  5.5430e-04],\n",
       "          [ 1.4061e-01,  1.2753e-01,  4.2448e-02],\n",
       "          [ 2.5367e-01,  1.9217e-01,  2.7437e-02],\n",
       "          [-4.3718e-01, -3.7371e-01, -6.0630e-02],\n",
       "          [ 1.7410e-01,  1.4671e-01,  2.7426e-02],\n",
       "          [-9.4486e-02, -8.3126e-02,  5.8191e-03]]], grad_fn=<CloneBackward0>))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hproj = x_drop @ gat1_attn_w\n",
    "Hproj.size(), Hproj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778e0292-700e-4f7c-bbd8-e175810261d1",
   "metadata": {},
   "source": [
    "#####  Linear Node Attention\n",
    "\n",
    "For the node attention you'll similarly see that the predictable scaling across heads is gone along with the sign.  Previously we'd initiated the destination with negative weights and now we can see that they're a mix of different signs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb6c41e-4292-4183-9c0a-6672cba71a3f",
   "metadata": {},
   "source": [
    "**Source Node (Query)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d67896f1-e813-4480-9be5-cc1a75d5a997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 1, 2]),\n",
       " tensor([[[-0.0120,  0.0013]],\n",
       " \n",
       "         [[-0.0729,  0.0089]],\n",
       " \n",
       "         [[-0.1493,  0.0237]],\n",
       " \n",
       "         [[ 0.1925, -0.0301]],\n",
       " \n",
       "         [[-0.0871,  0.0146]],\n",
       " \n",
       "         [[ 0.0391, -0.0087]],\n",
       " \n",
       "         [[-0.0098,  0.0019]],\n",
       " \n",
       "         [[-0.0666,  0.0069]],\n",
       " \n",
       "         [[-0.1047,  0.0187]],\n",
       " \n",
       "         [[ 0.1925, -0.0301]],\n",
       " \n",
       "         [[-0.0774,  0.0116]],\n",
       " \n",
       "         [[ 0.0391, -0.0087]]], grad_fn=<PermuteBackward0>))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_src = Hproj @ gat1_attn_src\n",
    "e_src = e_src.permute(1, 2, 0)\n",
    "e_src.size(), e_src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37e682c-f262-4727-8911-fb0dec288a79",
   "metadata": {},
   "source": [
    "**Destination Node (Key)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "525bdd18-9d73-4a6b-a4a0-86d5a77160ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 12, 2]),\n",
       " tensor([[[-0.0374, -0.0164],\n",
       "          [-0.2285, -0.0970],\n",
       "          [-0.4806, -0.1904],\n",
       "          [ 0.6172,  0.2459],\n",
       "          [-0.2812, -0.1100],\n",
       "          [ 0.1300,  0.0471],\n",
       "          [-0.0320, -0.0121],\n",
       "          [-0.2064, -0.0895],\n",
       "          [-0.3424, -0.1324],\n",
       "          [ 0.6172,  0.2459],\n",
       "          [-0.2473, -0.0984],\n",
       "          [ 0.1300,  0.0471]]], grad_fn=<PermuteBackward0>))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_dst = Hproj @ gat1_attn_dst\n",
    "e_dst = e_dst.permute(2, 1, 0)\n",
    "e_dst.size(), e_dst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c1ad97-05b9-46a3-89fb-35d45b103e07",
   "metadata": {},
   "source": [
    "**Combine Node Directions Together**\n",
    "\n",
    "As we combine we can see that the nodes are starting to learn some positive and negative edges.  We have moved away from predictability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "434efbfb-530b-4d08-b28d-b081b3f23c0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 2]),\n",
       " tensor([[[-4.9465e-02, -1.5084e-02],\n",
       "          [-2.4055e-01, -9.5718e-02],\n",
       "          [-4.9265e-01, -1.8915e-01],\n",
       "          [ 6.0514e-01,  2.4717e-01],\n",
       "          [-2.9328e-01, -1.0866e-01],\n",
       "          [ 1.1793e-01,  4.8396e-02],\n",
       "          [-4.4041e-02, -1.0783e-02],\n",
       "          [-2.1843e-01, -8.8216e-02],\n",
       "          [-3.5445e-01, -1.3109e-01],\n",
       "          [ 6.0514e-01,  2.4717e-01],\n",
       "          [-2.5935e-01, -9.7149e-02],\n",
       "          [ 1.1793e-01,  4.8396e-02]],\n",
       " \n",
       "         [[-1.1031e-01, -7.5011e-03],\n",
       "          [-3.0139e-01, -8.8135e-02],\n",
       "          [-5.5350e-01, -1.8157e-01],\n",
       "          [ 5.4429e-01,  2.5476e-01],\n",
       "          [-3.5412e-01, -1.0108e-01],\n",
       "          [ 5.7080e-02,  5.5979e-02],\n",
       "          [-1.0489e-01, -3.2002e-03],\n",
       "          [-2.7928e-01, -8.0633e-02],\n",
       "          [-4.1530e-01, -1.2350e-01],\n",
       "          [ 5.4429e-01,  2.5476e-01],\n",
       "          [-3.2020e-01, -8.9566e-02],\n",
       "          [ 5.7080e-02,  5.5979e-02]],\n",
       " \n",
       "         [[-1.8674e-01,  7.3048e-03],\n",
       "          [-3.7783e-01, -7.3329e-02],\n",
       "          [-6.2993e-01, -1.6676e-01],\n",
       "          [ 4.6786e-01,  2.6956e-01],\n",
       "          [-4.3055e-01, -8.6269e-02],\n",
       "          [-1.9352e-02,  7.0785e-02],\n",
       "          [-1.8132e-01,  1.1606e-02],\n",
       "          [-3.5571e-01, -6.5827e-02],\n",
       "          [-4.9173e-01, -1.0870e-01],\n",
       "          [ 4.6786e-01,  2.6956e-01],\n",
       "          [-3.9663e-01, -7.4760e-02],\n",
       "          [-1.9352e-02,  7.0785e-02]],\n",
       " \n",
       "         [[ 1.5507e-01, -4.6515e-02],\n",
       "          [-3.6012e-02, -1.2715e-01],\n",
       "          [-2.8812e-01, -2.2058e-01],\n",
       "          [ 8.0967e-01,  2.1574e-01],\n",
       "          [-8.8741e-02, -1.4009e-01],\n",
       "          [ 3.2246e-01,  1.6965e-02],\n",
       "          [ 1.6049e-01, -4.2214e-02],\n",
       "          [-1.3899e-02, -1.1965e-01],\n",
       "          [-1.4992e-01, -1.6252e-01],\n",
       "          [ 8.0967e-01,  2.1574e-01],\n",
       "          [-5.4818e-02, -1.2858e-01],\n",
       "          [ 3.2246e-01,  1.6965e-02]],\n",
       " \n",
       "         [[-1.2451e-01, -1.7610e-03],\n",
       "          [-3.1559e-01, -8.2395e-02],\n",
       "          [-5.6769e-01, -1.7583e-01],\n",
       "          [ 5.3009e-01,  2.6050e-01],\n",
       "          [-3.6832e-01, -9.5335e-02],\n",
       "          [ 4.2885e-02,  6.1719e-02],\n",
       "          [-1.1908e-01,  2.5399e-03],\n",
       "          [-2.9348e-01, -7.4893e-02],\n",
       "          [-4.2950e-01, -1.1776e-01],\n",
       "          [ 5.3009e-01,  2.6050e-01],\n",
       "          [-3.3439e-01, -8.3826e-02],\n",
       "          [ 4.2885e-02,  6.1719e-02]],\n",
       " \n",
       "         [[ 1.6941e-03, -2.5071e-02],\n",
       "          [-1.8939e-01, -1.0571e-01],\n",
       "          [-4.4149e-01, -1.9914e-01],\n",
       "          [ 6.5629e-01,  2.3719e-01],\n",
       "          [-2.4212e-01, -1.1864e-01],\n",
       "          [ 1.6909e-01,  3.8409e-02],\n",
       "          [ 7.1177e-03, -2.0770e-02],\n",
       "          [-1.6727e-01, -9.8203e-02],\n",
       "          [-3.0330e-01, -1.4107e-01],\n",
       "          [ 6.5629e-01,  2.3719e-01],\n",
       "          [-2.0819e-01, -1.0714e-01],\n",
       "          [ 1.6909e-01,  3.8409e-02]],\n",
       " \n",
       "         [[-4.7198e-02, -1.4487e-02],\n",
       "          [-2.3828e-01, -9.5121e-02],\n",
       "          [-4.9038e-01, -1.8856e-01],\n",
       "          [ 6.0740e-01,  2.4777e-01],\n",
       "          [-2.9101e-01, -1.0806e-01],\n",
       "          [ 1.2019e-01,  4.8993e-02],\n",
       "          [-4.1774e-02, -1.0186e-02],\n",
       "          [-2.1617e-01, -8.7619e-02],\n",
       "          [-3.5219e-01, -1.3049e-01],\n",
       "          [ 6.0740e-01,  2.4777e-01],\n",
       "          [-2.5708e-01, -9.6552e-02],\n",
       "          [ 1.2019e-01,  4.8993e-02]],\n",
       " \n",
       "         [[-1.0399e-01, -9.4408e-03],\n",
       "          [-2.9507e-01, -9.0075e-02],\n",
       "          [-5.4717e-01, -1.8351e-01],\n",
       "          [ 5.5061e-01,  2.5282e-01],\n",
       "          [-3.4780e-01, -1.0301e-01],\n",
       "          [ 6.3407e-02,  5.4039e-02],\n",
       "          [-9.8561e-02, -5.1399e-03],\n",
       "          [-2.7295e-01, -8.2573e-02],\n",
       "          [-4.0898e-01, -1.2544e-01],\n",
       "          [ 5.5061e-01,  2.5282e-01],\n",
       "          [-3.1387e-01, -9.1506e-02],\n",
       "          [ 6.3407e-02,  5.4039e-02]],\n",
       " \n",
       "         [[-1.4209e-01,  2.2801e-03],\n",
       "          [-3.3318e-01, -7.8354e-02],\n",
       "          [-5.8528e-01, -1.7179e-01],\n",
       "          [ 5.1251e-01,  2.6454e-01],\n",
       "          [-3.8590e-01, -9.1294e-02],\n",
       "          [ 2.5298e-02,  6.5760e-02],\n",
       "          [-1.3667e-01,  6.5810e-03],\n",
       "          [-3.1106e-01, -7.0852e-02],\n",
       "          [-4.4708e-01, -1.1372e-01],\n",
       "          [ 5.1251e-01,  2.6454e-01],\n",
       "          [-3.5198e-01, -7.9785e-02],\n",
       "          [ 2.5298e-02,  6.5760e-02]],\n",
       " \n",
       "         [[ 1.5507e-01, -4.6515e-02],\n",
       "          [-3.6012e-02, -1.2715e-01],\n",
       "          [-2.8812e-01, -2.2058e-01],\n",
       "          [ 8.0967e-01,  2.1574e-01],\n",
       "          [-8.8741e-02, -1.4009e-01],\n",
       "          [ 3.2246e-01,  1.6965e-02],\n",
       "          [ 1.6049e-01, -4.2214e-02],\n",
       "          [-1.3899e-02, -1.1965e-01],\n",
       "          [-1.4992e-01, -1.6252e-01],\n",
       "          [ 8.0967e-01,  2.1574e-01],\n",
       "          [-5.4818e-02, -1.2858e-01],\n",
       "          [ 3.2246e-01,  1.6965e-02]],\n",
       " \n",
       "         [[-1.1480e-01, -4.7367e-03],\n",
       "          [-3.0588e-01, -8.5371e-02],\n",
       "          [-5.5799e-01, -1.7881e-01],\n",
       "          [ 5.3980e-01,  2.5752e-01],\n",
       "          [-3.5861e-01, -9.8311e-02],\n",
       "          [ 5.2590e-02,  5.8743e-02],\n",
       "          [-1.0938e-01, -4.3579e-04],\n",
       "          [-2.8377e-01, -7.7869e-02],\n",
       "          [-4.1979e-01, -1.2074e-01],\n",
       "          [ 5.3980e-01,  2.5752e-01],\n",
       "          [-3.2469e-01, -8.6802e-02],\n",
       "          [ 5.2590e-02,  5.8743e-02]],\n",
       " \n",
       "         [[ 1.6941e-03, -2.5071e-02],\n",
       "          [-1.8939e-01, -1.0571e-01],\n",
       "          [-4.4149e-01, -1.9914e-01],\n",
       "          [ 6.5629e-01,  2.3719e-01],\n",
       "          [-2.4212e-01, -1.1864e-01],\n",
       "          [ 1.6909e-01,  3.8409e-02],\n",
       "          [ 7.1177e-03, -2.0770e-02],\n",
       "          [-1.6727e-01, -9.8203e-02],\n",
       "          [-3.0330e-01, -1.4107e-01],\n",
       "          [ 6.5629e-01,  2.3719e-01],\n",
       "          [-2.0819e-01, -1.0714e-01],\n",
       "          [ 1.6909e-01,  3.8409e-02]]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = e_src + e_dst\n",
    "e.size(), e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86d840e-f275-495e-a24b-81caa847a831",
   "metadata": {},
   "source": [
    "##### Signed Edge Attention\n",
    "\n",
    "Our edge adjacency prep is the same as before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b662a753-bec7-441b-905b-4fdf8afe4742",
   "metadata": {},
   "source": [
    "**Prep Adjacency Map**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fa730b03-4c4d-48bc-8660-15992a08dedd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 1]),\n",
       " tensor([[[False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True]],\n",
       " \n",
       "         [[ True],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True]],\n",
       " \n",
       "         [[False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True]],\n",
       " \n",
       "         [[False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [False]]]))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signed_a = a_blk.clone()\n",
    "signed_a.fill_diagonal_(0.5)                       # self-loops are positive\n",
    "mask = (signed_a == 0)   \n",
    "mask = mask.unsqueeze(-1)\n",
    "mask.size(), mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ec9d83b6-5707-418b-9cef-5171c52de435",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 1]),\n",
       " tensor([[[ 0.5000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 1.0000],\n",
       "          [-1.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000]],\n",
       " \n",
       "         [[ 0.0000],\n",
       "          [ 0.5000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 1.0000],\n",
       "          [ 1.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000]],\n",
       " \n",
       "         [[ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.5000],\n",
       "          [ 0.0000],\n",
       "          [ 1.0000],\n",
       "          [-1.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000]],\n",
       " \n",
       "         [[ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.5000],\n",
       "          [-1.0000],\n",
       "          [ 1.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000]],\n",
       " \n",
       "         [[ 1.0000],\n",
       "          [ 1.0000],\n",
       "          [ 1.0000],\n",
       "          [-1.0000],\n",
       "          [ 0.5000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000]],\n",
       " \n",
       "         [[-1.0000],\n",
       "          [ 1.0000],\n",
       "          [-1.0000],\n",
       "          [ 1.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.5000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000]],\n",
       " \n",
       "         [[ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.5000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [-1.0000],\n",
       "          [ 1.0000]],\n",
       " \n",
       "         [[ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.5000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [-1.0000],\n",
       "          [-1.0000]],\n",
       " \n",
       "         [[ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.5000],\n",
       "          [ 0.0000],\n",
       "          [ 1.0000],\n",
       "          [-1.0000]],\n",
       " \n",
       "         [[ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.5000],\n",
       "          [-1.0000],\n",
       "          [ 1.0000]],\n",
       " \n",
       "         [[ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [-1.0000],\n",
       "          [-1.0000],\n",
       "          [ 1.0000],\n",
       "          [-1.0000],\n",
       "          [ 0.5000],\n",
       "          [ 0.0000]],\n",
       " \n",
       "         [[ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 1.0000],\n",
       "          [-1.0000],\n",
       "          [-1.0000],\n",
       "          [ 1.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.5000]]]))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signed_a = signed_a.unsqueeze(-1) # [M,M,1] sign tensor\n",
    "signed_a.size(), signed_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb2ccb-c599-4964-9062-328a02faa85f",
   "metadata": {},
   "source": [
    "**Edge Attention**\n",
    "\n",
    "Interestingly both of our edge adjacencies have been pushed negative.  Showing a downweighting of the incoming signed graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e2a68252-8ba1-4533-9d01-0ed058a54897",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 2]),\n",
       " tensor([[[-0.3716, -0.2771],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.7432, -0.5542],\n",
       "          [ 0.7432,  0.5542],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000]],\n",
       " \n",
       "         [[-0.0000, -0.0000],\n",
       "          [-0.3716, -0.2771],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.7432, -0.5542],\n",
       "          [-0.7432, -0.5542],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000]],\n",
       " \n",
       "         [[-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.3716, -0.2771],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.7432, -0.5542],\n",
       "          [ 0.7432,  0.5542],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000]],\n",
       " \n",
       "         [[-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.3716, -0.2771],\n",
       "          [ 0.7432,  0.5542],\n",
       "          [-0.7432, -0.5542],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000]],\n",
       " \n",
       "         [[-0.7432, -0.5542],\n",
       "          [-0.7432, -0.5542],\n",
       "          [-0.7432, -0.5542],\n",
       "          [ 0.7432,  0.5542],\n",
       "          [-0.3716, -0.2771],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000]],\n",
       " \n",
       "         [[ 0.7432,  0.5542],\n",
       "          [-0.7432, -0.5542],\n",
       "          [ 0.7432,  0.5542],\n",
       "          [-0.7432, -0.5542],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.3716, -0.2771],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000]],\n",
       " \n",
       "         [[-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.3716, -0.2771],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [ 0.7432,  0.5542],\n",
       "          [-0.7432, -0.5542]],\n",
       " \n",
       "         [[-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.3716, -0.2771],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [ 0.7432,  0.5542],\n",
       "          [ 0.7432,  0.5542]],\n",
       " \n",
       "         [[-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.3716, -0.2771],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.7432, -0.5542],\n",
       "          [ 0.7432,  0.5542]],\n",
       " \n",
       "         [[-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.3716, -0.2771],\n",
       "          [ 0.7432,  0.5542],\n",
       "          [-0.7432, -0.5542]],\n",
       " \n",
       "         [[-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [ 0.7432,  0.5542],\n",
       "          [ 0.7432,  0.5542],\n",
       "          [-0.7432, -0.5542],\n",
       "          [ 0.7432,  0.5542],\n",
       "          [-0.3716, -0.2771],\n",
       "          [-0.0000, -0.0000]],\n",
       " \n",
       "         [[-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.7432, -0.5542],\n",
       "          [ 0.7432,  0.5542],\n",
       "          [ 0.7432,  0.5542],\n",
       "          [-0.7432, -0.5542],\n",
       "          [-0.0000, -0.0000],\n",
       "          [-0.3716, -0.2771]]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_sig_a = signed_a * gat1_attn_sign\n",
    "e_sig_a.size(), e_sig_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef05aff6-f081-4090-8f1a-effd37631ba0",
   "metadata": {},
   "source": [
    "**Combine Node and Edge Attention**\n",
    "\n",
    "Contrary to the first pass through, we now see that most of the weights at this point are negative.  This is an interesting focus on the down regulation part of the graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6badb187-d7fb-4ee6-8585-402dc183a81c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 2]),\n",
       " tensor([[[-0.4211, -0.2922],\n",
       "          [-0.2405, -0.0957],\n",
       "          [-0.4927, -0.1892],\n",
       "          [ 0.6051,  0.2472],\n",
       "          [-1.0365, -0.6629],\n",
       "          [ 0.8611,  0.6026],\n",
       "          [-0.0440, -0.0108],\n",
       "          [-0.2184, -0.0882],\n",
       "          [-0.3545, -0.1311],\n",
       "          [ 0.6051,  0.2472],\n",
       "          [-0.2594, -0.0971],\n",
       "          [ 0.1179,  0.0484]],\n",
       " \n",
       "         [[-0.1103, -0.0075],\n",
       "          [-0.6730, -0.3652],\n",
       "          [-0.5535, -0.1816],\n",
       "          [ 0.5443,  0.2548],\n",
       "          [-1.0973, -0.6553],\n",
       "          [-0.6861, -0.4982],\n",
       "          [-0.1049, -0.0032],\n",
       "          [-0.2793, -0.0806],\n",
       "          [-0.4153, -0.1235],\n",
       "          [ 0.5443,  0.2548],\n",
       "          [-0.3202, -0.0896],\n",
       "          [ 0.0571,  0.0560]],\n",
       " \n",
       "         [[-0.1867,  0.0073],\n",
       "          [-0.3778, -0.0733],\n",
       "          [-1.0015, -0.4439],\n",
       "          [ 0.4679,  0.2696],\n",
       "          [-1.1738, -0.6405],\n",
       "          [ 0.7239,  0.6250],\n",
       "          [-0.1813,  0.0116],\n",
       "          [-0.3557, -0.0658],\n",
       "          [-0.4917, -0.1087],\n",
       "          [ 0.4679,  0.2696],\n",
       "          [-0.3966, -0.0748],\n",
       "          [-0.0194,  0.0708]],\n",
       " \n",
       "         [[ 0.1551, -0.0465],\n",
       "          [-0.0360, -0.1271],\n",
       "          [-0.2881, -0.2206],\n",
       "          [ 0.4381, -0.0614],\n",
       "          [ 0.6545,  0.4141],\n",
       "          [-0.4207, -0.5372],\n",
       "          [ 0.1605, -0.0422],\n",
       "          [-0.0139, -0.1196],\n",
       "          [-0.1499, -0.1625],\n",
       "          [ 0.8097,  0.2157],\n",
       "          [-0.0548, -0.1286],\n",
       "          [ 0.3225,  0.0170]],\n",
       " \n",
       "         [[-0.8677, -0.5560],\n",
       "          [-1.0588, -0.6366],\n",
       "          [-1.3109, -0.7300],\n",
       "          [ 1.2733,  0.8147],\n",
       "          [-0.7399, -0.3724],\n",
       "          [ 0.0429,  0.0617],\n",
       "          [-0.1191,  0.0025],\n",
       "          [-0.2935, -0.0749],\n",
       "          [-0.4295, -0.1178],\n",
       "          [ 0.5301,  0.2605],\n",
       "          [-0.3344, -0.0838],\n",
       "          [ 0.0429,  0.0617]],\n",
       " \n",
       "         [[ 0.7449,  0.5291],\n",
       "          [-0.9326, -0.6599],\n",
       "          [ 0.3017,  0.3551],\n",
       "          [-0.0869, -0.3170],\n",
       "          [-0.2421, -0.1186],\n",
       "          [-0.2025, -0.2387],\n",
       "          [ 0.0071, -0.0208],\n",
       "          [-0.1673, -0.0982],\n",
       "          [-0.3033, -0.1411],\n",
       "          [ 0.6563,  0.2372],\n",
       "          [-0.2082, -0.1071],\n",
       "          [ 0.1691,  0.0384]],\n",
       " \n",
       "         [[-0.0472, -0.0145],\n",
       "          [-0.2383, -0.0951],\n",
       "          [-0.4904, -0.1886],\n",
       "          [ 0.6074,  0.2478],\n",
       "          [-0.2910, -0.1081],\n",
       "          [ 0.1202,  0.0490],\n",
       "          [-0.4134, -0.2873],\n",
       "          [-0.2162, -0.0876],\n",
       "          [-0.3522, -0.1305],\n",
       "          [ 0.6074,  0.2478],\n",
       "          [ 0.4861,  0.4577],\n",
       "          [-0.6230, -0.5052]],\n",
       " \n",
       "         [[-0.1040, -0.0094],\n",
       "          [-0.2951, -0.0901],\n",
       "          [-0.5472, -0.1835],\n",
       "          [ 0.5506,  0.2528],\n",
       "          [-0.3478, -0.1030],\n",
       "          [ 0.0634,  0.0540],\n",
       "          [-0.0986, -0.0051],\n",
       "          [-0.6446, -0.3597],\n",
       "          [-0.4090, -0.1254],\n",
       "          [ 0.5506,  0.2528],\n",
       "          [ 0.4293,  0.4627],\n",
       "          [ 0.8066,  0.6082]],\n",
       " \n",
       "         [[-0.1421,  0.0023],\n",
       "          [-0.3332, -0.0784],\n",
       "          [-0.5853, -0.1718],\n",
       "          [ 0.5125,  0.2645],\n",
       "          [-0.3859, -0.0913],\n",
       "          [ 0.0253,  0.0658],\n",
       "          [-0.1367,  0.0066],\n",
       "          [-0.3111, -0.0709],\n",
       "          [-0.8187, -0.3908],\n",
       "          [ 0.5125,  0.2645],\n",
       "          [-1.0952, -0.6340],\n",
       "          [ 0.7685,  0.6200]],\n",
       " \n",
       "         [[ 0.1551, -0.0465],\n",
       "          [-0.0360, -0.1271],\n",
       "          [-0.2881, -0.2206],\n",
       "          [ 0.8097,  0.2157],\n",
       "          [-0.0887, -0.1401],\n",
       "          [ 0.3225,  0.0170],\n",
       "          [ 0.1605, -0.0422],\n",
       "          [-0.0139, -0.1196],\n",
       "          [-0.1499, -0.1625],\n",
       "          [ 0.4381, -0.0614],\n",
       "          [ 0.6884,  0.4256],\n",
       "          [-0.4207, -0.5372]],\n",
       " \n",
       "         [[-0.1148, -0.0047],\n",
       "          [-0.3059, -0.0854],\n",
       "          [-0.5580, -0.1788],\n",
       "          [ 0.5398,  0.2575],\n",
       "          [-0.3586, -0.0983],\n",
       "          [ 0.0526,  0.0587],\n",
       "          [ 0.6338,  0.5538],\n",
       "          [ 0.4594,  0.4763],\n",
       "          [-1.1630, -0.6749],\n",
       "          [ 1.2830,  0.8117],\n",
       "          [-0.6963, -0.3639],\n",
       "          [ 0.0526,  0.0587]],\n",
       " \n",
       "         [[ 0.0017, -0.0251],\n",
       "          [-0.1894, -0.1057],\n",
       "          [-0.4415, -0.1991],\n",
       "          [ 0.6563,  0.2372],\n",
       "          [-0.2421, -0.1186],\n",
       "          [ 0.1691,  0.0384],\n",
       "          [-0.7361, -0.5750],\n",
       "          [ 0.5759,  0.4560],\n",
       "          [ 0.4399,  0.4131],\n",
       "          [-0.0869, -0.3170],\n",
       "          [-0.2082, -0.1071],\n",
       "          [-0.2025, -0.2387]]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = e + e_sig_a \n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d27609d-65cc-4be7-9001-3f8235bbb41a",
   "metadata": {},
   "source": [
    "##### Leaky ReLU\n",
    "\n",
    "Leaky ReLU has no learnable parameters so we mainly will see the negative values squeeze together more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "108fb1c6-91e0-434b-bc15-f0765648d015",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 2]),\n",
       " tensor([[[-0.2105, -0.1461],\n",
       "          [-0.1203, -0.0479],\n",
       "          [-0.2463, -0.0946],\n",
       "          [ 0.6051,  0.2472],\n",
       "          [-0.5182, -0.3314],\n",
       "          [ 0.8611,  0.6026],\n",
       "          [-0.0220, -0.0054],\n",
       "          [-0.1092, -0.0441],\n",
       "          [-0.1772, -0.0655],\n",
       "          [ 0.6051,  0.2472],\n",
       "          [-0.1297, -0.0486],\n",
       "          [ 0.1179,  0.0484]],\n",
       " \n",
       "         [[-0.0552, -0.0038],\n",
       "          [-0.3365, -0.1826],\n",
       "          [-0.2767, -0.0908],\n",
       "          [ 0.5443,  0.2548],\n",
       "          [-0.5487, -0.3276],\n",
       "          [-0.3431, -0.2491],\n",
       "          [-0.0524, -0.0016],\n",
       "          [-0.1396, -0.0403],\n",
       "          [-0.2077, -0.0618],\n",
       "          [ 0.5443,  0.2548],\n",
       "          [-0.1601, -0.0448],\n",
       "          [ 0.0571,  0.0560]],\n",
       " \n",
       "         [[-0.0934,  0.0073],\n",
       "          [-0.1889, -0.0367],\n",
       "          [-0.5008, -0.2219],\n",
       "          [ 0.4679,  0.2696],\n",
       "          [-0.5869, -0.3202],\n",
       "          [ 0.7239,  0.6250],\n",
       "          [-0.0907,  0.0116],\n",
       "          [-0.1779, -0.0329],\n",
       "          [-0.2459, -0.0543],\n",
       "          [ 0.4679,  0.2696],\n",
       "          [-0.1983, -0.0374],\n",
       "          [-0.0097,  0.0708]],\n",
       " \n",
       "         [[ 0.1551, -0.0233],\n",
       "          [-0.0180, -0.0636],\n",
       "          [-0.1441, -0.1103],\n",
       "          [ 0.4381, -0.0307],\n",
       "          [ 0.6545,  0.4141],\n",
       "          [-0.2104, -0.2686],\n",
       "          [ 0.1605, -0.0211],\n",
       "          [-0.0069, -0.0598],\n",
       "          [-0.0750, -0.0813],\n",
       "          [ 0.8097,  0.2157],\n",
       "          [-0.0274, -0.0643],\n",
       "          [ 0.3225,  0.0170]],\n",
       " \n",
       "         [[-0.4339, -0.2780],\n",
       "          [-0.5294, -0.3183],\n",
       "          [-0.6554, -0.3650],\n",
       "          [ 1.2733,  0.8147],\n",
       "          [-0.3700, -0.1862],\n",
       "          [ 0.0429,  0.0617],\n",
       "          [-0.0595,  0.0025],\n",
       "          [-0.1467, -0.0374],\n",
       "          [-0.2147, -0.0589],\n",
       "          [ 0.5301,  0.2605],\n",
       "          [-0.1672, -0.0419],\n",
       "          [ 0.0429,  0.0617]],\n",
       " \n",
       "         [[ 0.7449,  0.5291],\n",
       "          [-0.4663, -0.3300],\n",
       "          [ 0.3017,  0.3551],\n",
       "          [-0.0435, -0.1585],\n",
       "          [-0.1211, -0.0593],\n",
       "          [-0.1013, -0.1193],\n",
       "          [ 0.0071, -0.0104],\n",
       "          [-0.0836, -0.0491],\n",
       "          [-0.1516, -0.0705],\n",
       "          [ 0.6563,  0.2372],\n",
       "          [-0.1041, -0.0536],\n",
       "          [ 0.1691,  0.0384]],\n",
       " \n",
       "         [[-0.0236, -0.0072],\n",
       "          [-0.1191, -0.0476],\n",
       "          [-0.2452, -0.0943],\n",
       "          [ 0.6074,  0.2478],\n",
       "          [-0.1455, -0.0540],\n",
       "          [ 0.1202,  0.0490],\n",
       "          [-0.2067, -0.1436],\n",
       "          [-0.1081, -0.0438],\n",
       "          [-0.1761, -0.0652],\n",
       "          [ 0.6074,  0.2478],\n",
       "          [ 0.4861,  0.4577],\n",
       "          [-0.3115, -0.2526]],\n",
       " \n",
       "         [[-0.0520, -0.0047],\n",
       "          [-0.1475, -0.0450],\n",
       "          [-0.2736, -0.0918],\n",
       "          [ 0.5506,  0.2528],\n",
       "          [-0.1739, -0.0515],\n",
       "          [ 0.0634,  0.0540],\n",
       "          [-0.0493, -0.0026],\n",
       "          [-0.3223, -0.1798],\n",
       "          [-0.2045, -0.0627],\n",
       "          [ 0.5506,  0.2528],\n",
       "          [ 0.4293,  0.4627],\n",
       "          [ 0.8066,  0.6082]],\n",
       " \n",
       "         [[-0.0710,  0.0023],\n",
       "          [-0.1666, -0.0392],\n",
       "          [-0.2926, -0.0859],\n",
       "          [ 0.5125,  0.2645],\n",
       "          [-0.1930, -0.0456],\n",
       "          [ 0.0253,  0.0658],\n",
       "          [-0.0683,  0.0066],\n",
       "          [-0.1555, -0.0354],\n",
       "          [-0.4093, -0.1954],\n",
       "          [ 0.5125,  0.2645],\n",
       "          [-0.5476, -0.3170],\n",
       "          [ 0.7685,  0.6200]],\n",
       " \n",
       "         [[ 0.1551, -0.0233],\n",
       "          [-0.0180, -0.0636],\n",
       "          [-0.1441, -0.1103],\n",
       "          [ 0.8097,  0.2157],\n",
       "          [-0.0444, -0.0700],\n",
       "          [ 0.3225,  0.0170],\n",
       "          [ 0.1605, -0.0211],\n",
       "          [-0.0069, -0.0598],\n",
       "          [-0.0750, -0.0813],\n",
       "          [ 0.4381, -0.0307],\n",
       "          [ 0.6884,  0.4256],\n",
       "          [-0.2104, -0.2686]],\n",
       " \n",
       "         [[-0.0574, -0.0024],\n",
       "          [-0.1529, -0.0427],\n",
       "          [-0.2790, -0.0894],\n",
       "          [ 0.5398,  0.2575],\n",
       "          [-0.1793, -0.0492],\n",
       "          [ 0.0526,  0.0587],\n",
       "          [ 0.6338,  0.5538],\n",
       "          [ 0.4594,  0.4763],\n",
       "          [-0.5815, -0.3375],\n",
       "          [ 1.2830,  0.8117],\n",
       "          [-0.3481, -0.1820],\n",
       "          [ 0.0526,  0.0587]],\n",
       " \n",
       "         [[ 0.0017, -0.0125],\n",
       "          [-0.0947, -0.0529],\n",
       "          [-0.2207, -0.0996],\n",
       "          [ 0.6563,  0.2372],\n",
       "          [-0.1211, -0.0593],\n",
       "          [ 0.1691,  0.0384],\n",
       "          [-0.3680, -0.2875],\n",
       "          [ 0.5759,  0.4560],\n",
       "          [ 0.4399,  0.4131],\n",
       "          [-0.0435, -0.1585],\n",
       "          [-0.1041, -0.0536],\n",
       "          [-0.1013, -0.1193]]], grad_fn=<LeakyReluBackward1>))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = gat1_attn_lrelu(out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fe0456-8527-455a-8e22-dda3d79be5fd",
   "metadata": {},
   "source": [
    "##### Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e2c9493a-180a-49da-a5ef-08993627c89e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 2]),\n",
       " tensor([[[-0.2105, -0.1461],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [-0.5182, -0.3314],\n",
       "          [ 0.8611,  0.6026],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [-0.3365, -0.1826],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [-0.5487, -0.3276],\n",
       "          [-0.3431, -0.2491],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [-0.5008, -0.2219],\n",
       "          [   -inf,    -inf],\n",
       "          [-0.5869, -0.3202],\n",
       "          [ 0.7239,  0.6250],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.4381, -0.0307],\n",
       "          [ 0.6545,  0.4141],\n",
       "          [-0.2104, -0.2686],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf]],\n",
       " \n",
       "         [[-0.4339, -0.2780],\n",
       "          [-0.5294, -0.3183],\n",
       "          [-0.6554, -0.3650],\n",
       "          [ 1.2733,  0.8147],\n",
       "          [-0.3700, -0.1862],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf]],\n",
       " \n",
       "         [[ 0.7449,  0.5291],\n",
       "          [-0.4663, -0.3300],\n",
       "          [ 0.3017,  0.3551],\n",
       "          [-0.0435, -0.1585],\n",
       "          [   -inf,    -inf],\n",
       "          [-0.1013, -0.1193],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [-0.2067, -0.1436],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.4861,  0.4577],\n",
       "          [-0.3115, -0.2526]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [-0.3223, -0.1798],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.4293,  0.4627],\n",
       "          [ 0.8066,  0.6082]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [-0.4093, -0.1954],\n",
       "          [   -inf,    -inf],\n",
       "          [-0.5476, -0.3170],\n",
       "          [ 0.7685,  0.6200]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.4381, -0.0307],\n",
       "          [ 0.6884,  0.4256],\n",
       "          [-0.2104, -0.2686]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.6338,  0.5538],\n",
       "          [ 0.4594,  0.4763],\n",
       "          [-0.5815, -0.3375],\n",
       "          [ 1.2830,  0.8117],\n",
       "          [-0.3481, -0.1820],\n",
       "          [   -inf,    -inf]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [-0.3680, -0.2875],\n",
       "          [ 0.5759,  0.4560],\n",
       "          [ 0.4399,  0.4131],\n",
       "          [-0.0435, -0.1585],\n",
       "          [   -inf,    -inf],\n",
       "          [-0.1013, -0.1193]]], grad_fn=<MaskedFillBackward0>))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out.masked_fill(mask, float('-inf'))\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346b4fc9-02e2-46de-83c8-4153130133c7",
   "metadata": {},
   "source": [
    "##### SoftMax\n",
    "\n",
    "We can now see that once we apply softmax we have a diverse range of values but very similar values across the two heads.  Since we've passed through the same example we shouldn't yet expect the heads to diverge much. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "94629877-54c9-47da-9a35-24fd7db07b4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 2]),\n",
       " tensor([[[0.2148, 0.2535],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.1579, 0.2106],\n",
       "          [0.6273, 0.5359],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.3569, 0.3571],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2886, 0.3089],\n",
       "          [0.3545, 0.3341],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.1880, 0.2359],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.1724, 0.2138],\n",
       "          [0.6396, 0.5503],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3617, 0.2986],\n",
       "          [0.4491, 0.4659],\n",
       "          [0.1891, 0.2354],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.1077, 0.1438],\n",
       "          [0.0978, 0.1381],\n",
       "          [0.0863, 0.1318],\n",
       "          [0.5935, 0.4288],\n",
       "          [0.1148, 0.1576],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.3542, 0.3040],\n",
       "          [0.1055, 0.1288],\n",
       "          [0.2274, 0.2554],\n",
       "          [0.1610, 0.1528],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.1520, 0.1589],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2564, 0.2687],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.5127, 0.4903],\n",
       "          [0.2309, 0.2410]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.1610, 0.1961],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3413, 0.3728],\n",
       "          [0.4977, 0.4312]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.1954, 0.2412],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.1702, 0.2136],\n",
       "          [0.6345, 0.5452]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3562, 0.2970],\n",
       "          [0.4575, 0.4688],\n",
       "          [0.1863, 0.2341]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2260, 0.2434],\n",
       "          [0.1898, 0.2252],\n",
       "          [0.0670, 0.0998],\n",
       "          [0.4325, 0.3150],\n",
       "          [0.0846, 0.1166],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.1176, 0.1344],\n",
       "          [0.3023, 0.2827],\n",
       "          [0.2638, 0.2709],\n",
       "          [0.1627, 0.1529],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.1536, 0.1590]]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = torch.softmax(out, dim=1)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15384f1b-23ee-42d9-99b8-8388c5337314",
   "metadata": {},
   "source": [
    "##### Attention Dropout\n",
    "\n",
    "Without learnable parameters we see a similar behavior as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fc9f58a5-547f-42e5-b905-1c3770d12ef4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 2]),\n",
       " tensor([[[0.2261, 0.2668],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.1662, 0.2217],\n",
       "          [0.6603, 0.5641],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.3756, 0.3758],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3038, 0.3251],\n",
       "          [0.3732, 0.3517],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.1979, 0.2483],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.1815, 0.2251],\n",
       "          [0.6733, 0.5792],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3808, 0.3144],\n",
       "          [0.0000, 0.4905],\n",
       "          [0.1991, 0.2478],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.1133, 0.0000],\n",
       "          [0.0000, 0.1454],\n",
       "          [0.0908, 0.1387],\n",
       "          [0.6247, 0.4513],\n",
       "          [0.1208, 0.1659],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.3728, 0.3200],\n",
       "          [0.1110, 0.1355],\n",
       "          [0.2393, 0.2689],\n",
       "          [0.1695, 0.1609],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.1600, 0.1673],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2699, 0.2829],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.5397, 0.5161],\n",
       "          [0.2431, 0.2537]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.1694, 0.2064],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3593, 0.3924],\n",
       "          [0.5239, 0.4539]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2057, 0.2539],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.1791, 0.2248],\n",
       "          [0.6679, 0.5739]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3750, 0.0000],\n",
       "          [0.4816, 0.4935],\n",
       "          [0.1961, 0.2465]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2379, 0.2562],\n",
       "          [0.1998, 0.2371],\n",
       "          [0.0706, 0.1051],\n",
       "          [0.4553, 0.3316],\n",
       "          [0.0891, 0.1227],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.1238, 0.1415],\n",
       "          [0.3182, 0.2976],\n",
       "          [0.2777, 0.2851],\n",
       "          [0.1713, 0.1610],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.1617, 0.1674]]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = gat1_attn_drop(out) \n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3568ece-cb96-4202-ae61-92063cc63997",
   "metadata": {},
   "source": [
    "##### Sign Aware Attention\n",
    "\n",
    "As we apply Direction Scaled Attention we'll see our network attention emerge with our directional values.  As with the other layers, the previous semblance of patterns is now gone.  Interestingly, if you look closely at this point the negative edges tend to be larger than the positive, showing a model that is weighing them as more important. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8240dc47-a8ec-4bb7-bb71-0d6093cf0b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 12, 12]),\n",
       " tensor([[[ 0.1131,  0.0000,  0.0000,  0.0000,  0.1662, -0.6603,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.1878,  0.0000,  0.0000,  0.3038,  0.3732,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0989,  0.0000,  0.1815, -0.6733,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.1904, -0.0000,  0.1991,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.1133,  0.0000,  0.0908, -0.6247,  0.0604,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.3728,  0.1110, -0.2393,  0.1695,  0.0000,  0.0800,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1350,\n",
       "            0.0000,  0.0000,  0.0000, -0.5397,  0.2431],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0847,  0.0000,  0.0000, -0.3593, -0.5239],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.1028,  0.0000,  0.1791, -0.6679],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.1875, -0.4816,  0.1961],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.2379,\n",
       "           -0.1998,  0.0706, -0.4553,  0.0446,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1238,\n",
       "           -0.3182, -0.2777,  0.1713,  0.0000,  0.0808]],\n",
       " \n",
       "         [[ 0.1334,  0.0000,  0.0000,  0.0000,  0.2217, -0.5641,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.1879,  0.0000,  0.0000,  0.3251,  0.3517,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.1242,  0.0000,  0.2251, -0.5792,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.1572, -0.4905,  0.2478,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.1454,  0.1387, -0.4513,  0.0829,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.3200,  0.1355, -0.2689,  0.1609,  0.0000,  0.0837,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1414,\n",
       "            0.0000,  0.0000,  0.0000, -0.5161,  0.2537],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.1032,  0.0000,  0.0000, -0.3924, -0.4539],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.1270,  0.0000,  0.2248, -0.5739],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000, -0.4935,  0.2465],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.2562,\n",
       "           -0.2371,  0.1051, -0.3316,  0.0614,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1415,\n",
       "           -0.2976, -0.2851,  0.1610,  0.0000,  0.0837]]],\n",
       "        grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out.permute(2, 0, 1) # [H,M,M]\n",
    "s_heads = signed_a.squeeze(-1).unsqueeze(0).expand(heads, -1, -1) # [H,M,M]\n",
    "out = out * s_heads # [H,M,M]\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33272957-8515-437d-94f1-1ac503676728",
   "metadata": {},
   "source": [
    "##### Message Aggregation\n",
    "\n",
    "We can really see here how our heads have diverged in values when applied to the input linear projection.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a7829906-fb00-48b0-ba81-5b843d286c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 12, 3]),\n",
       " tensor([[[ 0.0994,  0.1371,  0.0300],\n",
       "          [ 0.0563,  0.0690,  0.0276],\n",
       "          [ 0.1357,  0.1827,  0.0436],\n",
       "          [-0.1031, -0.1368, -0.0377],\n",
       "          [ 0.3235,  0.4206,  0.1263],\n",
       "          [-0.1590, -0.2034, -0.0607],\n",
       "          [-0.1151, -0.1527, -0.0431],\n",
       "          [-0.0017,  0.0033, -0.0065],\n",
       "          [ 0.1217,  0.1638,  0.0382],\n",
       "          [-0.1865, -0.2456, -0.0712],\n",
       "          [ 0.1938,  0.2519,  0.0719],\n",
       "          [-0.1978, -0.2510, -0.0787]],\n",
       " \n",
       "         [[ 0.1013,  0.0875,  0.0025],\n",
       "          [ 0.0618,  0.0525,  0.0163],\n",
       "          [ 0.1429,  0.1209,  0.0078],\n",
       "          [-0.1907, -0.1623, -0.0186],\n",
       "          [ 0.2849,  0.2421,  0.0418],\n",
       "          [-0.1580, -0.1305, -0.0201],\n",
       "          [-0.1106, -0.0940, -0.0126],\n",
       "          [-0.0109, -0.0067, -0.0090],\n",
       "          [ 0.1256,  0.1051,  0.0063],\n",
       "          [-0.1092, -0.0929, -0.0121],\n",
       "          [ 0.1431,  0.1178,  0.0145],\n",
       "          [-0.1892, -0.1570, -0.0297]]], grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out @ Hproj # [H,M,O]\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d5fc3d-78a8-4387-9d3d-68b2fa8078f8",
   "metadata": {},
   "source": [
    "##### Concatenate Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "39f415b8-144d-47d6-9f8e-819f82178885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 6]),\n",
       " tensor([[ 0.0994,  0.1371,  0.0300,  0.1013,  0.0875,  0.0025],\n",
       "         [ 0.0563,  0.0690,  0.0276,  0.0618,  0.0525,  0.0163],\n",
       "         [ 0.1357,  0.1827,  0.0436,  0.1429,  0.1209,  0.0078],\n",
       "         [-0.1031, -0.1368, -0.0377, -0.1907, -0.1623, -0.0186],\n",
       "         [ 0.3235,  0.4206,  0.1263,  0.2849,  0.2421,  0.0418],\n",
       "         [-0.1590, -0.2034, -0.0607, -0.1580, -0.1305, -0.0201],\n",
       "         [-0.1151, -0.1527, -0.0431, -0.1106, -0.0940, -0.0126],\n",
       "         [-0.0017,  0.0033, -0.0065, -0.0109, -0.0067, -0.0090],\n",
       "         [ 0.1217,  0.1638,  0.0382,  0.1256,  0.1051,  0.0063],\n",
       "         [-0.1865, -0.2456, -0.0712, -0.1092, -0.0929, -0.0121],\n",
       "         [ 0.1938,  0.2519,  0.0719,  0.1431,  0.1178,  0.0145],\n",
       "         [-0.1978, -0.2510, -0.0787, -0.1892, -0.1570, -0.0297]],\n",
       "        grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out.permute(1, 0, 2) # [M,H,O]\n",
    "out = out.reshape(B_batch*N_nodes, heads * head_dim)\n",
    "out.size(), out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304c5c1d-d448-4e8e-900c-609fc19b3754",
   "metadata": {},
   "source": [
    "##### Add Bias\n",
    "\n",
    "We did see differing bias per channel so you can see some adjustments to each channel's value as it's applied. It is minor pushes as expected from bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7c25f6ba-6e13-4e53-8d3f-363d1a0128df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 6]),\n",
       " tensor([[ 0.0982,  0.1347,  0.0288,  0.1000,  0.0860,  0.0013],\n",
       "         [ 0.0551,  0.0666,  0.0264,  0.0606,  0.0510,  0.0151],\n",
       "         [ 0.1345,  0.1803,  0.0423,  0.1416,  0.1194,  0.0065],\n",
       "         [-0.1043, -0.1392, -0.0389, -0.1919, -0.1639, -0.0199],\n",
       "         [ 0.3223,  0.4182,  0.1251,  0.2837,  0.2406,  0.0405],\n",
       "         [-0.1602, -0.2057, -0.0619, -0.1592, -0.1320, -0.0213],\n",
       "         [-0.1164, -0.1551, -0.0443, -0.1118, -0.0955, -0.0138],\n",
       "         [-0.0029,  0.0009, -0.0077, -0.0122, -0.0082, -0.0103],\n",
       "         [ 0.1205,  0.1614,  0.0370,  0.1243,  0.1035,  0.0051],\n",
       "         [-0.1877, -0.2480, -0.0724, -0.1104, -0.0944, -0.0133],\n",
       "         [ 0.1926,  0.2495,  0.0707,  0.1418,  0.1162,  0.0132],\n",
       "         [-0.1990, -0.2534, -0.0799, -0.1904, -0.1586, -0.0309]],\n",
       "        grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out + gat1_bias\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c188f99b-3287-400a-83db-e99389b1962a",
   "metadata": {},
   "source": [
    "#### GAT Block - ELU\n",
    "\n",
    "Since our first GAT layer had some larger negative value outputs, we can now see the power of ELU.  For larger negative values (e.g. the last row), you can see that the values were brought closer to 0 smoothing out the negative impact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "74a0d697-2e7a-4197-88a0-1b9753e8fd67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 6]),\n",
       " tensor([[ 0.0982,  0.1347,  0.0288,  0.1000,  0.0860,  0.0013],\n",
       "         [ 0.0551,  0.0666,  0.0264,  0.0606,  0.0510,  0.0151],\n",
       "         [ 0.1345,  0.1803,  0.0423,  0.1416,  0.1194,  0.0065],\n",
       "         [-0.0991, -0.1300, -0.0382, -0.1746, -0.1512, -0.0197],\n",
       "         [ 0.3223,  0.4182,  0.1251,  0.2837,  0.2406,  0.0405],\n",
       "         [-0.1480, -0.1860, -0.0600, -0.1472, -0.1237, -0.0211],\n",
       "         [-0.1099, -0.1437, -0.0434, -0.1058, -0.0911, -0.0137],\n",
       "         [-0.0029,  0.0009, -0.0077, -0.0121, -0.0082, -0.0102],\n",
       "         [ 0.1205,  0.1614,  0.0370,  0.1243,  0.1035,  0.0051],\n",
       "         [-0.1711, -0.2196, -0.0699, -0.1046, -0.0901, -0.0132],\n",
       "         [ 0.1926,  0.2495,  0.0707,  0.1418,  0.1162,  0.0132],\n",
       "         [-0.1805, -0.2238, -0.0768, -0.1734, -0.1467, -0.0304]],\n",
       "        grad_fn=<EluBackward0>))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = gat_elu1(out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfcd0a9-f2f1-4a61-9cc2-0d6ad3271652",
   "metadata": {},
   "source": [
    "#### GAT Block - Second Attention Layer\n",
    "\n",
    "Now we'll go into the second attention block that mirrors the second hop. Recall that this layer also had significant changes away from the scaled initiation by head including the introduction of positive and negative weights by node and by channel.  We'll discuss the impact as we go through. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531916cf-2740-460f-bbad-173fa05d7880",
   "metadata": {},
   "source": [
    "##### Feature Dropout\n",
    "\n",
    "Again the dropout is not learnable so we'll end up with the same dropout behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a2073313-c60b-4c41-baf2-d629ddab9395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 6]),\n",
       " tensor([[ 0.1091,  0.1496,  0.0320,  0.1112,  0.0956,  0.0014],\n",
       "         [ 0.0612,  0.0740,  0.0293,  0.0000,  0.0000,  0.0167],\n",
       "         [ 0.1494,  0.2004,  0.0470,  0.1574,  0.1326,  0.0072],\n",
       "         [-0.1101, -0.1444, -0.0424, -0.1940, -0.1680, -0.0218],\n",
       "         [ 0.3581,  0.4646,  0.1390,  0.3152,  0.2673,  0.0451],\n",
       "         [-0.1645, -0.2066, -0.0667, -0.1636, -0.1374, -0.0234],\n",
       "         [-0.1221, -0.1596, -0.0482, -0.1175, -0.1012, -0.0153],\n",
       "         [-0.0000,  0.0010, -0.0085, -0.0134, -0.0091, -0.0000],\n",
       "         [ 0.1339,  0.1794,  0.0411,  0.1382,  0.1150,  0.0056],\n",
       "         [-0.0000, -0.0000, -0.0776, -0.1162, -0.1001, -0.0000],\n",
       "         [ 0.2140,  0.2772,  0.0786,  0.1576,  0.1291,  0.0147],\n",
       "         [-0.2005, -0.2487, -0.0853, -0.1927, -0.1630, -0.0000]],\n",
       "        grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_drop = gat2_fdrop(out)\n",
    "out_drop.size(), out_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d04f9e-7fa8-43f7-b91e-cadc105b2648",
   "metadata": {},
   "source": [
    "##### Per-Head Linear Projection\n",
    "\n",
    "We again see that our linear projection differs both by channel and head.  We're getting a lot of mixes in signs even for the same edge highlighting a network that's learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "bde75e98-5ed8-4876-bc4d-d5e4eaeec732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 12, 3]),\n",
       " tensor([[[-1.2238e-02,  8.7527e-02,  1.8730e-01],\n",
       "          [-1.0088e-02,  2.6161e-02,  6.2412e-02],\n",
       "          [-1.6725e-02,  1.2210e-01,  2.6093e-01],\n",
       "          [ 1.2335e-02, -1.2382e-01, -2.5998e-01],\n",
       "          [-4.5811e-02,  2.7205e-01,  5.8994e-01],\n",
       "          [ 2.0188e-02, -1.3225e-01, -2.8470e-01],\n",
       "          [ 1.5716e-02, -9.7071e-02, -2.0987e-01],\n",
       "          [ 6.6039e-04, -5.3594e-03, -1.1380e-02],\n",
       "          [-1.4738e-02,  1.0790e-01,  2.3055e-01],\n",
       "          [ 8.7499e-03, -5.0043e-02, -1.0884e-01],\n",
       "          [-2.6363e-02,  1.4788e-01,  3.2213e-01],\n",
       "          [ 2.3136e-02, -1.5490e-01, -3.3296e-01]],\n",
       " \n",
       "         [[-3.1766e-03,  9.6585e-02,  1.9635e-01],\n",
       "          [-7.1502e-03,  2.9097e-02,  6.5344e-02],\n",
       "          [-3.9907e-03,  1.3483e-01,  2.7365e-01],\n",
       "          [-1.1474e-03, -1.3730e-01, -2.7344e-01],\n",
       "          [-1.6539e-02,  3.0131e-01,  6.1917e-01],\n",
       "          [ 5.8274e-03, -1.4661e-01, -2.9904e-01],\n",
       "          [ 5.1941e-03, -1.0759e-01, -2.2037e-01],\n",
       "          [-2.6558e-04, -6.2851e-03, -1.2305e-02],\n",
       "          [-3.5583e-03,  1.1908e-01,  2.4172e-01],\n",
       "          [-1.8374e-04, -5.8973e-02, -1.1776e-01],\n",
       "          [-1.0714e-02,  1.6352e-01,  3.3775e-01],\n",
       "          [ 5.9753e-03, -1.7206e-01, -3.5009e-01]]], grad_fn=<CloneBackward0>))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hproj = out_drop @ gat2_attn_w\n",
    "Hproj.size(), Hproj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34244438-2af8-42a8-98fb-394ae9c2d7e0",
   "metadata": {},
   "source": [
    "#####  Linear Node Attention\n",
    "\n",
    "Interestingly with these layers we see that the source maintains the sign of the linear projection while the destination is flipping the signs.  This shows the model learning some interactions on the second hop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66513c53-3f24-48bd-9a45-aeb99749479d",
   "metadata": {},
   "source": [
    "**Source Node (Query)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "78c570a9-7d53-48dd-867a-082317200a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 1, 2]),\n",
       " tensor([[[ 0.0788,  0.0427]],\n",
       " \n",
       "         [[ 0.0235,  0.0131]],\n",
       " \n",
       "         [[ 0.1099,  0.0597]],\n",
       " \n",
       "         [[-0.1114, -0.0606]],\n",
       " \n",
       "         [[ 0.2449,  0.1336]],\n",
       " \n",
       "         [[-0.1190, -0.0649]],\n",
       " \n",
       "         [[-0.0874, -0.0477]],\n",
       " \n",
       "         [[-0.0048, -0.0028]],\n",
       " \n",
       "         [[ 0.0971,  0.0527]],\n",
       " \n",
       "         [[-0.0450, -0.0260]],\n",
       " \n",
       "         [[ 0.1331,  0.0726]],\n",
       " \n",
       "         [[-0.1394, -0.0762]]], grad_fn=<PermuteBackward0>))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_src = Hproj @ gat2_attn_src\n",
    "e_src = e_src.permute(1, 2, 0)\n",
    "e_src.size(), e_src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67a8807-b0d8-40ff-ab37-9956208dc38a",
   "metadata": {},
   "source": [
    "**Destination Node (Key)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5fcec189-75cb-406a-b96f-68d5e6de61bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 12, 2]),\n",
       " tensor([[[-0.2676, -0.2270],\n",
       "          [-0.0852, -0.0729],\n",
       "          [-0.3730, -0.3166],\n",
       "          [ 0.3745,  0.3186],\n",
       "          [-0.8380, -0.7131],\n",
       "          [ 0.4057,  0.3453],\n",
       "          [ 0.2985,  0.2541],\n",
       "          [ 0.0163,  0.0144],\n",
       "          [-0.3296, -0.2796],\n",
       "          [ 0.1544,  0.1371],\n",
       "          [-0.4567, -0.3883],\n",
       "          [ 0.4747,  0.4047]]], grad_fn=<PermuteBackward0>))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_dst = Hproj @ gat2_attn_dst\n",
    "e_dst = e_dst.permute(2, 1, 0)\n",
    "e_dst.size(), e_dst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afc6491-889d-4b2f-91ff-658b31d02348",
   "metadata": {},
   "source": [
    "**Combine Node Directions Together**\n",
    "\n",
    "The combined weights are again differing by head and by edge significantly showing some learning along the way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "19635443-4e9a-4d77-996a-a949f2b479bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 2]),\n",
       " tensor([[[-1.8879e-01, -1.8428e-01],\n",
       "          [-6.4097e-03, -3.0178e-02],\n",
       "          [-2.9419e-01, -2.7385e-01],\n",
       "          [ 4.5325e-01,  3.6132e-01],\n",
       "          [-7.5918e-01, -6.7036e-01],\n",
       "          [ 4.8444e-01,  3.8809e-01],\n",
       "          [ 3.7725e-01,  2.9685e-01],\n",
       "          [ 9.5088e-02,  5.7175e-02],\n",
       "          [-2.5079e-01, -2.3689e-01],\n",
       "          [ 2.3318e-01,  1.7981e-01],\n",
       "          [-3.7789e-01, -3.4552e-01],\n",
       "          [ 5.5351e-01,  4.4741e-01]],\n",
       " \n",
       "         [[-2.4402e-01, -2.1393e-01],\n",
       "          [-6.1641e-02, -5.9828e-02],\n",
       "          [-3.4942e-01, -3.0350e-01],\n",
       "          [ 3.9802e-01,  3.3167e-01],\n",
       "          [-8.1441e-01, -7.0001e-01],\n",
       "          [ 4.2921e-01,  3.5844e-01],\n",
       "          [ 3.2202e-01,  2.6720e-01],\n",
       "          [ 3.9857e-02,  2.7525e-02],\n",
       "          [-3.0603e-01, -2.6654e-01],\n",
       "          [ 1.7795e-01,  1.5016e-01],\n",
       "          [-4.3312e-01, -3.7517e-01],\n",
       "          [ 4.9828e-01,  4.1776e-01]],\n",
       " \n",
       "         [[-1.5767e-01, -1.6737e-01],\n",
       "          [ 2.4705e-02, -1.3268e-02],\n",
       "          [-2.6307e-01, -2.5694e-01],\n",
       "          [ 4.8436e-01,  3.7823e-01],\n",
       "          [-7.2807e-01, -6.5345e-01],\n",
       "          [ 5.1556e-01,  4.0500e-01],\n",
       "          [ 4.0837e-01,  3.1376e-01],\n",
       "          [ 1.2620e-01,  7.4085e-02],\n",
       "          [-2.1968e-01, -2.1998e-01],\n",
       "          [ 2.6429e-01,  1.9672e-01],\n",
       "          [-3.4677e-01, -3.2861e-01],\n",
       "          [ 5.8463e-01,  4.6432e-01]],\n",
       " \n",
       "         [[-3.7900e-01, -2.8759e-01],\n",
       "          [-1.9663e-01, -1.3349e-01],\n",
       "          [-4.8441e-01, -3.7716e-01],\n",
       "          [ 2.6303e-01,  2.5801e-01],\n",
       "          [-9.4940e-01, -7.7367e-01],\n",
       "          [ 2.9422e-01,  2.8478e-01],\n",
       "          [ 1.8704e-01,  1.9354e-01],\n",
       "          [-9.5128e-02, -4.6137e-02],\n",
       "          [-4.4101e-01, -3.4020e-01],\n",
       "          [ 4.2961e-02,  7.6501e-02],\n",
       "          [-5.6810e-01, -4.4883e-01],\n",
       "          [ 3.6329e-01,  3.4410e-01]],\n",
       " \n",
       "         [[-2.2709e-02, -9.3437e-02],\n",
       "          [ 1.5967e-01,  6.0666e-02],\n",
       "          [-1.2811e-01, -1.8300e-01],\n",
       "          [ 6.1933e-01,  4.5217e-01],\n",
       "          [-5.9310e-01, -5.7951e-01],\n",
       "          [ 6.5052e-01,  4.7893e-01],\n",
       "          [ 5.4333e-01,  3.8769e-01],\n",
       "          [ 2.6117e-01,  1.4802e-01],\n",
       "          [-8.4715e-02, -1.4605e-01],\n",
       "          [ 3.9926e-01,  2.7066e-01],\n",
       "          [-2.1181e-01, -2.5468e-01],\n",
       "          [ 7.1959e-01,  5.3826e-01]],\n",
       " \n",
       "         [[-3.8660e-01, -2.9195e-01],\n",
       "          [-2.0422e-01, -1.3785e-01],\n",
       "          [-4.9200e-01, -3.8152e-01],\n",
       "          [ 2.5544e-01,  2.5366e-01],\n",
       "          [-9.5699e-01, -7.7803e-01],\n",
       "          [ 2.8663e-01,  2.8042e-01],\n",
       "          [ 1.7945e-01,  1.8918e-01],\n",
       "          [-1.0272e-01, -5.0493e-02],\n",
       "          [-4.4860e-01, -3.4456e-01],\n",
       "          [ 3.5370e-02,  7.2145e-02],\n",
       "          [-5.7569e-01, -4.5319e-01],\n",
       "          [ 3.5570e-01,  3.3974e-01]],\n",
       " \n",
       "         [[-3.5493e-01, -2.7470e-01],\n",
       "          [-1.7255e-01, -1.2060e-01],\n",
       "          [-4.6033e-01, -3.6427e-01],\n",
       "          [ 2.8710e-01,  2.7090e-01],\n",
       "          [-9.2532e-01, -7.6078e-01],\n",
       "          [ 3.1830e-01,  2.9767e-01],\n",
       "          [ 2.1111e-01,  2.0643e-01],\n",
       "          [-7.1055e-02, -3.3247e-02],\n",
       "          [-4.1694e-01, -3.2731e-01],\n",
       "          [ 6.7034e-02,  8.9391e-02],\n",
       "          [-5.4403e-01, -4.3594e-01],\n",
       "          [ 3.8737e-01,  3.5699e-01]],\n",
       " \n",
       "         [[-2.7239e-01, -2.2979e-01],\n",
       "          [-9.0010e-02, -7.5689e-02],\n",
       "          [-3.7779e-01, -3.1936e-01],\n",
       "          [ 3.6965e-01,  3.1581e-01],\n",
       "          [-8.4278e-01, -7.1587e-01],\n",
       "          [ 4.0084e-01,  3.4258e-01],\n",
       "          [ 2.9365e-01,  2.5134e-01],\n",
       "          [ 1.1488e-02,  1.1663e-02],\n",
       "          [-3.3439e-01, -2.8240e-01],\n",
       "          [ 1.4958e-01,  1.3430e-01],\n",
       "          [-4.6149e-01, -3.9103e-01],\n",
       "          [ 4.6991e-01,  4.0190e-01]],\n",
       " \n",
       "         [[-1.7045e-01, -1.7434e-01],\n",
       "          [ 1.1929e-02, -2.0235e-02],\n",
       "          [-2.7585e-01, -2.6390e-01],\n",
       "          [ 4.7159e-01,  3.7127e-01],\n",
       "          [-7.4084e-01, -6.6041e-01],\n",
       "          [ 5.0278e-01,  3.9803e-01],\n",
       "          [ 3.9559e-01,  3.0679e-01],\n",
       "          [ 1.1343e-01,  6.7117e-02],\n",
       "          [-2.3246e-01, -2.2695e-01],\n",
       "          [ 2.5152e-01,  1.8976e-01],\n",
       "          [-3.5955e-01, -3.3558e-01],\n",
       "          [ 5.7185e-01,  4.5735e-01]],\n",
       " \n",
       "         [[-3.1261e-01, -2.5305e-01],\n",
       "          [-1.3023e-01, -9.8950e-02],\n",
       "          [-4.1801e-01, -3.4262e-01],\n",
       "          [ 3.2943e-01,  2.9255e-01],\n",
       "          [-8.8300e-01, -7.3913e-01],\n",
       "          [ 3.6062e-01,  3.1932e-01],\n",
       "          [ 2.5344e-01,  2.2808e-01],\n",
       "          [-2.8729e-02, -1.1598e-02],\n",
       "          [-3.7461e-01, -3.0566e-01],\n",
       "          [ 1.0936e-01,  1.1104e-01],\n",
       "          [-5.0170e-01, -4.1429e-01],\n",
       "          [ 4.2969e-01,  3.7864e-01]],\n",
       " \n",
       "         [[-1.3447e-01, -1.5447e-01],\n",
       "          [ 4.7907e-02, -3.6446e-04],\n",
       "          [-2.3987e-01, -2.4403e-01],\n",
       "          [ 5.0756e-01,  3.9114e-01],\n",
       "          [-7.0486e-01, -6.4054e-01],\n",
       "          [ 5.3876e-01,  4.1790e-01],\n",
       "          [ 4.3157e-01,  3.2666e-01],\n",
       "          [ 1.4940e-01,  8.6988e-02],\n",
       "          [-1.9648e-01, -2.0708e-01],\n",
       "          [ 2.8749e-01,  2.0963e-01],\n",
       "          [-3.2357e-01, -3.1571e-01],\n",
       "          [ 6.0783e-01,  4.7723e-01]],\n",
       " \n",
       "         [[-4.0698e-01, -3.0319e-01],\n",
       "          [-2.2460e-01, -1.4909e-01],\n",
       "          [-5.1238e-01, -3.9276e-01],\n",
       "          [ 2.3505e-01,  2.4242e-01],\n",
       "          [-9.7738e-01, -7.8927e-01],\n",
       "          [ 2.6625e-01,  2.6918e-01],\n",
       "          [ 1.5906e-01,  1.7794e-01],\n",
       "          [-1.2311e-01, -6.1734e-02],\n",
       "          [-4.6899e-01, -3.5580e-01],\n",
       "          [ 1.4982e-02,  6.0904e-02],\n",
       "          [-5.9608e-01, -4.6443e-01],\n",
       "          [ 3.3532e-01,  3.2850e-01]]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = e_src + e_dst            # [M,M,H\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8f04a0-f3a2-4297-a459-033442f96281",
   "metadata": {},
   "source": [
    "##### Leaky ReLU\n",
    "\n",
    "Leaky ReLU pulls in negatives again, cutting them by half "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b99a205f-46db-46c4-a8a0-239f7f8cdd54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 2]),\n",
       " tensor([[[-9.4394e-02, -9.2140e-02],\n",
       "          [-3.2048e-03, -1.5089e-02],\n",
       "          [-1.4709e-01, -1.3692e-01],\n",
       "          [ 4.5325e-01,  3.6132e-01],\n",
       "          [-3.7959e-01, -3.3518e-01],\n",
       "          [ 4.8444e-01,  3.8809e-01],\n",
       "          [ 3.7725e-01,  2.9685e-01],\n",
       "          [ 9.5088e-02,  5.7175e-02],\n",
       "          [-1.2540e-01, -1.1845e-01],\n",
       "          [ 2.3318e-01,  1.7981e-01],\n",
       "          [-1.8894e-01, -1.7276e-01],\n",
       "          [ 5.5351e-01,  4.4741e-01]],\n",
       " \n",
       "         [[-1.2201e-01, -1.0697e-01],\n",
       "          [-3.0820e-02, -2.9914e-02],\n",
       "          [-1.7471e-01, -1.5175e-01],\n",
       "          [ 3.9802e-01,  3.3167e-01],\n",
       "          [-4.0721e-01, -3.5000e-01],\n",
       "          [ 4.2921e-01,  3.5844e-01],\n",
       "          [ 3.2202e-01,  2.6720e-01],\n",
       "          [ 3.9857e-02,  2.7525e-02],\n",
       "          [-1.5301e-01, -1.3327e-01],\n",
       "          [ 1.7795e-01,  1.5016e-01],\n",
       "          [-2.1656e-01, -1.8758e-01],\n",
       "          [ 4.9828e-01,  4.1776e-01]],\n",
       " \n",
       "         [[-7.8837e-02, -8.3685e-02],\n",
       "          [ 2.4705e-02, -6.6339e-03],\n",
       "          [-1.3154e-01, -1.2847e-01],\n",
       "          [ 4.8436e-01,  3.7823e-01],\n",
       "          [-3.6403e-01, -3.2672e-01],\n",
       "          [ 5.1556e-01,  4.0500e-01],\n",
       "          [ 4.0837e-01,  3.1376e-01],\n",
       "          [ 1.2620e-01,  7.4085e-02],\n",
       "          [-1.0984e-01, -1.0999e-01],\n",
       "          [ 2.6429e-01,  1.9672e-01],\n",
       "          [-1.7339e-01, -1.6430e-01],\n",
       "          [ 5.8463e-01,  4.6432e-01]],\n",
       " \n",
       "         [[-1.8950e-01, -1.4380e-01],\n",
       "          [-9.8313e-02, -6.6745e-02],\n",
       "          [-2.4220e-01, -1.8858e-01],\n",
       "          [ 2.6303e-01,  2.5801e-01],\n",
       "          [-4.7470e-01, -3.8683e-01],\n",
       "          [ 2.9422e-01,  2.8478e-01],\n",
       "          [ 1.8704e-01,  1.9354e-01],\n",
       "          [-4.7564e-02, -2.3068e-02],\n",
       "          [-2.2051e-01, -1.7010e-01],\n",
       "          [ 4.2961e-02,  7.6501e-02],\n",
       "          [-2.8405e-01, -2.2442e-01],\n",
       "          [ 3.6329e-01,  3.4410e-01]],\n",
       " \n",
       "         [[-1.1355e-02, -4.6718e-02],\n",
       "          [ 1.5967e-01,  6.0666e-02],\n",
       "          [-6.4055e-02, -9.1502e-02],\n",
       "          [ 6.1933e-01,  4.5217e-01],\n",
       "          [-2.9655e-01, -2.8976e-01],\n",
       "          [ 6.5052e-01,  4.7893e-01],\n",
       "          [ 5.4333e-01,  3.8769e-01],\n",
       "          [ 2.6117e-01,  1.4802e-01],\n",
       "          [-4.2357e-02, -7.3023e-02],\n",
       "          [ 3.9926e-01,  2.7066e-01],\n",
       "          [-1.0590e-01, -1.2734e-01],\n",
       "          [ 7.1959e-01,  5.3826e-01]],\n",
       " \n",
       "         [[-1.9330e-01, -1.4597e-01],\n",
       "          [-1.0211e-01, -6.8923e-02],\n",
       "          [-2.4600e-01, -1.9076e-01],\n",
       "          [ 2.5544e-01,  2.5366e-01],\n",
       "          [-4.7849e-01, -3.8901e-01],\n",
       "          [ 2.8663e-01,  2.8042e-01],\n",
       "          [ 1.7945e-01,  1.8918e-01],\n",
       "          [-5.1360e-02, -2.5247e-02],\n",
       "          [-2.2430e-01, -1.7228e-01],\n",
       "          [ 3.5370e-02,  7.2145e-02],\n",
       "          [-2.8785e-01, -2.2659e-01],\n",
       "          [ 3.5570e-01,  3.3974e-01]],\n",
       " \n",
       "         [[-1.7747e-01, -1.3735e-01],\n",
       "          [-8.6276e-02, -6.0300e-02],\n",
       "          [-2.3017e-01, -1.8213e-01],\n",
       "          [ 2.8710e-01,  2.7090e-01],\n",
       "          [-4.6266e-01, -3.8039e-01],\n",
       "          [ 3.1830e-01,  2.9767e-01],\n",
       "          [ 2.1111e-01,  2.0643e-01],\n",
       "          [-3.5527e-02, -1.6624e-02],\n",
       "          [-2.0847e-01, -1.6366e-01],\n",
       "          [ 6.7034e-02,  8.9391e-02],\n",
       "          [-2.7201e-01, -2.1797e-01],\n",
       "          [ 3.8737e-01,  3.5699e-01]],\n",
       " \n",
       "         [[-1.3619e-01, -1.1490e-01],\n",
       "          [-4.5005e-02, -3.7845e-02],\n",
       "          [-1.8890e-01, -1.5968e-01],\n",
       "          [ 3.6965e-01,  3.1581e-01],\n",
       "          [-4.2139e-01, -3.5793e-01],\n",
       "          [ 4.0084e-01,  3.4258e-01],\n",
       "          [ 2.9365e-01,  2.5134e-01],\n",
       "          [ 1.1488e-02,  1.1663e-02],\n",
       "          [-1.6720e-01, -1.4120e-01],\n",
       "          [ 1.4958e-01,  1.3430e-01],\n",
       "          [-2.3074e-01, -1.9552e-01],\n",
       "          [ 4.6991e-01,  4.0190e-01]],\n",
       " \n",
       "         [[-8.5225e-02, -8.7169e-02],\n",
       "          [ 1.1929e-02, -1.0117e-02],\n",
       "          [-1.3793e-01, -1.3195e-01],\n",
       "          [ 4.7159e-01,  3.7127e-01],\n",
       "          [-3.7042e-01, -3.3021e-01],\n",
       "          [ 5.0278e-01,  3.9803e-01],\n",
       "          [ 3.9559e-01,  3.0679e-01],\n",
       "          [ 1.1343e-01,  6.7117e-02],\n",
       "          [-1.1623e-01, -1.1347e-01],\n",
       "          [ 2.5152e-01,  1.8976e-01],\n",
       "          [-1.7977e-01, -1.6779e-01],\n",
       "          [ 5.7185e-01,  4.5735e-01]],\n",
       " \n",
       "         [[-1.5630e-01, -1.2653e-01],\n",
       "          [-6.5113e-02, -4.9475e-02],\n",
       "          [-2.0900e-01, -1.7131e-01],\n",
       "          [ 3.2943e-01,  2.9255e-01],\n",
       "          [-4.4150e-01, -3.6956e-01],\n",
       "          [ 3.6062e-01,  3.1932e-01],\n",
       "          [ 2.5344e-01,  2.2808e-01],\n",
       "          [-1.4364e-02, -5.7988e-03],\n",
       "          [-1.8731e-01, -1.5283e-01],\n",
       "          [ 1.0936e-01,  1.1104e-01],\n",
       "          [-2.5085e-01, -2.0715e-01],\n",
       "          [ 4.2969e-01,  3.7864e-01]],\n",
       " \n",
       "         [[-6.7236e-02, -7.7234e-02],\n",
       "          [ 4.7907e-02, -1.8223e-04],\n",
       "          [-1.1994e-01, -1.2202e-01],\n",
       "          [ 5.0756e-01,  3.9114e-01],\n",
       "          [-3.5243e-01, -3.2027e-01],\n",
       "          [ 5.3876e-01,  4.1790e-01],\n",
       "          [ 4.3157e-01,  3.2666e-01],\n",
       "          [ 1.4940e-01,  8.6988e-02],\n",
       "          [-9.8239e-02, -1.0354e-01],\n",
       "          [ 2.8749e-01,  2.0963e-01],\n",
       "          [-1.6179e-01, -1.5785e-01],\n",
       "          [ 6.0783e-01,  4.7723e-01]],\n",
       " \n",
       "         [[-2.0349e-01, -1.5159e-01],\n",
       "          [-1.1230e-01, -7.4543e-02],\n",
       "          [-2.5619e-01, -1.9638e-01],\n",
       "          [ 2.3505e-01,  2.4242e-01],\n",
       "          [-4.8869e-01, -3.9463e-01],\n",
       "          [ 2.6625e-01,  2.6918e-01],\n",
       "          [ 1.5906e-01,  1.7794e-01],\n",
       "          [-6.1553e-02, -3.0867e-02],\n",
       "          [-2.3449e-01, -1.7790e-01],\n",
       "          [ 1.4982e-02,  6.0904e-02],\n",
       "          [-2.9804e-01, -2.3221e-01],\n",
       "          [ 3.3532e-01,  3.2850e-01]]], grad_fn=<LeakyReluBackward1>))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = gat2_attn_lrelu(out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619a2819-a9c2-4931-a1ef-4915bb21b853",
   "metadata": {},
   "source": [
    "##### Masking\n",
    "\n",
    "The same masks are applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "acb7c040-9571-4e21-ae4b-fe5ec329a84b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 1]),\n",
       " tensor([[[False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True]],\n",
       " \n",
       "         [[ True],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True]],\n",
       " \n",
       "         [[False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True]],\n",
       " \n",
       "         [[False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True]],\n",
       " \n",
       "         [[ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [ True],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [False],\n",
       "          [ True],\n",
       "          [False]]]))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signed_a = a_blk.clone()\n",
    "signed_a.fill_diagonal_(0.5)                       # self-loops are positive\n",
    "mask = (signed_a == 0)  \n",
    "mask = mask.unsqueeze(-1)\n",
    "mask.size(), mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9d260373-6e3d-49f1-b860-2aba362b2ff7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 2]),\n",
       " tensor([[[-0.0944, -0.0921],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [-0.3796, -0.3352],\n",
       "          [ 0.4844,  0.3881],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [-0.0308, -0.0299],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [-0.4072, -0.3500],\n",
       "          [ 0.4292,  0.3584],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [-0.1315, -0.1285],\n",
       "          [   -inf,    -inf],\n",
       "          [-0.3640, -0.3267],\n",
       "          [ 0.5156,  0.4050],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.2630,  0.2580],\n",
       "          [-0.4747, -0.3868],\n",
       "          [ 0.2942,  0.2848],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf]],\n",
       " \n",
       "         [[-0.0114, -0.0467],\n",
       "          [ 0.1597,  0.0607],\n",
       "          [-0.0641, -0.0915],\n",
       "          [ 0.6193,  0.4522],\n",
       "          [-0.2966, -0.2898],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf]],\n",
       " \n",
       "         [[-0.1933, -0.1460],\n",
       "          [-0.1021, -0.0689],\n",
       "          [-0.2460, -0.1908],\n",
       "          [ 0.2554,  0.2537],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.2866,  0.2804],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.2111,  0.2064],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [-0.2720, -0.2180],\n",
       "          [ 0.3874,  0.3570]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.0115,  0.0117],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [-0.2307, -0.1955],\n",
       "          [ 0.4699,  0.4019]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [-0.1162, -0.1135],\n",
       "          [   -inf,    -inf],\n",
       "          [-0.1798, -0.1678],\n",
       "          [ 0.5718,  0.4574]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.1094,  0.1110],\n",
       "          [-0.2509, -0.2071],\n",
       "          [ 0.4297,  0.3786]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.4316,  0.3267],\n",
       "          [ 0.1494,  0.0870],\n",
       "          [-0.0982, -0.1035],\n",
       "          [ 0.2875,  0.2096],\n",
       "          [-0.1618, -0.1579],\n",
       "          [   -inf,    -inf]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.1591,  0.1779],\n",
       "          [-0.0616, -0.0309],\n",
       "          [-0.2345, -0.1779],\n",
       "          [ 0.0150,  0.0609],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.3353,  0.3285]]], grad_fn=<MaskedFillBackward0>))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out.masked_fill(mask, float('-inf'))\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ac0657-451b-4889-bdf3-ce5a60233281",
   "metadata": {},
   "source": [
    "##### SoftMax\n",
    "\n",
    "With softmax we can see that for our edge attention we see both heads have a similar result, even if there were different sign representations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e9adc79a-e531-46f8-92eb-073de8427bd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 2]),\n",
       " tensor([[[0.2828, 0.2941],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2126, 0.2306],\n",
       "          [0.5045, 0.4753],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.3058, 0.3124],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2099, 0.2269],\n",
       "          [0.4844, 0.4607],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2701, 0.2837],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2141, 0.2327],\n",
       "          [0.5159, 0.4836],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3984, 0.3919],\n",
       "          [0.1905, 0.2056],\n",
       "          [0.4110, 0.4025],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.1734, 0.1818],\n",
       "          [0.2058, 0.2024],\n",
       "          [0.1645, 0.1738],\n",
       "          [0.3259, 0.2994],\n",
       "          [0.1304, 0.1426],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.1606, 0.1650],\n",
       "          [0.1759, 0.1783],\n",
       "          [0.1524, 0.1578],\n",
       "          [0.2516, 0.2461],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2595, 0.2528],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3559, 0.3550],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2196, 0.2322],\n",
       "          [0.4245, 0.4127]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2970, 0.3039],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2331, 0.2471],\n",
       "          [0.4698, 0.4490]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2546, 0.2690],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2389, 0.2548],\n",
       "          [0.5065, 0.4761]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3252, 0.3296],\n",
       "          [0.2268, 0.2397],\n",
       "          [0.4480, 0.4307]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2659, 0.2536],\n",
       "          [0.2005, 0.1996],\n",
       "          [0.1565, 0.1650],\n",
       "          [0.2302, 0.2256],\n",
       "          [0.1469, 0.1562],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2205, 0.2191],\n",
       "          [0.1768, 0.1778],\n",
       "          [0.1488, 0.1535],\n",
       "          [0.1909, 0.1949],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2630, 0.2547]]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = torch.softmax(out, dim=1)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f38401e-7ea9-449d-81a6-4fb5d8872e8a",
   "metadata": {},
   "source": [
    "##### Attention Dropout\n",
    "\n",
    "Again dropout is not learnable so we get the same behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e0960046-0949-4a80-8224-9c13612e4613",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 2]),\n",
       " tensor([[[0.2977, 0.3095],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2238, 0.2428],\n",
       "          [0.5311, 0.5003],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.3219, 0.3289],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2209, 0.2388],\n",
       "          [0.5099, 0.4850],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2843, 0.2986],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2253, 0.2449],\n",
       "          [0.5430, 0.5091],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.4194, 0.4125],\n",
       "          [0.2006, 0.2165],\n",
       "          [0.0000, 0.4237],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.1826, 0.1914],\n",
       "          [0.2166, 0.2131],\n",
       "          [0.1732, 0.1830],\n",
       "          [0.3430, 0.3152],\n",
       "          [0.1373, 0.1501],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.1691, 0.1737],\n",
       "          [0.1852, 0.1876],\n",
       "          [0.1604, 0.1661],\n",
       "          [0.2648, 0.2591],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2732, 0.2661],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3747, 0.3737],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2311, 0.2445],\n",
       "          [0.4469, 0.4344]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3127, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2454, 0.2601],\n",
       "          [0.4945, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2680, 0.2832],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2515, 0.2682],\n",
       "          [0.5332, 0.5012]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3423, 0.3469],\n",
       "          [0.2388, 0.2524],\n",
       "          [0.4716, 0.4534]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2799, 0.2670],\n",
       "          [0.2111, 0.2101],\n",
       "          [0.1648, 0.1736],\n",
       "          [0.2423, 0.2375],\n",
       "          [0.1546, 0.1645],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2321, 0.2306],\n",
       "          [0.1862, 0.1872],\n",
       "          [0.1566, 0.1616],\n",
       "          [0.2010, 0.2052],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2768, 0.2681]]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = gat2_attn_drop(out) \n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a1e41-5242-4df5-a6fd-f94cbe3c1220",
   "metadata": {},
   "source": [
    "##### Sign Aware Attention\n",
    "\n",
    "We again apply sign awareness to the direction scaled attention.  We can now see that most values are in the similar ranges without much difference between positives and negatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3db06a43-a6f2-45fd-a72b-34ca3644b07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 12, 12]),\n",
       " tensor([[[ 0.1489,  0.0000,  0.0000,  0.0000,  0.2238, -0.5311,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.1609,  0.0000,  0.0000,  0.2209,  0.5099,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.1422,  0.0000,  0.2253, -0.5430,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.2097, -0.2006,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.1826,  0.2166,  0.1732, -0.3430,  0.0686,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.1691,  0.1852, -0.1604,  0.2648,  0.0000,  0.1366,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1873,\n",
       "            0.0000,  0.0000,  0.0000, -0.2311,  0.4469],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.1563,  0.0000,  0.0000, -0.2454, -0.4945],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.1340,  0.0000,  0.2515, -0.5332],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.1712, -0.2388,  0.4716],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.2799,\n",
       "           -0.2111,  0.1648, -0.2423,  0.0773,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.2321,\n",
       "           -0.1862, -0.1566,  0.2010,  0.0000,  0.1384]],\n",
       " \n",
       "         [[ 0.1548,  0.0000,  0.0000,  0.0000,  0.2428, -0.5003,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.1644,  0.0000,  0.0000,  0.2388,  0.4850,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.1493,  0.0000,  0.2449, -0.5091,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.2062, -0.2165,  0.4237,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.1914,  0.2131,  0.1830, -0.3152,  0.0750,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.1737,  0.1876, -0.1661,  0.2591,  0.0000,  0.1330,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1869,\n",
       "            0.0000,  0.0000,  0.0000, -0.2445,  0.4344],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000, -0.2601, -0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.1416,  0.0000,  0.2682, -0.5012],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.1735, -0.2524,  0.4534],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.2670,\n",
       "           -0.2101,  0.1736, -0.2375,  0.0822,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.2306,\n",
       "           -0.1872, -0.1616,  0.2052,  0.0000,  0.1341]]],\n",
       "        grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out.permute(2, 0, 1) # [H,M,M]\n",
    "s_heads = signed_a.squeeze(-1).unsqueeze(0).expand(heads, -1, -1) # [H,M,M]\n",
    "out = out * s_heads # [H,M,M]\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a5d4bc-a9b4-4629-8bd4-63a3aece161b",
   "metadata": {},
   "source": [
    "##### Message Aggregation\n",
    "\n",
    "As we pull in the nodes we can see the semblance of a pattern is again lost with signs and values changing by head, channel, and edge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "fb149e16-6bdc-4034-88bf-d8550798178d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 12, 3]),\n",
       " tensor([[[-0.0228,  0.1442,  0.3111],\n",
       "          [-0.0015, -0.0031, -0.0048],\n",
       "          [-0.0237,  0.1505,  0.3246],\n",
       "          [ 0.0118, -0.0805, -0.1728],\n",
       "          [-0.0147,  0.1039,  0.2226],\n",
       "          [ 0.0089, -0.0804, -0.1697],\n",
       "          [ 0.0194, -0.1216, -0.2625],\n",
       "          [-0.0049,  0.0395,  0.0838],\n",
       "          [-0.0209,  0.1342,  0.2894],\n",
       "          [ 0.0187, -0.1169, -0.2526],\n",
       "          [-0.0111,  0.0696,  0.1504],\n",
       "          [ 0.0108, -0.0699, -0.1507]],\n",
       " \n",
       "         [[-0.0074,  0.1614,  0.3303],\n",
       "          [-0.0023,  0.0056,  0.0136],\n",
       "          [-0.0076,  0.1686,  0.3447],\n",
       "          [ 0.0058, -0.1557, -0.3171],\n",
       "          [-0.0037,  0.1152,  0.2342],\n",
       "          [ 0.0004, -0.0888, -0.1779],\n",
       "          [ 0.0062, -0.1348, -0.2758],\n",
       "          [ 0.0028, -0.0425, -0.0878],\n",
       "          [-0.0064,  0.1470,  0.3003],\n",
       "          [ 0.0054, -0.1295, -0.2644],\n",
       "          [-0.0028,  0.0782,  0.1591],\n",
       "          [ 0.0026, -0.0780, -0.1587]]], grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out @ Hproj # [H,M,O]\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966c0d48-8056-4ba5-96ac-12d855563949",
   "metadata": {},
   "source": [
    "##### Concatenate Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "88613b15-1b13-4ed5-bf5c-c671cf7f61e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 6]),\n",
       " tensor([[-0.0228,  0.1442,  0.3111, -0.0074,  0.1614,  0.3303],\n",
       "         [-0.0015, -0.0031, -0.0048, -0.0023,  0.0056,  0.0136],\n",
       "         [-0.0237,  0.1505,  0.3246, -0.0076,  0.1686,  0.3447],\n",
       "         [ 0.0118, -0.0805, -0.1728,  0.0058, -0.1557, -0.3171],\n",
       "         [-0.0147,  0.1039,  0.2226, -0.0037,  0.1152,  0.2342],\n",
       "         [ 0.0089, -0.0804, -0.1697,  0.0004, -0.0888, -0.1779],\n",
       "         [ 0.0194, -0.1216, -0.2625,  0.0062, -0.1348, -0.2758],\n",
       "         [-0.0049,  0.0395,  0.0838,  0.0028, -0.0425, -0.0878],\n",
       "         [-0.0209,  0.1342,  0.2894, -0.0064,  0.1470,  0.3003],\n",
       "         [ 0.0187, -0.1169, -0.2526,  0.0054, -0.1295, -0.2644],\n",
       "         [-0.0111,  0.0696,  0.1504, -0.0028,  0.0782,  0.1591],\n",
       "         [ 0.0108, -0.0699, -0.1507,  0.0026, -0.0780, -0.1587]],\n",
       "        grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out.permute(1, 0, 2) # [M,H,O]\n",
    "out = out.reshape(B_batch*N_nodes, heads * head_dim)\n",
    "out.size(), out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c58948-abb2-4aa4-8956-3115a039295d",
   "metadata": {},
   "source": [
    "##### Add Bias\n",
    "\n",
    "Our bias in this case is the same across the board so we see all the channels increment by the same amount. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e09175bd-5d6e-4430-8b70-7226daa6895b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 6]),\n",
       " tensor([[-2.3154e-02,  1.4381e-01,  3.1078e-01, -7.7790e-03,  1.6109e-01,\n",
       "           3.2996e-01],\n",
       "         [-1.8071e-03, -3.4796e-03, -5.1525e-03, -2.6560e-03,  5.2831e-03,\n",
       "           1.3222e-02],\n",
       "         [-2.4018e-02,  1.5011e-01,  3.2426e-01, -7.9700e-03,  1.6821e-01,\n",
       "           3.4439e-01],\n",
       "         [ 1.1418e-02, -8.0883e-02, -1.7319e-01,  5.4556e-03, -1.5601e-01,\n",
       "          -3.1747e-01],\n",
       "         [-1.5048e-02,  1.0358e-01,  2.2221e-01, -4.0977e-03,  1.1488e-01,\n",
       "           2.3385e-01],\n",
       "         [ 8.5503e-03, -8.0744e-02, -1.7004e-01, -5.4898e-06, -8.9147e-02,\n",
       "          -1.7829e-01],\n",
       "         [ 1.9019e-02, -1.2194e-01, -2.6291e-01,  5.8290e-03, -1.3519e-01,\n",
       "          -2.7620e-01],\n",
       "         [-5.2251e-03,  3.9119e-02,  8.3467e-02,  2.4295e-03, -4.2882e-02,\n",
       "          -8.8194e-02],\n",
       "         [-2.1297e-02,  1.3388e-01,  2.8907e-01, -6.7291e-03,  1.4660e-01,\n",
       "           2.9993e-01],\n",
       "         [ 1.8346e-02, -1.1728e-01, -2.5291e-01,  5.0241e-03, -1.2986e-01,\n",
       "          -2.6474e-01],\n",
       "         [-1.1481e-02,  6.9279e-02,  1.5004e-01, -3.1428e-03,  7.7815e-02,\n",
       "           1.5877e-01],\n",
       "         [ 1.0437e-02, -7.0284e-02, -1.5101e-01,  2.2292e-03, -7.8397e-02,\n",
       "          -1.5902e-01]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out + gat2_bias\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1930010-513b-4e53-a7cb-110998b890fd",
   "metadata": {},
   "source": [
    "### GAT Block - Residual Connection\n",
    "\n",
    "As we add in the residual block, we can see some shifting around and the value of passing the embedding projection around the attention block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8b9a4c96-286e-4096-b2b1-b56f99a40a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 6]),\n",
       " tensor([[-0.0374,  0.1305,  0.2985, -0.0191,  0.1508,  0.3207],\n",
       "         [-0.0355, -0.0716, -0.0723, -0.0768, -0.0598, -0.0509],\n",
       "         [-0.1671, -0.0526,  0.1226, -0.2087, -0.0315,  0.1457],\n",
       "         [ 0.2184,  0.1271,  0.0358,  0.1485,  0.0550, -0.1055],\n",
       "         [-0.1200, -0.0004,  0.1192, -0.0601,  0.0139,  0.1339],\n",
       "         [ 0.0669, -0.0214, -0.1097,  0.0481, -0.0268, -0.1540],\n",
       "         [ 0.0047, -0.1352, -0.2752, -0.0055, -0.1455, -0.2855],\n",
       "         [-0.0390, -0.0290,  0.0163, -0.0717, -0.1080, -0.1523],\n",
       "         [-0.1644, -0.0688,  0.0874, -0.2074, -0.0531,  0.1012],\n",
       "         [ 0.2254,  0.0907, -0.0439,  0.1481,  0.0812, -0.0527],\n",
       "         [-0.1165, -0.0347,  0.0471, -0.0592, -0.0232,  0.0588],\n",
       "         [ 0.0688, -0.0109, -0.0906,  0.0504, -0.0160, -0.1348]],\n",
       "        grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out + x\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052c1e8d-6c09-4c3f-98af-224ddbf64a7c",
   "metadata": {},
   "source": [
    "### GAT Block - Post Residual ELU\n",
    "\n",
    "Since our negatives are now larger we can see that the ELU layer makes them less negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0aa012e5-fc48-4e93-a842-5b117db939a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 6]),\n",
       " tensor([[-0.0368,  0.1305,  0.2985, -0.0189,  0.1508,  0.3207],\n",
       "         [-0.0349, -0.0691, -0.0697, -0.0739, -0.0581, -0.0496],\n",
       "         [-0.1539, -0.0512,  0.1226, -0.1883, -0.0310,  0.1457],\n",
       "         [ 0.2184,  0.1271,  0.0358,  0.1485,  0.0550, -0.1001],\n",
       "         [-0.1131, -0.0004,  0.1192, -0.0584,  0.0139,  0.1339],\n",
       "         [ 0.0669, -0.0211, -0.1039,  0.0481, -0.0264, -0.1428],\n",
       "         [ 0.0047, -0.1265, -0.2406, -0.0055, -0.1354, -0.2484],\n",
       "         [-0.0382, -0.0286,  0.0163, -0.0692, -0.1024, -0.1413],\n",
       "         [-0.1516, -0.0665,  0.0874, -0.1873, -0.0517,  0.1012],\n",
       "         [ 0.2254,  0.0907, -0.0429,  0.1481,  0.0812, -0.0514],\n",
       "         [-0.1099, -0.0341,  0.0471, -0.0575, -0.0229,  0.0588],\n",
       "         [ 0.0688, -0.0108, -0.0866,  0.0504, -0.0159, -0.1261]],\n",
       "        grad_fn=<EluBackward0>))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elu2 = nn.ELU()\n",
    "out = elu2(out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af23fc98-29eb-41cc-9b32-44ea571bfcc3",
   "metadata": {},
   "source": [
    "### GAT Block - Reinsert The Batch Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "40bcf220-a03e-4d1c-aa06-c0d0b6d110b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 6]),\n",
       " tensor([[[-0.0368,  0.1305,  0.2985, -0.0189,  0.1508,  0.3207],\n",
       "          [-0.0349, -0.0691, -0.0697, -0.0739, -0.0581, -0.0496],\n",
       "          [-0.1539, -0.0512,  0.1226, -0.1883, -0.0310,  0.1457],\n",
       "          [ 0.2184,  0.1271,  0.0358,  0.1485,  0.0550, -0.1001],\n",
       "          [-0.1131, -0.0004,  0.1192, -0.0584,  0.0139,  0.1339],\n",
       "          [ 0.0669, -0.0211, -0.1039,  0.0481, -0.0264, -0.1428]],\n",
       " \n",
       "         [[ 0.0047, -0.1265, -0.2406, -0.0055, -0.1354, -0.2484],\n",
       "          [-0.0382, -0.0286,  0.0163, -0.0692, -0.1024, -0.1413],\n",
       "          [-0.1516, -0.0665,  0.0874, -0.1873, -0.0517,  0.1012],\n",
       "          [ 0.2254,  0.0907, -0.0429,  0.1481,  0.0812, -0.0514],\n",
       "          [-0.1099, -0.0341,  0.0471, -0.0575, -0.0229,  0.0588],\n",
       "          [ 0.0688, -0.0108, -0.0866,  0.0504, -0.0159, -0.1261]]],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out.view(B_batch, N_nodes,n_embd)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ebca7-8ec0-438a-b7d9-20549f679626",
   "metadata": {},
   "source": [
    "### Output Layers AKA Model Heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5335861d-9bd2-4a32-b6e8-5e2ff68a8563",
   "metadata": {},
   "source": [
    "#### Output Layer - Node Head\n",
    "\n",
    "With our node head we can now see that we no longer have the same scaling for each node in the batch but we still see an overall prevelence for the first class 'T'.  Since this is the majority it's not bad but it's still not learning as much as we'd want. While this shows that the model is starting to move away from random and learn how to better categorize cell types based on the gene's expression patterns we'd need more cycles to fully learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "23272922-d813-4264-a843-2e95c3f75384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 2]),\n",
       " tensor([[[ 0.1601, -0.1186],\n",
       "          [ 0.0436, -0.0581],\n",
       "          [ 0.0798, -0.0798],\n",
       "          [ 0.0892, -0.0783],\n",
       "          [ 0.0944, -0.0851],\n",
       "          [ 0.0445, -0.0562]],\n",
       " \n",
       "         [[-0.0031, -0.0322],\n",
       "          [ 0.0377, -0.0564],\n",
       "          [ 0.0681, -0.0737],\n",
       "          [ 0.0890, -0.0771],\n",
       "          [ 0.0728, -0.0737],\n",
       "          [ 0.0500, -0.0591]]], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_logits = node_head(out)\n",
    "node_logits.size(), node_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead55033-7a73-4d94-a536-308ae76bbff1",
   "metadata": {},
   "source": [
    "#### Output Layer - Graph Head\n",
    "\n",
    "Similarly we can see that the graph head is moving its values away from the initialized weights and it aligns with the classes we expect `[0,1]`.  This is great and shows that we're learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a579d06-cf6d-4e65-a936-ee3751fc9f30",
   "metadata": {},
   "source": [
    "**Graph Head - Average Pooling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6337fed5-216e-4002-bd5e-e830a5cb85d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6]),\n",
       " tensor([[-0.0018,  0.0343,  0.0968, -0.0331,  0.0292,  0.0792],\n",
       "         [ 0.0101, -0.0327, -0.0450, -0.0285, -0.0521, -0.0849]],\n",
       "        grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = gene_mask.float().unsqueeze(0)\n",
    "denom = mask.sum(1, keepdim=True).clamp_min(1.0)\n",
    "pooled = (out * mask.unsqueeze(-1)).sum(1) / denom\n",
    "pooled.size(), pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8c9159-e407-4e8c-b324-30cdf3b5b01d",
   "metadata": {},
   "source": [
    "**Graph Head - Output Projection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "03fd636e-f989-438f-b0db-bf9c7c9a2c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2]),\n",
       " tensor([[ 0.0259, -0.0165],\n",
       "         [-0.0246,  0.0127]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_logits = graph_head(pooled)  # [B,2]\n",
    "graph_logits.size(), graph_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c146ce-eb89-4871-8a88-c2ce7bca7f31",
   "metadata": {},
   "source": [
    "### Updated Loss calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922ab015-7ebb-4675-80b6-e3646bce9b35",
   "metadata": {},
   "source": [
    "Now we'll calculate the updated loss.  Our first pass's total loss was 0.69315 for each of the heads and a total of 1.3863, on par with random. Since we're passing through the same example and used a fairly high learning rate we should see a significant improvement with just 1 learning pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "130be022-76de-4392-8a1b-c8d8e0a9ad65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor(0.6932, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6932, grad_fn=<NllLossBackward0>)],\n",
       " tensor(1.3863, grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8b13ef-36d2-4f9c-982b-7b2d856623ff",
   "metadata": {},
   "source": [
    "#### Node Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "048d0e47-35b7-401b-a3dd-676f474d4488",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "valid = (gene_mask & (y_node>=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "53069ebb-a3b3-4848-be45-6ad083348c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6737, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nl = F.cross_entropy(node_logits[valid], y_node[valid])\n",
    "losses.append(nl)\n",
    "nl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8381f6ad-00c6-4e5f-a141-220fdcbd8222",
   "metadata": {},
   "source": [
    "#### Graph Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "209d0586-a0fb-4bdf-8b55-074a6b22b29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6734, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gl = F.cross_entropy(graph_logits, y_graph)\n",
    "losses.append(gl)\n",
    "gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4d9732dd-1344-4fe0-80bb-d425b8b64c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3472, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_loss = sum(losses) \n",
    "updated_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1c44979d-4d23-4481-9133-069003bb97ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 round of training resulted in an loss improvment of 0.0392'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'1 round of training resulted in an loss improvment of {loss.item() - updated_loss.item():.4f}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7ecbd2-60f3-43c3-86c3-f5c66b1f5215",
   "metadata": {},
   "source": [
    "## Training SUCCESS!\n",
    "Our training improved the loss by about **~3%** overall (amount may vary since we didn't set a seed). Interestingly we did see that the loss on both heads improved. If we wanted to prioritize a specific head we could change the alpha in our loss sum. There are flaws with this, mainly passing the same example through a second time and a crazy high learning rate, but this helps show the fundamentals of what learning does inside a GAT style GNN model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c840484e-2632-40cc-86f3-a00c030b7e01",
   "metadata": {},
   "source": [
    "## Logit to Token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc3cb90-286a-4e0d-8e1f-310f7cae25d1",
   "metadata": {},
   "source": [
    "For our last piece of code on this notebook, we'll actually now convert our logits for each head into actual class predictions.  For class prediction we apply argmax to the logits. Argmax returns the input index at which an array attains its maximum: \n",
    "$$\n",
    "\\operatorname*{argmax}_x f(x)={x\\mid f(x)\\ge f(y)\\ \\forall y}\n",
    "$$\n",
    "Since we're trying to find the class, argmax will return the index at which the class is maximized. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d98f7-f292-4bda-9f38-aa9c27df4e2b",
   "metadata": {},
   "source": [
    "### Graph class prediction \n",
    "As a reminder our graph head predicts whether a graph is cancerous or not.  We can see here that it overly defaulted to true which makes sense given the input data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6598c4a2-f2d7-49bb-b150-4ec9bdd8e252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph preds: [0, 1]  True: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Graph preds:\", graph_logits.argmax(-1).tolist(), \" True:\", y_graph.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e801bab-e011-4510-9de5-2b27da8ed668",
   "metadata": {},
   "source": [
    "### Node class prediction \n",
    "As a reminder our node head predicts the cell type for a node in every single example.  It looks at the gene type and, with the assumption it's upregulated, predicts the cell type it's most commonly upregulated in. We could aggregate across different dimensions using argmax again to get a graph level or gene level predictor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "3cc05b5f-a11d-4a7b-99f9-26096d34b720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-node preds (genes only):\n",
      "sample0: CD3D: pred=T true=T | LCK: pred=T true=T | ZAP70: pred=T true=T | CD19: pred=T true=B X\n",
      "sample1: CD3D: pred=T true=B X | ZAP70: pred=T true=T | CD19: pred=T true=B X\n"
     ]
    }
   ],
   "source": [
    "print(\"Per-node preds (genes only):\")\n",
    "label = {0: \"T\", 1: \"B\"}\n",
    "pred = node_logits.argmax(-1)\n",
    "ytrue = y_node\n",
    "valid = (ytrue >= 0)\n",
    "names = node_order\n",
    "\n",
    "\n",
    "for b in range(pred.size(0)):\n",
    "    idxs = torch.nonzero(valid[b], as_tuple=False).squeeze(1).tolist()\n",
    "    parts = []\n",
    "    for i in idxs:\n",
    "        p = pred[b, i].item()\n",
    "        t = ytrue[b, i].item()\n",
    "        cor = '' if p ==  t else ' X'\n",
    "        parts.append(f'{names[i]}: pred={label[p]} true={label[t]}{cor}')\n",
    "    print(f'sample{b}: ' + ' | '.join(parts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
