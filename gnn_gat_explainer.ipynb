{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a55a7864-bcb0-41e6-902b-8d162fd9bd62",
   "metadata": {},
   "source": [
    "# GNN GAT Explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eca57eb-1a17-4ce1-887d-6d08b36240bd",
   "metadata": {},
   "source": [
    "The goal of this notebook is to walk through Graph Attention Network (GATs) flavor of a graph neural network (GNN). GATs operate on graphs where nodes are entities  and edges encode relations. Like other attention based networks (e.g. GPT), GATS use attention layers to learn the importance of different relationships A standard GAT layer builds 3 attention layers: 1 for for the features, and 2 for each direction of connection for a node (out, back). The standard GAT typically does not use the graph's adjacency matrix (node neighbor map) and rather uses attention + masking to learn it. There are variants that take add in attention based on the adjacency to make it easier to learn.  A GAT stacks attention layers with ELU normalizations, requiring as many layers as hops that the model is trying to learn. Because GATs learn their networks they are preferable when neighbor quality is uneven or noisy and you want the model to learn which neighbors to trust instead of averaging them uniformly, like a GCn. They’re also better when relationships are asymmetric or context-dependent, since attention can weight edges differently in each direction and based on node features. Also, GATs offer more interpretable edge-level importance scores, which is valuable in domains like biology where explaining which genes or interactions drove a prediction matters. In this notebook we'll show both a traditional GAT layer and a signed GAT layer. to show how they adapt differently \n",
    "\n",
    "To help display how GATs work, we'll use a representation of a gene regulatory network where we have 2 specific cell types, a set of genes that are up/down regulated, and then a label for each sample to flag if it's cancerous or not. With our GAT, the goal will be to train it to take in a set of genes and how they're regulated and then predict the cell type as the *node task*, or node level prediction and whether it's cancerous as the *graph task* or graph level prediction. This dual goal will require us to balance 2 loss functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e757d71e-e53e-40ea-85db-6c8229b1c79d",
   "metadata": {},
   "source": [
    "## Graph Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a39342f-3d6f-47ba-bbd2-8140bce5e506",
   "metadata": {},
   "source": [
    "We'll start with a common preprocessing step. Instead of a typical tokenizer, we have to create a numerical representation of our graph by enumerating the nodes and edges. This process first starts by creating a series of \"token-like\" IDS for our node vocabulary. In our example that becomes the gene and cell types.  We then use integers to map the relationship of the gene and cell types. This is where our **signed graph** starts since we use `+1` edges for up-regulation and `-1` for down-regulation.   As a result we end up with the following generated:\n",
    "1. **x_tokens** - a list of nodes\n",
    "2. **y_node** - per-node (gene) gene cell type labels. This uses a balance of the up-regulated and down-regulated genes and flags for each sample if the gene is more common with B-cells or T-cells.  We use a `-1` here to flag cells to ignore\n",
    "3. **y_graph** - per-graph (sample) cancer type label. We use `1` for cancerous and `0` for benign\n",
    "4. **a_list** - the nodes in our graph that link the different cell types and genes together.  We pool all the samples together into a large block-diagonal matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "736484bd-3859-4b69-9a1f-36c9eddce277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcbcbe30-349a-48d4-9053-2ab972162cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "benign = {\n",
    "    'Tcells':{'up': ['CD3D','LCK','ZAP70'],'down': ['CD19']},\n",
    "    'Bcells':{'up': ['CD19','LCK'],'down': ['CD3D','ZAP70']},\n",
    "  #  'Macrophages':{'up': ['CSF1R'],'down': ['CD3D','MS4A1','CD19']},\n",
    "}\n",
    "\n",
    "cancerous = {\n",
    "     'Tcells':{'up': ['ZAP70'],'down': ['CD19','CD3D','LCK']},\n",
    "     'Bcells':{'up': ['CD19','CD3D'],'down': ['ZAP70','LCK']},\n",
    "   # 'Macrophages':{'up': ['CSF1R'],'down': ['CD3D','MS4A1','CD19']},\n",
    "}\n",
    "\n",
    "# cancerous_2 = {\n",
    "#    'Tcells':{'up': ['CD3D','ZAP70'],'down': ['CD19','LCK']},\n",
    "#    'Bcells':{'up': ['CD19','LCK','CD3D'],'down': ['ZAP70']},\n",
    "#    # 'Macrophages':{'up': ['CSF1R'],'down': ['CD3D','MS4A1','CD19']},\n",
    "# }\n",
    "\n",
    "# cancerous_3 = {\n",
    "#     'Tcells':{'up': ['LCK'],'down': ['ZAP70','CD19','CD3D']},\n",
    "#     'Bcells':{'up': ['CD19','ZAP70'],'down': ['LCK','CD3D']},\n",
    "#    # 'Macrophages':{'up': ['CSF1R'],'down': ['CD3D','MS4A1','CD19']},\n",
    "# }\n",
    "graphs = [benign, cancerous] #, cancerous_2, cancerous_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5b348e6-4bde-4391-849a-cd787d83b397",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_types = ['Tcells','Bcells']\n",
    "genes = ['CD3D','LCK','ZAP70','CD19'] #only focused on genes present\n",
    "node_order = genes + [f'CT_{ct}' for ct in cell_types]   # genes first, then CT nodes\n",
    "gene_mask = torch.tensor([1,1,1,1,0,0], dtype=torch.bool)\n",
    "N = len(node_order) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cfd44a1-953e-4a90-8a1d-56bd122b068e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,\n",
       " {'CD3D': 0, 'LCK': 1, 'ZAP70': 2, 'CD19': 3, 'CT_Tcells': 4, 'CT_Bcells': 5})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_vocab(genes, cell_types):\n",
    "    toks = genes + [f'CT_{ct}' for ct in cell_types]\n",
    "    stoi = {t:i for i,t in enumerate(toks)}; itos = {i:t for t,i in stoi.items()}\n",
    "    return stoi, itos\n",
    "\n",
    "stoi, itos = build_vocab(genes, cell_types)\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "vocab_size, stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10be2b0f-4aca-4219-971a-7d9876007957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph_signed(spec):\n",
    "    N = len(node_order); \n",
    "    A_signed = torch.zeros(N, N)\n",
    "    def idx(n): \n",
    "        return node_order.index(n)\n",
    "    def add_edge(g, ct, s):\n",
    "        i, j = idx(g), idx(f\"CT_{ct}\")\n",
    "        A_signed[i,j] = s\n",
    "        A_signed[j,i] = s\n",
    "    for ct in cell_types:\n",
    "        for g in spec[ct].get('up', []):   \n",
    "            add_edge(g, ct, +1)\n",
    "        for g in spec[ct].get('down', []): \n",
    "            add_edge(g, ct, -1)\n",
    "    # labels for node task (0=T, 1=B) on genes only\n",
    "    y_node = torch.full((N,), -1, dtype=torch.long)\n",
    "    X = torch.tensor([stoi[n] for n in node_order], dtype=torch.long)\n",
    "    \n",
    "    # per-gene cell-type label: 0=Tcells, 1=Bcells (prefer UP, tie-break T→B, fallback from DOWN)\n",
    "    y_node = torch.full((N,), -1, dtype=torch.long)\n",
    "    for g in genes:\n",
    "        t_up = g in spec['Tcells'].get('up', [])\n",
    "        b_up = g in spec['Bcells'].get('up', [])\n",
    "        t_dn = g in spec['Tcells'].get('down', [])\n",
    "        b_dn = g in spec['Bcells'].get('down', [])\n",
    "        lab = None\n",
    "        if t_up and not b_up: \n",
    "            lab = 0\n",
    "        elif b_up and not t_up: \n",
    "            lab = 1\n",
    "        elif t_up and b_up: \n",
    "            lab = 0\n",
    "        else:\n",
    "            if t_dn and not b_dn: \n",
    "                lab = 1\n",
    "            elif b_dn and not t_dn: \n",
    "                lab = 0\n",
    "        if lab is not None: \n",
    "            y_node[idx(g)] = lab\n",
    "    return X, y_node, A_signed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59713212-5422-4580-8a4f-4aa49ed067a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([0, 1, 2, 3, 4, 5]), tensor([0, 1, 2, 3, 4, 5])],\n",
       " [tensor([ 0,  0,  0,  1, -1, -1]), tensor([ 1, -1,  0,  1, -1, -1])],\n",
       " [tensor([[ 0.,  0.,  0.,  0.,  1., -1.],\n",
       "          [ 0.,  0.,  0.,  0.,  1.,  1.],\n",
       "          [ 0.,  0.,  0.,  0.,  1., -1.],\n",
       "          [ 0.,  0.,  0.,  0., -1.,  1.],\n",
       "          [ 1.,  1.,  1., -1.,  0.,  0.],\n",
       "          [-1.,  1., -1.,  1.,  0.,  0.]]),\n",
       "  tensor([[ 0.,  0.,  0.,  0., -1.,  1.],\n",
       "          [ 0.,  0.,  0.,  0., -1., -1.],\n",
       "          [ 0.,  0.,  0.,  0.,  1., -1.],\n",
       "          [ 0.,  0.,  0.,  0., -1.,  1.],\n",
       "          [-1., -1.,  1., -1.,  0.,  0.],\n",
       "          [ 1., -1., -1.,  1.,  0.,  0.]])])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_list, y_node_list, a_list = [], [], []\n",
    "for spec in graphs:\n",
    "    X_i, y_i, A_i = make_graph_signed(spec)\n",
    "    x_list.append(X_i); y_node_list.append(y_i); a_list.append(A_i)\n",
    "\n",
    "x_list, y_node_list, a_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a32ecc-62b1-49dc-b086-f3446193cbdc",
   "metadata": {},
   "source": [
    "**x_tokens** - a list of nodes\n",
    "\n",
    "Notice that all nodes are present in each of our samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ff129f5-5109-4537-a284-fca7becbc618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6]),\n",
       " tensor([[0, 1, 2, 3, 4, 5],\n",
       "         [0, 1, 2, 3, 4, 5]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tokens = torch.stack(x_list)\n",
    "x_tokens.size(), x_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e78cf94-1c19-46a1-b2d5-263a103a06af",
   "metadata": {},
   "source": [
    "**y_node** - per-node (gene) gene cell type labels. This uses a balance of the up-regulated and down-regulated genes and flags for each sample. If the gene is more commonly upregulated with B-cells we flag it as `0` and if it's more commonly up-regulated in T-cells we flag it as `1`.  This evaluation is done per sample (seen here as per row). \n",
    "\n",
    "The last two columns are embeddings purely for cell type so we flag them as -1 to be masked during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6275918-276e-45ba-9532-b8f0c1f5e599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6]),\n",
       " tensor([[ 0,  0,  0,  1, -1, -1],\n",
       "         [ 1, -1,  0,  1, -1, -1]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_node = torch.stack(y_node_list)\n",
    "y_node.size(), y_node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6a8ff8-e05c-43d7-aad5-6d089e7b421a",
   "metadata": {},
   "source": [
    "**y_graph** - per-graph (sample) cancer type label. We use `1` for cancerous and `0` for benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67fe0752-f9b0-4dad-8a5f-e3e4fab3082c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2]), tensor([0, 1]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_graph = torch.tensor([0,1])  # 0=wild, 1=cancer\n",
    "y_graph.size(), y_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30046b6d-4bae-4820-9c77-537e861d37af",
   "metadata": {},
   "source": [
    "**Graph** \n",
    "\n",
    "Since this is a GNN explainer, let's visualize actually how these graphs look. This is in essence the graph the model is looking at and learning how to read so that if it sees a new one, it can predict the properties we are minimizing loss on (cancerous, cell type). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b08438b-16e2-4c3a-b509-f7b29aedbb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_positions(genes, cell_types):\n",
    "    # left column (genes), right column (cell types)\n",
    "    gx, cx = 0.1, 0.9\n",
    "    gy = list(range(len(genes)-1, -1, -1))  # top-to-bottom\n",
    "    if len(cell_types) > 1:\n",
    "        # spread CTs across same vertical span as genes\n",
    "        span = (len(genes)-1)\n",
    "        cy = [span * (1 - i/(len(cell_types)-1)) for i in range(len(cell_types))]\n",
    "    else:\n",
    "        cy = [(len(genes)-1)/2]\n",
    "    pos = {g:(gx, gy[i]) for i,g in enumerate(genes)}\n",
    "    pos.update({f\"CT_{ct}\":(cx, cy[j]) for j,ct in enumerate(cell_types)})\n",
    "    return pos\n",
    "\n",
    "def draw_sample(ax, spec, title, genes, cell_types):\n",
    "    pos = compute_positions(genes, cell_types)\n",
    "    # draw nodes\n",
    "    for i,g in enumerate(genes):\n",
    "        x,y = pos[g]\n",
    "        ax.scatter([x],[y], s=300, marker='o', zorder=3)\n",
    "        ax.text(x-0.03, y, g, va='center', ha='right', fontsize=10)\n",
    "    for ct in cell_types:\n",
    "        name = f\"CT_{ct}\"\n",
    "        x,y = pos[name]\n",
    "        ax.scatter([x],[y], s=350, marker='s', zorder=3)\n",
    "        ax.text(x+0.03, y, ct, va='center', ha='left', fontsize=10)\n",
    "    # draw edges: solid = upregulated (+1), dashed = downregulated (-1)\n",
    "    for ct in cell_types:\n",
    "        ct_name = f\"CT_{ct}\"\n",
    "        for g in spec[ct].get('up', []):\n",
    "            if g in genes:\n",
    "                x1,y1 = pos[g]; x2,y2 = pos[ct_name]\n",
    "                ax.plot([x1,x2],[y1,y2], linestyle='-', linewidth=2, zorder=2)\n",
    "        for g in spec[ct].get('down', []):\n",
    "            if g in genes:\n",
    "                x1,y1 = pos[g]; x2,y2 = pos[ct_name]\n",
    "                ax.plot([x1,x2],[y1,y2], linestyle='--', linewidth=2, zorder=2)\n",
    "\n",
    "    # aesthetics\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim(0.0, 1.0)\n",
    "    ys = list(range(len(genes)))\n",
    "    ax.set_ylim(-0.5, len(genes)-0.5)\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    for spine in ax.spines.values(): spine.set_visible(False)\n",
    "\n",
    "    # legend (solid=up, dashed=down)\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_lines = [\n",
    "        Line2D([0],[0], linestyle='-', linewidth=2, label='Upregulated'),\n",
    "        Line2D([0],[0], linestyle='--', linewidth=2, label='Downregulated'),\n",
    "    ]\n",
    "    ax.legend(handles=legend_lines, loc='upper center', frameon=False, fontsize=9)\n",
    "\n",
    "# ----- draw all four samples -----\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "samples = [\n",
    "    ('Sample 1 [benign]',    benign),\n",
    "    ('Sample 2 [cancerous]',    cancerous),\n",
    "    #('Sample 3 [cancerous]',  cancerous_2),\n",
    "    #('Sample 4 [cancerous]',  cancerous_3),\n",
    "]\n",
    "for ax, (name, spec) in zip(axes.ravel(), samples):\n",
    "    draw_sample(ax, spec, name, genes, cell_types)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99e6534-842b-499f-888b-dddb402d03d6",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04587318-bfc6-43d1-a95c-83f1d9ff4f4c",
   "metadata": {},
   "source": [
    "A machine learning model forward pass now uses the data prep information, runs several layers of linear algebra on it, and then \"predicts\" the probability of our different tasks, in this case the cell type based on the node and whether a graph is cancerous. When it is noisy (like you will see in this example), this process results in gibberish.  The training process changes the noise to pattern during the \"backward pass\" as you'll see. We'll show 3 steps that are focused on training:\n",
    "1. **Data Loading** - this step pulls from the raw data enough examples and batches to complete a forward pass and loss calculation.  If the model is inference only, this step is replaced with taking in the inference input and preparing it similarly as the forward pass. \n",
    "2. **Forward Pass** - using the data and the model architecture we run a prediction for the tokens. When training we also compare against the expected to get loss, but in inference, we use the logits to complete the inference task.\n",
    "3. **Back Propagation, aka Backward Pass & Training** - using differentials we can understand what parameters most drive the difference between forward pass' impact on its prediction versus what is actually right based on the data loading step. We compare this based on the loss function and use the partial derivative gradients to make very minor adjustments to the impactful parameters with the hope it improves future predictions.\n",
    "\n",
    "After our back prop, we'll show a final **Forward Pass** with the updated weights we did in #3 and then convert those final weights to a **Model Inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27430abf-ee0c-4cdf-af24-5634a09a2e92",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267f237e-d685-41d6-b403-1712be222fbe",
   "metadata": {},
   "source": [
    "To start, we need to get enough data to run the forward and backward passes.  Since our total dataset in a real experiment is likely too big to be held in memory all at once in real practice, we will read just enough file information into memory so that we can run the passes, leaving memory and compute to be used on the passes instead of static data holding. \n",
    "To start, we have to identify the batch size and the model context length to determine how much data we need.  Consequently, these dimensions also form 2 of the 3 dimensions in the initial matrix.\n",
    "- **Batch Size (B)** - This is the number of examples you'll train on in a single pass. \n",
    "- **Number of Nodes (N)** - This is basically the \"context length\" for a GNN.  This is the max number of nodes that a model can use in a pass.\n",
    "\n",
    "Beyond these, in a GCN, the depth also controls how much context, or complexity, can be learned. This is because each GCN layer learns 1 hop, or 1 relationship of neighbors. This means that after $L$ layers, a model can learn $L$-hops worth of context.\n",
    "\n",
    "In our case we'll set our batch to be our 4 examples, and nodes to the nodes we have configured, 6. As we walk through you'll also see our GCN will have 2 layers to model 2 network hops: gene > cell type > other genes.\n",
    "\n",
    "We'll prepare 2 sets of data. Our **Inputs** will be the `x_token`, or our list of nodes for each example, and `a_list`, our list of node connections.  Our **Outputs** will be `y_node`, our node level cell type identification, and `y_graph`, the graph level cancerous identification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3157770a-afb9-424b-be9d-50cba46f0dd9",
   "metadata": {},
   "source": [
    "**x_tokens** — list of nodes for each example. Each entry is an integer token id for the node at that position in node_order (e.g.`['CD3D','LCK','ZAP70','CD19','CT_Tcells','CT_Bcells']`).  In our case you'll notice that each example contains all the nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1cf69c7-8f81-4d86-9228-2b69c0ded301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " 6,\n",
       " tensor([[0, 1, 2, 3, 4, 5],\n",
       "         [0, 1, 2, 3, 4, 5]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_batch, N_nodes = x_tokens.shape\n",
    "B_batch, N_nodes, x_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4b9aea-2c17-4fa2-bc63-a4c1135eea2c",
   "metadata": {},
   "source": [
    "**y_node** - per-gene label for cell-type. `0` for T-cell marker and tie, `1` for B-cell marker. As a reminder, this is an aggregation of the up-regulated and down-regulated genes. We focus on which cell type has the gene up-regulated and, if both have it, we use 0. There are ways to handle ties better but we won't get into it.  Since `y_node` also includes the cell types, we'll use -1 to mask them as ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fe5efc3-97fe-4638-b04b-9eb29d7fb42d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6]),\n",
       " tensor([[ 0,  0,  0,  1, -1, -1],\n",
       "         [ 1, -1,  0,  1, -1, -1]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_node.size(), y_node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986b92da-0888-444a-98c2-df2d45389b48",
   "metadata": {},
   "source": [
    "**y_graph** - per-graph label to determine if an example is cancerous. `0` is for benign and `1` is for cancerous.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e984511-b313-42ac-bc7c-e88cf27399d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2]), tensor([0, 1]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_graph.size(), y_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec818687-4747-4e8c-bb10-a400891d7619",
   "metadata": {},
   "source": [
    "**A_list** - The relationships for each of our cells.  You'll notice here that only the last two rows and columns are used.  In this tensor `+1` is for **upregulated** gene per cell type and `-1` is for **downregulated**. `0` is for not in the network.  We also include here a **Gene_mask** that will act in our loss function as a flag to suppress the gene x gene portions of the matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45348c5b-90d0-481f-bbf4-ed5bb93e94d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ True,  True,  True,  True, False, False]),\n",
       " [tensor([[ 0.,  0.,  0.,  0.,  1., -1.],\n",
       "          [ 0.,  0.,  0.,  0.,  1.,  1.],\n",
       "          [ 0.,  0.,  0.,  0.,  1., -1.],\n",
       "          [ 0.,  0.,  0.,  0., -1.,  1.],\n",
       "          [ 1.,  1.,  1., -1.,  0.,  0.],\n",
       "          [-1.,  1., -1.,  1.,  0.,  0.]]),\n",
       "  tensor([[ 0.,  0.,  0.,  0., -1.,  1.],\n",
       "          [ 0.,  0.,  0.,  0., -1., -1.],\n",
       "          [ 0.,  0.,  0.,  0.,  1., -1.],\n",
       "          [ 0.,  0.,  0.,  0., -1.,  1.],\n",
       "          [-1., -1.,  1., -1.,  0.,  0.],\n",
       "          [ 1., -1., -1.,  1.,  0.,  0.]])])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_mask, a_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727058d6-eafd-4b74-88fd-49d4219d30eb",
   "metadata": {},
   "source": [
    "### Data Loading - Blocked Diagonal Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663c8ee9-64e4-4c90-a9a3-69049cbccafc",
   "metadata": {},
   "source": [
    "Since we are trying to learn from all the examples in our batch, we want a uniform tensor of the network connections to learn from. We do this using a block diagonal join that creates a large tensor out of the inputs by simply sliding each new tensor to start at `[i+1,j+1]`.  In our case this results in a `[121224]` matrix since we have 2 examples with 6 nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba483d33-38e5-4b3f-9bc1-7514aeb1a85c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12]),\n",
       " tensor([[ 0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 1.,  1.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [-1.,  1., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  1.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1., -1.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  1.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0., -1., -1.,  1., -1.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  1., -1., -1.,  1.,  0.,  0.]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_blk = torch.block_diag(*a_list)\n",
    "a_blk.size(), a_blk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e81275-ae8f-4968-b325-1deab4139fa4",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a478cb-44ef-48c6-8363-bca483005b15",
   "metadata": {},
   "source": [
    "<img src=\"explainer_screenshots/gat/full_network.png\" width=\"300\">\n",
    "\n",
    "During training, in the GAT we've built, the forward pass takes in the set of nodes per example and a signed network map and uses this to predict the cell type by node and the cancerous level of the graph. This can be viewed as two different classification task at the head level where the head is different for each task. Here the \"batch\" and \"token\" dimensions are one in the same, being that we just have 1 dimension to signify our examples, which we'll call batch.  We could layer in batch based learning but it would make this notebook too complicated and lengthy. \n",
    "\n",
    "Our walkthrough of the forward pass is focused on training where we'll pass in the input nodes `x_tokens` and signed graph `a_blk`, carry that input through the layers, and generate 2 matrices of the probability for the node and graph level prediction. These predictions will be two different sets of `logits`. During the forward pass, after embedding our input nodes we'll pass through two different GAT layers to show how a signed layer and a typical layer behave and to allow the model to learn from 2 hops instead of just 1. \n",
    "\n",
    "At the end of the forward pass we then compare the probability in the logits to the actual next token in `y_node, y_graph` and calculate `loss` based on the difference. You'll see that we calculate the loss on each head, then sum it for a final loss (and so that we can distribute across both pathways in backprop). This difference is what we'll then use in the backprop/training steps.  \n",
    "\n",
    "*Note that we will do some layer initialization to simplify following along.  In reality, layers are often initialized to normal distribution with some adjustments made for parameter sizes, called Xaviar Uniform, to keep the weights properly noisy.  Xaviar Uniform distributes close to 0 and  is designed so that variance of activations and gradients is preserved across layers for linear / tanh / sigmoid-style networks.  We will not cover initialization in this series*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c7257-0fae-48cb-8a5c-193e4e205a48",
   "metadata": {},
   "source": [
    "**Attention Heads**\n",
    "\n",
    "The beauty with attention is the use of multiple heads to allow for learning of different complext concepts.  The more complex concepts you think you want to learn, the more heads you will want to include.  This scaling allows for a broader generalization during training and usage.  \n",
    "\n",
    "Note that the embedding channels need to be cleanly divisible into the heads to allow for flow-through. This is different than in a GPT where each attention head has the same number of channels as the embedding layer.  We're mainly doing it differently to show that both structures work, to make gradient flow easier to aid in explainability, and to reduce the footprint used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37f358c2-7943-4b3e-8ac6-0909ca97e243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_embd= 6\n",
    "heads = 2\n",
    "head_dim = n_embd // heads\n",
    "depth = 1 # just a single hop\n",
    "head_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bddabbf-fbc0-475f-a367-cf7add671d07",
   "metadata": {},
   "source": [
    "### Embedding Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b9e16e-b8b9-4cdf-9248-7887767aeb6a",
   "metadata": {},
   "source": [
    "<img src=\"explainer_screenshots/gat/input_layer.png\" width=\"400\">\n",
    "\n",
    "We'll first create an initial **embedding layer** for our sample level node tokens. Recall that this is the layer that will add the second dimension to our node list. We start with supplying only the nodes. In parallel our adjacency graphs will also be kept on the side as they'll be inputs to the next layer. Generally, by using an embedding on the node, we give the graph a chance to learn how important the different nodes are per example and how to use them, and the node order, in our output prediction task.  We'll also use a small embedding dimension to allow the network to learn a deeper representation of our nodes. After doing the embedding, we'll then remove our batch dimensions for the remaining training to simplify our training.  \n",
    "\n",
    "We'll start by initializing the weight to a sliding scale so that we can quickly see each layer's impact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04abaee3-cd77-4a4f-a165-7918f675ff26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 6]),\n",
       " Parameter containing:\n",
       " tensor([[0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060],\n",
       "         [0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070],\n",
       "         [0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080],\n",
       "         [0.0040, 0.0050, 0.0060, 0.0070, 0.0080, 0.0090],\n",
       "         [0.0050, 0.0060, 0.0070, 0.0080, 0.0090, 0.0100],\n",
       "         [0.0060, 0.0070, 0.0080, 0.0090, 0.0100, 0.0110]], requires_grad=True))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_emb = nn.Embedding(vocab_size, n_embd)\n",
    "with torch.no_grad(): # initilize to W[i,j] = 0.001*(1+i+j) for easy following \n",
    "    vs, d = vocab_size, n_embd\n",
    "    rows = torch.arange(vs).unsqueeze(1)  # (vs,1)\n",
    "    cols = torch.arange(d).unsqueeze(0)  # (1,d)\n",
    "    pattern = 0.001*(1 + rows + cols)  # W[i,j] = 0.001*(1+i+j)\n",
    "    tok_emb.weight.copy_(pattern)\n",
    "tok_emb.weight.size(), tok_emb.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98abc341-ce9a-4d3e-a306-4a8e14bd3444",
   "metadata": {},
   "source": [
    "**Embedding projection**\n",
    "\n",
    "Remember that each of our samples includes all the nodes so we expect that all the weights will be used repeatedly for each sample.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d211712-b32c-4e63-b194-347c9e116707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 6]),\n",
       " tensor([[[0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060],\n",
       "          [0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070],\n",
       "          [0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080],\n",
       "          [0.0040, 0.0050, 0.0060, 0.0070, 0.0080, 0.0090],\n",
       "          [0.0050, 0.0060, 0.0070, 0.0080, 0.0090, 0.0100],\n",
       "          [0.0060, 0.0070, 0.0080, 0.0090, 0.0100, 0.0110]],\n",
       " \n",
       "         [[0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060],\n",
       "          [0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070],\n",
       "          [0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080],\n",
       "          [0.0040, 0.0050, 0.0060, 0.0070, 0.0080, 0.0090],\n",
       "          [0.0050, 0.0060, 0.0070, 0.0080, 0.0090, 0.0100],\n",
       "          [0.0060, 0.0070, 0.0080, 0.0090, 0.0100, 0.0110]]],\n",
       "        grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tok_emb(x_tokens)\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76222510-e2f3-4ba8-a0a2-62bc5fb4b271",
   "metadata": {},
   "source": [
    "**Remove batch**\n",
    "\n",
    "Now we'll remove the batch for further training and learning. If you recall we combined our networks using blocked diagonal so the batch removal now aligns the \"sample\"/\"node\" dimension  with the dimension in the network tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3d6ca48-56aa-4d04-babf-2cdc139b0be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 6]),\n",
       " tensor([[0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060],\n",
       "         [0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070],\n",
       "         [0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080],\n",
       "         [0.0040, 0.0050, 0.0060, 0.0070, 0.0080, 0.0090],\n",
       "         [0.0050, 0.0060, 0.0070, 0.0080, 0.0090, 0.0100],\n",
       "         [0.0060, 0.0070, 0.0080, 0.0090, 0.0100, 0.0110],\n",
       "         [0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060],\n",
       "         [0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070],\n",
       "         [0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080],\n",
       "         [0.0040, 0.0050, 0.0060, 0.0070, 0.0080, 0.0090],\n",
       "         [0.0050, 0.0060, 0.0070, 0.0080, 0.0090, 0.0100],\n",
       "         [0.0060, 0.0070, 0.0080, 0.0090, 0.0100, 0.0110]],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.view(B_batch*N_nodes,n_embd) # remove batch\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eb2ba0-e6d1-49ae-ac16-11b54a94ecb3",
   "metadata": {},
   "source": [
    "### GAT Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a60bfd6-b215-4725-b6ae-d49a8901e6e5",
   "metadata": {},
   "source": [
    "<img src=\"explainer_screenshots/gat/block_details.png\" width=\"400\">\n",
    "\n",
    "To support our two hops of learning (Cell type > gene > cell type) we stack two signed graph attention layers with a nonlinearity to learn increasingly expressive node features. We have two different types of attention layers: one that includes the signed graph attention and one closer to a vanilla GAT that only has attention on the nodes. \n",
    "\n",
    "\n",
    "Each graph attention layer first projects node embeddings with a shared linear map per head, then computes edge-wise attention scores that depend on both endpoint features and, if signed, the sign of the interaction. For each head $h$, we form\n",
    "$$\n",
    "e_{ij}^{h}=\\text{LeakyReLU}\\big((a_{src}^{h})^\\top W^{h}x_i + (a_{dst}^{h})^\\top W^{h}x_j\\big),\n",
    "$$\n",
    "If the GAT is signed an addition $a_{sign}^{h} s_{ij}$ is added. \n",
    "\n",
    "After this, we mask non-edges, and apply a softmax over neighbors $j$ to get attention weights $\\alpha_{ij}^{h}$. The updated node state is then a signed, attention-weighted aggregation of neighbor “values”\n",
    "\n",
    "$$\n",
    "h_i^{\\prime(h)}=\\sum_j \\alpha_{ij}^{h} W^{h}x_j\n",
    "$$\n",
    "\n",
    "If the GAT is signed, we also multiply by our signed edges $s_{ij}$ again. We concatenate the multi-head outputs back into a single embedding.  In this structure, the attention Query, Key, and Value can be thought of as\n",
    "* $Query = (a_{src}^{h})^\\top W^{h}x_i$\n",
    "* $Key = (a_{dst}^{h})^\\top W^{h}x_j$\n",
    "* $Value = W^{h}x_j$\n",
    "\n",
    "We apply dropout to node features and attention coefficients to regularize both representations and neighbor selection. We use an LeakyReLU nonlinearity inside each GAT and ELU between GAT to preserve informative negative signals, and add residual connections around stacked attention layers so information can bypass noisy edges, allowing deeper models that remain expressive while still respecting up vs. down regulation patterns encoded by the signed edges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf8de80-8021-458f-aea0-d30a6fa5e7ed",
   "metadata": {},
   "source": [
    "#### GAT Block - Signed First Attention Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c023339-d911-4524-a4fa-3d9f545ad7bc",
   "metadata": {},
   "source": [
    "<img src=\"explainer_screenshots/gat/signed_gat.png\" width=\"400\">\n",
    "\n",
    "Our first GAT layer is signed so it included the attention for the signed network to the standard GAT structure. This means that we perform the following:\n",
    "$$\n",
    "e_{ij}^{h}=\\text{LeakyReLU}\\big((a_{src}^{h})^\\top W^{h}x_i + (a_{dst}^{h})^\\top W^{h}x_j + a_{sign}^{h} \\odot s_{ij}\\big),\n",
    "$$\n",
    "\n",
    "The addition of the signed edge attention allows the model to use the known graph edges without having to learn them in the attention alone. This allows the model to learn with less data.  \n",
    "\n",
    "For this GAT layer, we'll \n",
    "1. Run feature dropout\n",
    "2. Calculate our linear projection and node attention\n",
    "3. Calculate the signed edge attention\n",
    "4. Run nonlinearity, masking, and softmax\n",
    "5. Calculate the Hadamard product of the attention and the signed network map.\n",
    "6. Take the dot product of the edge attention with the linear projection creating a learned network map.\n",
    "7. A final ELU nonlinearity to support positive and negative values. \n",
    "\n",
    "The inclusion of the Hadamard product allows for gradients to flow down to individual elements of the network map allowing for more precise learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d27d36-de15-451a-a34e-cc45f293abdf",
   "metadata": {},
   "source": [
    "##### Feature Dropout\n",
    "\n",
    "We'll start with Feature level dropout in the incoming embedding.  Dropout will randomly zero out any value effectively removing that specific node from impacting prediction. Since this is Bernoulli based dropout, in addition to zeroing out weights the surviving entries are scaled by $1/(1-p)$.  Feature level dropout stops the model from overfitting to very specific feature patterns and encourages robustness in the learned representations before attention even happens. You can think of it as applying the following: “don’t over-trust any single feature dimension.” \n",
    "\n",
    "Since this introduces `0` you can quickly see the dropout's impact on the embeddings.  You'll also see a 10% increase in the value of each row due to the normalizaiton that dropout does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c467044-9de1-434d-9498-4d02f321c74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dropout=0.10\n",
    "gat1_fdrop = nn.Dropout(feat_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a18d23a3-fb31-4b91-9e0b-5fc6d1c2c857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 6]),\n",
       " tensor([[0.0011, 0.0022, 0.0033, 0.0000, 0.0056, 0.0000],\n",
       "         [0.0022, 0.0033, 0.0044, 0.0056, 0.0067, 0.0078],\n",
       "         [0.0033, 0.0044, 0.0056, 0.0067, 0.0078, 0.0089],\n",
       "         [0.0044, 0.0056, 0.0067, 0.0078, 0.0089, 0.0100],\n",
       "         [0.0056, 0.0067, 0.0078, 0.0089, 0.0100, 0.0111],\n",
       "         [0.0067, 0.0000, 0.0089, 0.0000, 0.0111, 0.0000],\n",
       "         [0.0011, 0.0022, 0.0033, 0.0044, 0.0056, 0.0067],\n",
       "         [0.0022, 0.0033, 0.0044, 0.0056, 0.0067, 0.0078],\n",
       "         [0.0033, 0.0044, 0.0056, 0.0067, 0.0078, 0.0000],\n",
       "         [0.0044, 0.0056, 0.0067, 0.0000, 0.0089, 0.0100],\n",
       "         [0.0056, 0.0067, 0.0078, 0.0089, 0.0100, 0.0111],\n",
       "         [0.0067, 0.0000, 0.0089, 0.0100, 0.0111, 0.0000]],\n",
       "        grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_drop = gat1_fdrop(x)\n",
    "x_drop.size(), x_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b018776e-ac01-4119-b2e7-9d583c2ceaed",
   "metadata": {},
   "source": [
    "##### Per-Head linear projection\n",
    "Our first step is to a multi-headed linear projection of the node features.  We create a separate weighted matrix per head so that each head can learn concepts indepndently.  This slices the embedding space into learned sub-spaces for attention to interact with. This subspace is at the node level allowing the attention to interact with different weights of each node when building connections. \n",
    "\n",
    "We'll initilize both heads at the same sliding weight. Because of this you'll see the two heads result in the same product and each channel scales the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80c8d12f-b80a-402e-8e2e-6a92dfb894b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 3]),\n",
       " Parameter containing:\n",
       " tensor([[[0.1000, 0.2000, 0.3000],\n",
       "          [0.1000, 0.2000, 0.3000],\n",
       "          [0.1000, 0.2000, 0.3000],\n",
       "          [0.1000, 0.2000, 0.3000],\n",
       "          [0.1000, 0.2000, 0.3000],\n",
       "          [0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "         [[0.1000, 0.2000, 0.3000],\n",
       "          [0.1000, 0.2000, 0.3000],\n",
       "          [0.1000, 0.2000, 0.3000],\n",
       "          [0.1000, 0.2000, 0.3000],\n",
       "          [0.1000, 0.2000, 0.3000],\n",
       "          [0.1000, 0.2000, 0.3000]]], requires_grad=True))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gat1_attn_w = nn.Parameter(torch.empty(heads, n_embd, head_dim))\n",
    "with torch.no_grad(): \n",
    "    vs, d = n_embd, head_dim\n",
    "    rows = torch.arange(vs).unsqueeze(1)  # (vs,1)\n",
    "    cols = torch.arange(d).unsqueeze(0)  # (1,d)\n",
    "    pattern = 0.1*(1 + cols)  # W[i,j] = 0.001*(1+i+j)\n",
    "    gat1_attn_w.copy_(pattern)\n",
    "gat1_attn_w.size(), gat1_attn_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a44f6030-b424-4489-9001-6d76bc86d832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 12, 3]),\n",
       " tensor([[[0.0012, 0.0024, 0.0037],\n",
       "          [0.0030, 0.0060, 0.0090],\n",
       "          [0.0037, 0.0073, 0.0110],\n",
       "          [0.0043, 0.0087, 0.0130],\n",
       "          [0.0050, 0.0100, 0.0150],\n",
       "          [0.0027, 0.0053, 0.0080],\n",
       "          [0.0023, 0.0047, 0.0070],\n",
       "          [0.0030, 0.0060, 0.0090],\n",
       "          [0.0028, 0.0056, 0.0083],\n",
       "          [0.0036, 0.0071, 0.0107],\n",
       "          [0.0050, 0.0100, 0.0150],\n",
       "          [0.0037, 0.0073, 0.0110]],\n",
       " \n",
       "         [[0.0012, 0.0024, 0.0037],\n",
       "          [0.0030, 0.0060, 0.0090],\n",
       "          [0.0037, 0.0073, 0.0110],\n",
       "          [0.0043, 0.0087, 0.0130],\n",
       "          [0.0050, 0.0100, 0.0150],\n",
       "          [0.0027, 0.0053, 0.0080],\n",
       "          [0.0023, 0.0047, 0.0070],\n",
       "          [0.0030, 0.0060, 0.0090],\n",
       "          [0.0028, 0.0056, 0.0083],\n",
       "          [0.0036, 0.0071, 0.0107],\n",
       "          [0.0050, 0.0100, 0.0150],\n",
       "          [0.0037, 0.0073, 0.0110]]], grad_fn=<CloneBackward0>))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hproj = x_drop @ gat1_attn_w\n",
    "Hproj.size(), Hproj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf228036-14d6-48b1-9da2-b34e29f62b29",
   "metadata": {},
   "source": [
    "#####  Linear Node Attention\n",
    "\n",
    "Our linear Node attention now takes the per-head project and starts creating edge-specific weigths between each pair of connected nodes. This is where our GAT layer begins to learn the adjacency importance. We still skeep separation by head so that each head can learn it's own relationships. For each head, it scores a builds potential edges edge $i \\leftarrow j$ by applying a learned vector to the destination features (thought of as a key) and another to the source features (thought of as a value), then adding them to get a compatibility logit $e_{ij}^{h}$. These learned weights represent “how much should node $i$ listen to node $j$ for this head.” This makes neighbor influence data-driven and asymmetric: different heads can specialize to different patterns (e.g. strong T-cell–like signatures, aberrant cancer patterns), and each node can selectively amplify or suppress specific neighbors rather than averaging them uniformly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8874f1-82f6-4852-9e5b-dfb2e834e4e8",
   "metadata": {},
   "source": [
    "**Source node (query)**\n",
    "\n",
    "We'll start with the source first. We'll initialize the heads so that the second head is double the first.  As a result you can see that the output of this shows the second head as double the first. We will also initilize with consisten weights across the head dimension so that that we can see in backprop how the model changes the importance of the different dimensions thereby building a network representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85cc0b71-5536-4361-9b31-b3a8f1a0724e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 1]),\n",
       " Parameter containing:\n",
       " tensor([[[0.1000],\n",
       "          [0.1000],\n",
       "          [0.1000]],\n",
       " \n",
       "         [[0.2000],\n",
       "          [0.2000],\n",
       "          [0.2000]]], requires_grad=True))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gat1_attn_src = nn.Parameter(torch.empty(heads, head_dim, 1))\n",
    "with torch.no_grad(): \n",
    "    src_pattern = torch.tensor([[[0.1],[0.1],[0.1]],[[0.2],[0.2],[0.2]]])\n",
    "    gat1_attn_src.copy_(src_pattern)\n",
    "gat1_attn_src.size(), gat1_attn_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d80cb9ed-9564-4fc8-a857-9545411d78d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 12, 1]),\n",
       " tensor([[[0.0007],\n",
       "          [0.0018],\n",
       "          [0.0022],\n",
       "          [0.0026],\n",
       "          [0.0030],\n",
       "          [0.0016],\n",
       "          [0.0014],\n",
       "          [0.0018],\n",
       "          [0.0017],\n",
       "          [0.0021],\n",
       "          [0.0030],\n",
       "          [0.0022]],\n",
       " \n",
       "         [[0.0015],\n",
       "          [0.0036],\n",
       "          [0.0044],\n",
       "          [0.0052],\n",
       "          [0.0060],\n",
       "          [0.0032],\n",
       "          [0.0028],\n",
       "          [0.0036],\n",
       "          [0.0033],\n",
       "          [0.0043],\n",
       "          [0.0060],\n",
       "          [0.0044]]], grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_src = Hproj @ gat1_attn_src\n",
    "e_src.size(), e_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e184327-1563-432e-ba3e-58078762683a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 1, 2]),\n",
       " tensor([[[0.0007, 0.0015]],\n",
       " \n",
       "         [[0.0018, 0.0036]],\n",
       " \n",
       "         [[0.0022, 0.0044]],\n",
       " \n",
       "         [[0.0026, 0.0052]],\n",
       " \n",
       "         [[0.0030, 0.0060]],\n",
       " \n",
       "         [[0.0016, 0.0032]],\n",
       " \n",
       "         [[0.0014, 0.0028]],\n",
       " \n",
       "         [[0.0018, 0.0036]],\n",
       " \n",
       "         [[0.0017, 0.0033]],\n",
       " \n",
       "         [[0.0021, 0.0043]],\n",
       " \n",
       "         [[0.0030, 0.0060]],\n",
       " \n",
       "         [[0.0022, 0.0044]]], grad_fn=<PermuteBackward0>))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_src = e_src.permute(1, 2, 0)\n",
    "e_src.size(), e_src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2767adf-e6e3-4227-ab61-70c6200d9814",
   "metadata": {},
   "source": [
    "**Destination (key)**\n",
    "Next we'll build the destination attention. We'll initialize the heads so that the first head is 1.5x the second and that both heads are negative.  As a result you can see the ratio is maintained for the destination and you'll see that we get negative weights, signaling in our case downregulation being important. We are doing this to help show that both positive and negative weights are useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2d7572c-fa56-417d-8613-3a59bb39c8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 1]),\n",
       " Parameter containing:\n",
       " tensor([[[-0.3000],\n",
       "          [-0.3000],\n",
       "          [-0.3000]],\n",
       " \n",
       "         [[-0.2000],\n",
       "          [-0.2000],\n",
       "          [-0.2000]]], requires_grad=True))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gat1_attn_dst = nn.Parameter(torch.empty(heads, head_dim, 1))\n",
    "with torch.no_grad(): \n",
    "    dst_pattern = torch.tensor([[[-0.3],[-0.3],[-0.3]],[[-0.2],[-0.2],[-0.2]]])\n",
    "    gat1_attn_dst.copy_(dst_pattern)\n",
    "gat1_attn_dst.size(), gat1_attn_dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c746cc1-3415-48ed-a8a1-78234e161735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 12, 1]),\n",
       " tensor([[[-0.0022],\n",
       "          [-0.0054],\n",
       "          [-0.0066],\n",
       "          [-0.0078],\n",
       "          [-0.0090],\n",
       "          [-0.0048],\n",
       "          [-0.0042],\n",
       "          [-0.0054],\n",
       "          [-0.0050],\n",
       "          [-0.0064],\n",
       "          [-0.0090],\n",
       "          [-0.0066]],\n",
       " \n",
       "         [[-0.0015],\n",
       "          [-0.0036],\n",
       "          [-0.0044],\n",
       "          [-0.0052],\n",
       "          [-0.0060],\n",
       "          [-0.0032],\n",
       "          [-0.0028],\n",
       "          [-0.0036],\n",
       "          [-0.0033],\n",
       "          [-0.0043],\n",
       "          [-0.0060],\n",
       "          [-0.0044]]], grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_dst = Hproj @ gat1_attn_dst\n",
    "e_dst.size(), e_dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6eb761a-f640-43a4-ba20-f479a92244cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 12, 2]),\n",
       " tensor([[[-0.0022, -0.0015],\n",
       "          [-0.0054, -0.0036],\n",
       "          [-0.0066, -0.0044],\n",
       "          [-0.0078, -0.0052],\n",
       "          [-0.0090, -0.0060],\n",
       "          [-0.0048, -0.0032],\n",
       "          [-0.0042, -0.0028],\n",
       "          [-0.0054, -0.0036],\n",
       "          [-0.0050, -0.0033],\n",
       "          [-0.0064, -0.0043],\n",
       "          [-0.0090, -0.0060],\n",
       "          [-0.0066, -0.0044]]], grad_fn=<PermuteBackward0>))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_dst = e_dst.permute(2, 1, 0)\n",
    "e_dst.size(), e_dst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46beb74a-a8ac-431d-a876-f6ac7afd3930",
   "metadata": {},
   "source": [
    "**Combine Node Directions Together**\n",
    "\n",
    "Now we'll need to combine our two attention weights together.  This combination creates `[12,12]` weights creating the dimension necessary to learn the network that we have encoded in `a_blk`.  You'll see that this matches up with the dimensions of our adjacency map which is a nice mapping and shows what these weights are attempting to learn.  \n",
    "\n",
    "Because we used a negative destination, with scalers bigger than the source, you'll see that in most cases the values are negative.  This just means that at this current initiation state the model thinks down-regulation is very important. Also, because of how our input `x_token` and embeddings have been initiated, you'll see that we actually have a repetition of two equal `[6,12,2]`.  As we do backprop and learning, this should adjust. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "17a09eff-93bb-49d8-abbe-591c456695b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 2]),\n",
       " tensor([[[-1.4667e-03,  0.0000e+00],\n",
       "          [-4.6667e-03, -2.1333e-03],\n",
       "          [-5.8667e-03, -2.9333e-03],\n",
       "          [-7.0667e-03, -3.7333e-03],\n",
       "          [-8.2667e-03, -4.5333e-03],\n",
       "          [-4.0667e-03, -1.7333e-03],\n",
       "          [-3.4667e-03, -1.3333e-03],\n",
       "          [-4.6667e-03, -2.1333e-03],\n",
       "          [-4.2667e-03, -1.8667e-03],\n",
       "          [-5.6667e-03, -2.8000e-03],\n",
       "          [-8.2667e-03, -4.5333e-03],\n",
       "          [-5.8667e-03, -2.9333e-03]],\n",
       " \n",
       "         [[-4.0000e-04,  2.1333e-03],\n",
       "          [-3.6000e-03,  0.0000e+00],\n",
       "          [-4.8000e-03, -8.0000e-04],\n",
       "          [-6.0000e-03, -1.6000e-03],\n",
       "          [-7.2000e-03, -2.4000e-03],\n",
       "          [-3.0000e-03,  4.0000e-04],\n",
       "          [-2.4000e-03,  8.0000e-04],\n",
       "          [-3.6000e-03,  0.0000e+00],\n",
       "          [-3.2000e-03,  2.6667e-04],\n",
       "          [-4.6000e-03, -6.6667e-04],\n",
       "          [-7.2000e-03, -2.4000e-03],\n",
       "          [-4.8000e-03, -8.0000e-04]],\n",
       " \n",
       "         [[-2.3283e-10,  2.9333e-03],\n",
       "          [-3.2000e-03,  8.0000e-04],\n",
       "          [-4.4000e-03,  0.0000e+00],\n",
       "          [-5.6000e-03, -8.0000e-04],\n",
       "          [-6.8000e-03, -1.6000e-03],\n",
       "          [-2.6000e-03,  1.2000e-03],\n",
       "          [-2.0000e-03,  1.6000e-03],\n",
       "          [-3.2000e-03,  8.0000e-04],\n",
       "          [-2.8000e-03,  1.0667e-03],\n",
       "          [-4.2000e-03,  1.3333e-04],\n",
       "          [-6.8000e-03, -1.6000e-03],\n",
       "          [-4.4000e-03,  0.0000e+00]],\n",
       " \n",
       "         [[ 4.0000e-04,  3.7333e-03],\n",
       "          [-2.8000e-03,  1.6000e-03],\n",
       "          [-4.0000e-03,  8.0000e-04],\n",
       "          [-5.2000e-03,  0.0000e+00],\n",
       "          [-6.4000e-03, -8.0000e-04],\n",
       "          [-2.2000e-03,  2.0000e-03],\n",
       "          [-1.6000e-03,  2.4000e-03],\n",
       "          [-2.8000e-03,  1.6000e-03],\n",
       "          [-2.4000e-03,  1.8667e-03],\n",
       "          [-3.8000e-03,  9.3333e-04],\n",
       "          [-6.4000e-03, -8.0000e-04],\n",
       "          [-4.0000e-03,  8.0000e-04]],\n",
       " \n",
       "         [[ 8.0000e-04,  4.5333e-03],\n",
       "          [-2.4000e-03,  2.4000e-03],\n",
       "          [-3.6000e-03,  1.6000e-03],\n",
       "          [-4.8000e-03,  8.0000e-04],\n",
       "          [-6.0000e-03,  0.0000e+00],\n",
       "          [-1.8000e-03,  2.8000e-03],\n",
       "          [-1.2000e-03,  3.2000e-03],\n",
       "          [-2.4000e-03,  2.4000e-03],\n",
       "          [-2.0000e-03,  2.6667e-03],\n",
       "          [-3.4000e-03,  1.7333e-03],\n",
       "          [-6.0000e-03,  0.0000e+00],\n",
       "          [-3.6000e-03,  1.6000e-03]],\n",
       " \n",
       "         [[-6.0000e-04,  1.7333e-03],\n",
       "          [-3.8000e-03, -4.0000e-04],\n",
       "          [-5.0000e-03, -1.2000e-03],\n",
       "          [-6.2000e-03, -2.0000e-03],\n",
       "          [-7.4000e-03, -2.8000e-03],\n",
       "          [-3.2000e-03,  0.0000e+00],\n",
       "          [-2.6000e-03,  4.0000e-04],\n",
       "          [-3.8000e-03, -4.0000e-04],\n",
       "          [-3.4000e-03, -1.3333e-04],\n",
       "          [-4.8000e-03, -1.0667e-03],\n",
       "          [-7.4000e-03, -2.8000e-03],\n",
       "          [-5.0000e-03, -1.2000e-03]],\n",
       " \n",
       "         [[-8.0000e-04,  1.3333e-03],\n",
       "          [-4.0000e-03, -8.0000e-04],\n",
       "          [-5.2000e-03, -1.6000e-03],\n",
       "          [-6.4000e-03, -2.4000e-03],\n",
       "          [-7.6000e-03, -3.2000e-03],\n",
       "          [-3.4000e-03, -4.0000e-04],\n",
       "          [-2.8000e-03,  0.0000e+00],\n",
       "          [-4.0000e-03, -8.0000e-04],\n",
       "          [-3.6000e-03, -5.3333e-04],\n",
       "          [-5.0000e-03, -1.4667e-03],\n",
       "          [-7.6000e-03, -3.2000e-03],\n",
       "          [-5.2000e-03, -1.6000e-03]],\n",
       " \n",
       "         [[-4.0000e-04,  2.1333e-03],\n",
       "          [-3.6000e-03,  0.0000e+00],\n",
       "          [-4.8000e-03, -8.0000e-04],\n",
       "          [-6.0000e-03, -1.6000e-03],\n",
       "          [-7.2000e-03, -2.4000e-03],\n",
       "          [-3.0000e-03,  4.0000e-04],\n",
       "          [-2.4000e-03,  8.0000e-04],\n",
       "          [-3.6000e-03,  0.0000e+00],\n",
       "          [-3.2000e-03,  2.6667e-04],\n",
       "          [-4.6000e-03, -6.6667e-04],\n",
       "          [-7.2000e-03, -2.4000e-03],\n",
       "          [-4.8000e-03, -8.0000e-04]],\n",
       " \n",
       "         [[-5.3333e-04,  1.8667e-03],\n",
       "          [-3.7333e-03, -2.6667e-04],\n",
       "          [-4.9333e-03, -1.0667e-03],\n",
       "          [-6.1333e-03, -1.8667e-03],\n",
       "          [-7.3333e-03, -2.6667e-03],\n",
       "          [-3.1333e-03,  1.3333e-04],\n",
       "          [-2.5333e-03,  5.3333e-04],\n",
       "          [-3.7333e-03, -2.6667e-04],\n",
       "          [-3.3333e-03,  0.0000e+00],\n",
       "          [-4.7333e-03, -9.3333e-04],\n",
       "          [-7.3333e-03, -2.6667e-03],\n",
       "          [-4.9333e-03, -1.0667e-03]],\n",
       " \n",
       "         [[-6.6667e-05,  2.8000e-03],\n",
       "          [-3.2667e-03,  6.6667e-04],\n",
       "          [-4.4667e-03, -1.3333e-04],\n",
       "          [-5.6667e-03, -9.3333e-04],\n",
       "          [-6.8667e-03, -1.7333e-03],\n",
       "          [-2.6667e-03,  1.0667e-03],\n",
       "          [-2.0667e-03,  1.4667e-03],\n",
       "          [-3.2667e-03,  6.6667e-04],\n",
       "          [-2.8667e-03,  9.3333e-04],\n",
       "          [-4.2667e-03,  0.0000e+00],\n",
       "          [-6.8667e-03, -1.7333e-03],\n",
       "          [-4.4667e-03, -1.3333e-04]],\n",
       " \n",
       "         [[ 8.0000e-04,  4.5333e-03],\n",
       "          [-2.4000e-03,  2.4000e-03],\n",
       "          [-3.6000e-03,  1.6000e-03],\n",
       "          [-4.8000e-03,  8.0000e-04],\n",
       "          [-6.0000e-03,  0.0000e+00],\n",
       "          [-1.8000e-03,  2.8000e-03],\n",
       "          [-1.2000e-03,  3.2000e-03],\n",
       "          [-2.4000e-03,  2.4000e-03],\n",
       "          [-2.0000e-03,  2.6667e-03],\n",
       "          [-3.4000e-03,  1.7333e-03],\n",
       "          [-6.0000e-03,  0.0000e+00],\n",
       "          [-3.6000e-03,  1.6000e-03]],\n",
       " \n",
       "         [[-2.3283e-10,  2.9333e-03],\n",
       "          [-3.2000e-03,  8.0000e-04],\n",
       "          [-4.4000e-03,  0.0000e+00],\n",
       "          [-5.6000e-03, -8.0000e-04],\n",
       "          [-6.8000e-03, -1.6000e-03],\n",
       "          [-2.6000e-03,  1.2000e-03],\n",
       "          [-2.0000e-03,  1.6000e-03],\n",
       "          [-3.2000e-03,  8.0000e-04],\n",
       "          [-2.8000e-03,  1.0667e-03],\n",
       "          [-4.2000e-03,  1.3333e-04],\n",
       "          [-6.8000e-03, -1.6000e-03],\n",
       "          [-4.4000e-03,  0.0000e+00]]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = e_src + e_dst\n",
    "e.size(), e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae2ce9d-bda0-431d-82d1-cf92138bc1a9",
   "metadata": {},
   "source": [
    "##### Signed Edge Attention\n",
    "\n",
    "This version of our GAT includes learned attention for the signed edges.  This is non-standard for GAT but I wanted to show how you'd inject it into the Attention block.  Since we want this to be an influence on the learned network attention, we need to learn how much an edge should influence. We achieve this by using a learned scaler and the Hadamard product, or element wise product. This allows each value to be multiplied by a learned scalar.  Additionally, since we have multi-headed attention, we will need two scalares so that each head can learn independently. \n",
    "\n",
    "We'll start by first prepping our adjacency map.  We'll need to do 3 things:\n",
    "1. We want to make sure that our adjacency allows nodes to learn self references, so we first want to fill in the diagonal with a non-0 value.  We'll use `0.5` since `1` is upregulated and `-1` is downregulated. \n",
    "2. Since we have the adjacency and are going to manipulate it, we first will extract the adjacency mask out.  This mask will be used on our network attention to break the back-propogation on node interactions that are not present in our example.  We'll cover how that's done in the masking step.\n",
    "3. Extract the edges out of the adjacency and multiply them by the scalar. This will be our learned Signed Edge Attention where the model can decide how much of the edge is important to the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d30eb41-6191-43b7-9b15-9d675f78bc2f",
   "metadata": {},
   "source": [
    "**Prep Adjacency Map**\n",
    "\n",
    "We'll first start by filling the diagnoal and extracting a mask.  The mask will be a boolean tensor that shows all of the entries in our adjacency that are not connected.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a282808-d4e3-466f-a608-e1521595ded6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12]),\n",
       " tensor([[False,  True,  True,  True, False, False,  True,  True,  True,  True,\n",
       "           True,  True],\n",
       "         [ True, False,  True,  True, False, False,  True,  True,  True,  True,\n",
       "           True,  True],\n",
       "         [ True,  True, False,  True, False, False,  True,  True,  True,  True,\n",
       "           True,  True],\n",
       "         [ True,  True,  True, False, False, False,  True,  True,  True,  True,\n",
       "           True,  True],\n",
       "         [False, False, False, False, False,  True,  True,  True,  True,  True,\n",
       "           True,  True],\n",
       "         [False, False, False, False,  True, False,  True,  True,  True,  True,\n",
       "           True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "          False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "          False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "          False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "          False, False],\n",
       "         [ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
       "          False,  True],\n",
       "         [ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
       "           True, False]]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signed_a = a_blk.clone()\n",
    "signed_a.fill_diagonal_(0.5)                       # self-loops are positive\n",
    "mask = (signed_a == 0)  \n",
    "mask.size(), mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6877cbc7-0ee6-4901-b190-89840c8eb921",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 1]),\n",
       " tensor([[[ 0.5000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 1.0000],\n",
       "          [-1.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000]],\n",
       " \n",
       "         [[ 0.0000],\n",
       "          [ 0.5000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 1.0000],\n",
       "          [ 1.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000]],\n",
       " \n",
       "         [[ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.5000],\n",
       "          [ 0.0000],\n",
       "          [ 1.0000],\n",
       "          [-1.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000]],\n",
       " \n",
       "         [[ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.5000],\n",
       "          [-1.0000],\n",
       "          [ 1.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000]],\n",
       " \n",
       "         [[ 1.0000],\n",
       "          [ 1.0000],\n",
       "          [ 1.0000],\n",
       "          [-1.0000],\n",
       "          [ 0.5000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000]],\n",
       " \n",
       "         [[-1.0000],\n",
       "          [ 1.0000],\n",
       "          [-1.0000],\n",
       "          [ 1.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.5000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000]],\n",
       " \n",
       "         [[ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.5000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [-1.0000],\n",
       "          [ 1.0000]],\n",
       " \n",
       "         [[ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.5000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [-1.0000],\n",
       "          [-1.0000]],\n",
       " \n",
       "         [[ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.5000],\n",
       "          [ 0.0000],\n",
       "          [ 1.0000],\n",
       "          [-1.0000]],\n",
       " \n",
       "         [[ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.5000],\n",
       "          [-1.0000],\n",
       "          [ 1.0000]],\n",
       " \n",
       "         [[ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [-1.0000],\n",
       "          [-1.0000],\n",
       "          [ 1.0000],\n",
       "          [-1.0000],\n",
       "          [ 0.5000],\n",
       "          [ 0.0000]],\n",
       " \n",
       "         [[ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.0000],\n",
       "          [ 1.0000],\n",
       "          [-1.0000],\n",
       "          [-1.0000],\n",
       "          [ 1.0000],\n",
       "          [ 0.0000],\n",
       "          [ 0.5000]]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signed_a = signed_a.unsqueeze(-1) # [M,M,1] sign tensor\n",
    "signed_a.size(), signed_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2182fd6-20df-48b0-b6cf-4e9dd85d017c",
   "metadata": {},
   "source": [
    "**Edge Attention**\n",
    "\n",
    "Now we'll calcualte our edge attention. We'll initiate our first head to be 2x our second.  Also, because our adjacency map is simply either `[-1,0.5,1]` when it's not 0, you can see how the edge gets projected to the two heads.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d946f0ec-e326-4012-915b-e8f42d4c1ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 2]),\n",
       " Parameter containing:\n",
       " tensor([[[0.2000, 0.1000]]], requires_grad=True))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gat1_attn_sign = nn.Parameter(torch.zeros(1, 1, heads))\n",
    "with torch.no_grad(): \n",
    "    sign_pattern = torch.tensor([[[0.2, 0.1]]])\n",
    "    gat1_attn_sign.copy_(sign_pattern)\n",
    "gat1_attn_sign.size(), gat1_attn_sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9ac5ffd7-e677-4295-bcac-f8d0650318d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 2]),\n",
       " tensor([[[ 0.1000,  0.0500],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.2000,  0.1000],\n",
       "          [-0.2000, -0.1000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.0000,  0.0000],\n",
       "          [ 0.1000,  0.0500],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.2000,  0.1000],\n",
       "          [ 0.2000,  0.1000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.1000,  0.0500],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.2000,  0.1000],\n",
       "          [-0.2000, -0.1000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.1000,  0.0500],\n",
       "          [-0.2000, -0.1000],\n",
       "          [ 0.2000,  0.1000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.2000,  0.1000],\n",
       "          [ 0.2000,  0.1000],\n",
       "          [ 0.2000,  0.1000],\n",
       "          [-0.2000, -0.1000],\n",
       "          [ 0.1000,  0.0500],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.2000, -0.1000],\n",
       "          [ 0.2000,  0.1000],\n",
       "          [-0.2000, -0.1000],\n",
       "          [ 0.2000,  0.1000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.1000,  0.0500],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.1000,  0.0500],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [-0.2000, -0.1000],\n",
       "          [ 0.2000,  0.1000]],\n",
       " \n",
       "         [[ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.1000,  0.0500],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [-0.2000, -0.1000],\n",
       "          [-0.2000, -0.1000]],\n",
       " \n",
       "         [[ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.1000,  0.0500],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.2000,  0.1000],\n",
       "          [-0.2000, -0.1000]],\n",
       " \n",
       "         [[ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.1000,  0.0500],\n",
       "          [-0.2000, -0.1000],\n",
       "          [ 0.2000,  0.1000]],\n",
       " \n",
       "         [[ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [-0.2000, -0.1000],\n",
       "          [-0.2000, -0.1000],\n",
       "          [ 0.2000,  0.1000],\n",
       "          [-0.2000, -0.1000],\n",
       "          [ 0.1000,  0.0500],\n",
       "          [ 0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.2000,  0.1000],\n",
       "          [-0.2000, -0.1000],\n",
       "          [-0.2000, -0.1000],\n",
       "          [ 0.2000,  0.1000],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.1000,  0.0500]]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_sig_a = signed_a * gat1_attn_sign\n",
    "e_sig_a.size(), e_sig_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96843a4d-3955-4a73-8914-0fdfae2848a1",
   "metadata": {},
   "source": [
    "**Combine Node and Edge Attention**\n",
    "Now to allow our Edge attention to have an impact on the learned node attention, we need to combine them.  This is a simple summation of the two weights. You'll see that in some of the edges the values were pulled positive showing how impactful the signed edge attention can be when included.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a874dcc0-5c78-4e16-8bc3-3eaaa998fb69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 2]),\n",
       " tensor([[[ 9.8533e-02,  5.0000e-02],\n",
       "          [-4.6667e-03, -2.1333e-03],\n",
       "          [-5.8667e-03, -2.9333e-03],\n",
       "          [-7.0667e-03, -3.7333e-03],\n",
       "          [ 1.9173e-01,  9.5467e-02],\n",
       "          [-2.0407e-01, -1.0173e-01],\n",
       "          [-3.4667e-03, -1.3333e-03],\n",
       "          [-4.6667e-03, -2.1333e-03],\n",
       "          [-4.2667e-03, -1.8667e-03],\n",
       "          [-5.6667e-03, -2.8000e-03],\n",
       "          [-8.2667e-03, -4.5333e-03],\n",
       "          [-5.8667e-03, -2.9333e-03]],\n",
       " \n",
       "         [[-4.0000e-04,  2.1333e-03],\n",
       "          [ 9.6400e-02,  5.0000e-02],\n",
       "          [-4.8000e-03, -8.0000e-04],\n",
       "          [-6.0000e-03, -1.6000e-03],\n",
       "          [ 1.9280e-01,  9.7600e-02],\n",
       "          [ 1.9700e-01,  1.0040e-01],\n",
       "          [-2.4000e-03,  8.0000e-04],\n",
       "          [-3.6000e-03,  0.0000e+00],\n",
       "          [-3.2000e-03,  2.6667e-04],\n",
       "          [-4.6000e-03, -6.6667e-04],\n",
       "          [-7.2000e-03, -2.4000e-03],\n",
       "          [-4.8000e-03, -8.0000e-04]],\n",
       " \n",
       "         [[-2.3283e-10,  2.9333e-03],\n",
       "          [-3.2000e-03,  8.0000e-04],\n",
       "          [ 9.5600e-02,  5.0000e-02],\n",
       "          [-5.6000e-03, -8.0000e-04],\n",
       "          [ 1.9320e-01,  9.8400e-02],\n",
       "          [-2.0260e-01, -9.8800e-02],\n",
       "          [-2.0000e-03,  1.6000e-03],\n",
       "          [-3.2000e-03,  8.0000e-04],\n",
       "          [-2.8000e-03,  1.0667e-03],\n",
       "          [-4.2000e-03,  1.3333e-04],\n",
       "          [-6.8000e-03, -1.6000e-03],\n",
       "          [-4.4000e-03,  0.0000e+00]],\n",
       " \n",
       "         [[ 4.0000e-04,  3.7333e-03],\n",
       "          [-2.8000e-03,  1.6000e-03],\n",
       "          [-4.0000e-03,  8.0000e-04],\n",
       "          [ 9.4800e-02,  5.0000e-02],\n",
       "          [-2.0640e-01, -1.0080e-01],\n",
       "          [ 1.9780e-01,  1.0200e-01],\n",
       "          [-1.6000e-03,  2.4000e-03],\n",
       "          [-2.8000e-03,  1.6000e-03],\n",
       "          [-2.4000e-03,  1.8667e-03],\n",
       "          [-3.8000e-03,  9.3333e-04],\n",
       "          [-6.4000e-03, -8.0000e-04],\n",
       "          [-4.0000e-03,  8.0000e-04]],\n",
       " \n",
       "         [[ 2.0080e-01,  1.0453e-01],\n",
       "          [ 1.9760e-01,  1.0240e-01],\n",
       "          [ 1.9640e-01,  1.0160e-01],\n",
       "          [-2.0480e-01, -9.9200e-02],\n",
       "          [ 9.4000e-02,  5.0000e-02],\n",
       "          [-1.8000e-03,  2.8000e-03],\n",
       "          [-1.2000e-03,  3.2000e-03],\n",
       "          [-2.4000e-03,  2.4000e-03],\n",
       "          [-2.0000e-03,  2.6667e-03],\n",
       "          [-3.4000e-03,  1.7333e-03],\n",
       "          [-6.0000e-03,  0.0000e+00],\n",
       "          [-3.6000e-03,  1.6000e-03]],\n",
       " \n",
       "         [[-2.0060e-01, -9.8267e-02],\n",
       "          [ 1.9620e-01,  9.9600e-02],\n",
       "          [-2.0500e-01, -1.0120e-01],\n",
       "          [ 1.9380e-01,  9.8000e-02],\n",
       "          [-7.4000e-03, -2.8000e-03],\n",
       "          [ 9.6800e-02,  5.0000e-02],\n",
       "          [-2.6000e-03,  4.0000e-04],\n",
       "          [-3.8000e-03, -4.0000e-04],\n",
       "          [-3.4000e-03, -1.3333e-04],\n",
       "          [-4.8000e-03, -1.0667e-03],\n",
       "          [-7.4000e-03, -2.8000e-03],\n",
       "          [-5.0000e-03, -1.2000e-03]],\n",
       " \n",
       "         [[-8.0000e-04,  1.3333e-03],\n",
       "          [-4.0000e-03, -8.0000e-04],\n",
       "          [-5.2000e-03, -1.6000e-03],\n",
       "          [-6.4000e-03, -2.4000e-03],\n",
       "          [-7.6000e-03, -3.2000e-03],\n",
       "          [-3.4000e-03, -4.0000e-04],\n",
       "          [ 9.7200e-02,  5.0000e-02],\n",
       "          [-4.0000e-03, -8.0000e-04],\n",
       "          [-3.6000e-03, -5.3333e-04],\n",
       "          [-5.0000e-03, -1.4667e-03],\n",
       "          [-2.0760e-01, -1.0320e-01],\n",
       "          [ 1.9480e-01,  9.8400e-02]],\n",
       " \n",
       "         [[-4.0000e-04,  2.1333e-03],\n",
       "          [-3.6000e-03,  0.0000e+00],\n",
       "          [-4.8000e-03, -8.0000e-04],\n",
       "          [-6.0000e-03, -1.6000e-03],\n",
       "          [-7.2000e-03, -2.4000e-03],\n",
       "          [-3.0000e-03,  4.0000e-04],\n",
       "          [-2.4000e-03,  8.0000e-04],\n",
       "          [ 9.6400e-02,  5.0000e-02],\n",
       "          [-3.2000e-03,  2.6667e-04],\n",
       "          [-4.6000e-03, -6.6667e-04],\n",
       "          [-2.0720e-01, -1.0240e-01],\n",
       "          [-2.0480e-01, -1.0080e-01]],\n",
       " \n",
       "         [[-5.3333e-04,  1.8667e-03],\n",
       "          [-3.7333e-03, -2.6667e-04],\n",
       "          [-4.9333e-03, -1.0667e-03],\n",
       "          [-6.1333e-03, -1.8667e-03],\n",
       "          [-7.3333e-03, -2.6667e-03],\n",
       "          [-3.1333e-03,  1.3333e-04],\n",
       "          [-2.5333e-03,  5.3333e-04],\n",
       "          [-3.7333e-03, -2.6667e-04],\n",
       "          [ 9.6667e-02,  5.0000e-02],\n",
       "          [-4.7333e-03, -9.3333e-04],\n",
       "          [ 1.9267e-01,  9.7333e-02],\n",
       "          [-2.0493e-01, -1.0107e-01]],\n",
       " \n",
       "         [[-6.6667e-05,  2.8000e-03],\n",
       "          [-3.2667e-03,  6.6667e-04],\n",
       "          [-4.4667e-03, -1.3333e-04],\n",
       "          [-5.6667e-03, -9.3333e-04],\n",
       "          [-6.8667e-03, -1.7333e-03],\n",
       "          [-2.6667e-03,  1.0667e-03],\n",
       "          [-2.0667e-03,  1.4667e-03],\n",
       "          [-3.2667e-03,  6.6667e-04],\n",
       "          [-2.8667e-03,  9.3333e-04],\n",
       "          [ 9.5733e-02,  5.0000e-02],\n",
       "          [-2.0687e-01, -1.0173e-01],\n",
       "          [ 1.9553e-01,  9.9867e-02]],\n",
       " \n",
       "         [[ 8.0000e-04,  4.5333e-03],\n",
       "          [-2.4000e-03,  2.4000e-03],\n",
       "          [-3.6000e-03,  1.6000e-03],\n",
       "          [-4.8000e-03,  8.0000e-04],\n",
       "          [-6.0000e-03,  0.0000e+00],\n",
       "          [-1.8000e-03,  2.8000e-03],\n",
       "          [-2.0120e-01, -9.6800e-02],\n",
       "          [-2.0240e-01, -9.7600e-02],\n",
       "          [ 1.9800e-01,  1.0267e-01],\n",
       "          [-2.0340e-01, -9.8267e-02],\n",
       "          [ 9.4000e-02,  5.0000e-02],\n",
       "          [-3.6000e-03,  1.6000e-03]],\n",
       " \n",
       "         [[-2.3283e-10,  2.9333e-03],\n",
       "          [-3.2000e-03,  8.0000e-04],\n",
       "          [-4.4000e-03,  0.0000e+00],\n",
       "          [-5.6000e-03, -8.0000e-04],\n",
       "          [-6.8000e-03, -1.6000e-03],\n",
       "          [-2.6000e-03,  1.2000e-03],\n",
       "          [ 1.9800e-01,  1.0160e-01],\n",
       "          [-2.0320e-01, -9.9200e-02],\n",
       "          [-2.0280e-01, -9.8933e-02],\n",
       "          [ 1.9580e-01,  1.0013e-01],\n",
       "          [-6.8000e-03, -1.6000e-03],\n",
       "          [ 9.5600e-02,  5.0000e-02]]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = e + e_sig_a \n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1504fe05-0514-4125-bd20-d5c1bcfac85b",
   "metadata": {},
   "source": [
    "##### Leaky ReLU\n",
    "\n",
    "Next we apply leaky ReLU (rectified linear units), an element-wise nonlinearity that leaves positive activations unchanged and scales negative activations by a small factor, in our case `0.5`. In graph networks it is commonly used to introduce nonlinearity while preserving gradient flow through negative values and avoiding “dying units.” Additionally, leaky ReLU preserves relative ordering between moderately negative vs strongly negative scores, which still matters once you apply the softmax across neighbors. In our signed setup, it’s especially useful because negative contributions (e.g. down-regulation) don’t get hard-clipped; they remain part of the competition for attention instead of being erased outright like would be the case if using ReLU.\n",
    "\n",
    "The formula applied is:\n",
    "$$\n",
    "y=\\max(\\alpha x,x)=\n",
    "\\begin{cases}\n",
    "x,& x>0\\\\\n",
    "\\alpha x,& x\\leq 0\n",
    "\\end{cases} \\quad\n",
    "\\frac{\\partial y}{\\partial x}=\n",
    "\\begin{cases}\n",
    "1,& x>0 \\\\\n",
    "\\alpha,& x\\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "LeakyReLU is stateless, meaning it has no learnable parameters.  As an output you can see that positive values remained in their state while negative values were cut in half. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bfd85eb6-5308-4df8-be1f-c28694cb080e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 2]),\n",
       " tensor([[[ 9.8533e-02,  5.0000e-02],\n",
       "          [-2.3333e-03, -1.0667e-03],\n",
       "          [-2.9333e-03, -1.4667e-03],\n",
       "          [-3.5333e-03, -1.8667e-03],\n",
       "          [ 1.9173e-01,  9.5467e-02],\n",
       "          [-1.0203e-01, -5.0867e-02],\n",
       "          [-1.7333e-03, -6.6667e-04],\n",
       "          [-2.3333e-03, -1.0667e-03],\n",
       "          [-2.1333e-03, -9.3333e-04],\n",
       "          [-2.8333e-03, -1.4000e-03],\n",
       "          [-4.1333e-03, -2.2667e-03],\n",
       "          [-2.9333e-03, -1.4667e-03]],\n",
       " \n",
       "         [[-2.0000e-04,  2.1333e-03],\n",
       "          [ 9.6400e-02,  5.0000e-02],\n",
       "          [-2.4000e-03, -4.0000e-04],\n",
       "          [-3.0000e-03, -8.0000e-04],\n",
       "          [ 1.9280e-01,  9.7600e-02],\n",
       "          [ 1.9700e-01,  1.0040e-01],\n",
       "          [-1.2000e-03,  8.0000e-04],\n",
       "          [-1.8000e-03,  0.0000e+00],\n",
       "          [-1.6000e-03,  2.6667e-04],\n",
       "          [-2.3000e-03, -3.3333e-04],\n",
       "          [-3.6000e-03, -1.2000e-03],\n",
       "          [-2.4000e-03, -4.0000e-04]],\n",
       " \n",
       "         [[-1.1642e-10,  2.9333e-03],\n",
       "          [-1.6000e-03,  8.0000e-04],\n",
       "          [ 9.5600e-02,  5.0000e-02],\n",
       "          [-2.8000e-03, -4.0000e-04],\n",
       "          [ 1.9320e-01,  9.8400e-02],\n",
       "          [-1.0130e-01, -4.9400e-02],\n",
       "          [-1.0000e-03,  1.6000e-03],\n",
       "          [-1.6000e-03,  8.0000e-04],\n",
       "          [-1.4000e-03,  1.0667e-03],\n",
       "          [-2.1000e-03,  1.3333e-04],\n",
       "          [-3.4000e-03, -8.0000e-04],\n",
       "          [-2.2000e-03,  0.0000e+00]],\n",
       " \n",
       "         [[ 4.0000e-04,  3.7333e-03],\n",
       "          [-1.4000e-03,  1.6000e-03],\n",
       "          [-2.0000e-03,  8.0000e-04],\n",
       "          [ 9.4800e-02,  5.0000e-02],\n",
       "          [-1.0320e-01, -5.0400e-02],\n",
       "          [ 1.9780e-01,  1.0200e-01],\n",
       "          [-8.0000e-04,  2.4000e-03],\n",
       "          [-1.4000e-03,  1.6000e-03],\n",
       "          [-1.2000e-03,  1.8667e-03],\n",
       "          [-1.9000e-03,  9.3333e-04],\n",
       "          [-3.2000e-03, -4.0000e-04],\n",
       "          [-2.0000e-03,  8.0000e-04]],\n",
       " \n",
       "         [[ 2.0080e-01,  1.0453e-01],\n",
       "          [ 1.9760e-01,  1.0240e-01],\n",
       "          [ 1.9640e-01,  1.0160e-01],\n",
       "          [-1.0240e-01, -4.9600e-02],\n",
       "          [ 9.4000e-02,  5.0000e-02],\n",
       "          [-9.0000e-04,  2.8000e-03],\n",
       "          [-6.0000e-04,  3.2000e-03],\n",
       "          [-1.2000e-03,  2.4000e-03],\n",
       "          [-1.0000e-03,  2.6667e-03],\n",
       "          [-1.7000e-03,  1.7333e-03],\n",
       "          [-3.0000e-03,  0.0000e+00],\n",
       "          [-1.8000e-03,  1.6000e-03]],\n",
       " \n",
       "         [[-1.0030e-01, -4.9133e-02],\n",
       "          [ 1.9620e-01,  9.9600e-02],\n",
       "          [-1.0250e-01, -5.0600e-02],\n",
       "          [ 1.9380e-01,  9.8000e-02],\n",
       "          [-3.7000e-03, -1.4000e-03],\n",
       "          [ 9.6800e-02,  5.0000e-02],\n",
       "          [-1.3000e-03,  4.0000e-04],\n",
       "          [-1.9000e-03, -2.0000e-04],\n",
       "          [-1.7000e-03, -6.6667e-05],\n",
       "          [-2.4000e-03, -5.3333e-04],\n",
       "          [-3.7000e-03, -1.4000e-03],\n",
       "          [-2.5000e-03, -6.0000e-04]],\n",
       " \n",
       "         [[-4.0000e-04,  1.3333e-03],\n",
       "          [-2.0000e-03, -4.0000e-04],\n",
       "          [-2.6000e-03, -8.0000e-04],\n",
       "          [-3.2000e-03, -1.2000e-03],\n",
       "          [-3.8000e-03, -1.6000e-03],\n",
       "          [-1.7000e-03, -2.0000e-04],\n",
       "          [ 9.7200e-02,  5.0000e-02],\n",
       "          [-2.0000e-03, -4.0000e-04],\n",
       "          [-1.8000e-03, -2.6667e-04],\n",
       "          [-2.5000e-03, -7.3333e-04],\n",
       "          [-1.0380e-01, -5.1600e-02],\n",
       "          [ 1.9480e-01,  9.8400e-02]],\n",
       " \n",
       "         [[-2.0000e-04,  2.1333e-03],\n",
       "          [-1.8000e-03,  0.0000e+00],\n",
       "          [-2.4000e-03, -4.0000e-04],\n",
       "          [-3.0000e-03, -8.0000e-04],\n",
       "          [-3.6000e-03, -1.2000e-03],\n",
       "          [-1.5000e-03,  4.0000e-04],\n",
       "          [-1.2000e-03,  8.0000e-04],\n",
       "          [ 9.6400e-02,  5.0000e-02],\n",
       "          [-1.6000e-03,  2.6667e-04],\n",
       "          [-2.3000e-03, -3.3333e-04],\n",
       "          [-1.0360e-01, -5.1200e-02],\n",
       "          [-1.0240e-01, -5.0400e-02]],\n",
       " \n",
       "         [[-2.6667e-04,  1.8667e-03],\n",
       "          [-1.8667e-03, -1.3333e-04],\n",
       "          [-2.4667e-03, -5.3333e-04],\n",
       "          [-3.0667e-03, -9.3333e-04],\n",
       "          [-3.6667e-03, -1.3333e-03],\n",
       "          [-1.5667e-03,  1.3333e-04],\n",
       "          [-1.2667e-03,  5.3333e-04],\n",
       "          [-1.8667e-03, -1.3333e-04],\n",
       "          [ 9.6667e-02,  5.0000e-02],\n",
       "          [-2.3667e-03, -4.6667e-04],\n",
       "          [ 1.9267e-01,  9.7333e-02],\n",
       "          [-1.0247e-01, -5.0533e-02]],\n",
       " \n",
       "         [[-3.3333e-05,  2.8000e-03],\n",
       "          [-1.6333e-03,  6.6667e-04],\n",
       "          [-2.2333e-03, -6.6667e-05],\n",
       "          [-2.8333e-03, -4.6667e-04],\n",
       "          [-3.4333e-03, -8.6667e-04],\n",
       "          [-1.3333e-03,  1.0667e-03],\n",
       "          [-1.0333e-03,  1.4667e-03],\n",
       "          [-1.6333e-03,  6.6667e-04],\n",
       "          [-1.4333e-03,  9.3333e-04],\n",
       "          [ 9.5733e-02,  5.0000e-02],\n",
       "          [-1.0343e-01, -5.0867e-02],\n",
       "          [ 1.9553e-01,  9.9867e-02]],\n",
       " \n",
       "         [[ 8.0000e-04,  4.5333e-03],\n",
       "          [-1.2000e-03,  2.4000e-03],\n",
       "          [-1.8000e-03,  1.6000e-03],\n",
       "          [-2.4000e-03,  8.0000e-04],\n",
       "          [-3.0000e-03,  0.0000e+00],\n",
       "          [-9.0000e-04,  2.8000e-03],\n",
       "          [-1.0060e-01, -4.8400e-02],\n",
       "          [-1.0120e-01, -4.8800e-02],\n",
       "          [ 1.9800e-01,  1.0267e-01],\n",
       "          [-1.0170e-01, -4.9133e-02],\n",
       "          [ 9.4000e-02,  5.0000e-02],\n",
       "          [-1.8000e-03,  1.6000e-03]],\n",
       " \n",
       "         [[-1.1642e-10,  2.9333e-03],\n",
       "          [-1.6000e-03,  8.0000e-04],\n",
       "          [-2.2000e-03,  0.0000e+00],\n",
       "          [-2.8000e-03, -4.0000e-04],\n",
       "          [-3.4000e-03, -8.0000e-04],\n",
       "          [-1.3000e-03,  1.2000e-03],\n",
       "          [ 1.9800e-01,  1.0160e-01],\n",
       "          [-1.0160e-01, -4.9600e-02],\n",
       "          [-1.0140e-01, -4.9467e-02],\n",
       "          [ 1.9580e-01,  1.0013e-01],\n",
       "          [-3.4000e-03, -8.0000e-04],\n",
       "          [ 9.5600e-02,  5.0000e-02]]], grad_fn=<LeakyReluBackward1>))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gat1_attn_lrelu = nn.LeakyReLU(0.5, inplace=True)\n",
    "out = gat1_attn_lrelu(out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8c0ad9-bef6-434a-9b40-b03577b28902",
   "metadata": {},
   "source": [
    "##### Masking\n",
    "Our next step is to mask the edges that we know are not present in our example.  Recall that we built the boolean based mask from our adjacency map when building the signed edge attention. In this mask, a `true` value represented a connection that needed to be suppressed. \n",
    "\n",
    "Since this is the branch for node connections , we need to mute the weights for where there is no edge so that gradients can't flow backwards there. We'll do this in 2 steps.  First we'll replace the values with $-\\infty$ and then during `softmax` those values will become 0. \n",
    "\n",
    "Similar to other steps, we'll need to unrabel our mask to a representation of each edge and then our masked fill will apply the value across both heads. You'll see that after masking the weight sparcity significantly jumps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6fdfc7ac-688b-41af-8e0d-570dc1234ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 2]),\n",
       " tensor([[[ 0.0985,  0.0500],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.1917,  0.0955],\n",
       "          [-0.1020, -0.0509],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [ 0.0964,  0.0500],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.1928,  0.0976],\n",
       "          [ 0.1970,  0.1004],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.0956,  0.0500],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.1932,  0.0984],\n",
       "          [-0.1013, -0.0494],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.0948,  0.0500],\n",
       "          [-0.1032, -0.0504],\n",
       "          [ 0.1978,  0.1020],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf]],\n",
       " \n",
       "         [[ 0.2008,  0.1045],\n",
       "          [ 0.1976,  0.1024],\n",
       "          [ 0.1964,  0.1016],\n",
       "          [-0.1024, -0.0496],\n",
       "          [ 0.0940,  0.0500],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf]],\n",
       " \n",
       "         [[-0.1003, -0.0491],\n",
       "          [ 0.1962,  0.0996],\n",
       "          [-0.1025, -0.0506],\n",
       "          [ 0.1938,  0.0980],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.0968,  0.0500],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.0972,  0.0500],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [-0.1038, -0.0516],\n",
       "          [ 0.1948,  0.0984]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.0964,  0.0500],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [-0.1036, -0.0512],\n",
       "          [-0.1024, -0.0504]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.0967,  0.0500],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.1927,  0.0973],\n",
       "          [-0.1025, -0.0505]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.0957,  0.0500],\n",
       "          [-0.1034, -0.0509],\n",
       "          [ 0.1955,  0.0999]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [-0.1006, -0.0484],\n",
       "          [-0.1012, -0.0488],\n",
       "          [ 0.1980,  0.1027],\n",
       "          [-0.1017, -0.0491],\n",
       "          [ 0.0940,  0.0500],\n",
       "          [   -inf,    -inf]],\n",
       " \n",
       "         [[   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.1980,  0.1016],\n",
       "          [-0.1016, -0.0496],\n",
       "          [-0.1014, -0.0495],\n",
       "          [ 0.1958,  0.1001],\n",
       "          [   -inf,    -inf],\n",
       "          [ 0.0956,  0.0500]]], grad_fn=<MaskedFillBackward0>))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out.masked_fill(mask.unsqueeze(-1), float('-inf'))\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9299fa2a-cbd6-4abe-9d87-51c619f7f0f1",
   "metadata": {},
   "source": [
    "##### SoftMax\n",
    "Now we'll apply the softmax. Softmax turns a real-valued vector into a probability distribution by exponentiating each component and normalizing by the sum so all outputs are nonnegative and sum to 1.  It does this by applying the following: \n",
    "\n",
    "$\\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}$\n",
    "\n",
    "Because of this calculation, we can expect that the values we masked with $-\\infty$ will become 0 due to their extreme negativeness, showing the power of the mask.  Also, now you'll see that each row changes in value since you have `n+1` elements in each row to distribute across. Finally, we'll now see that all of our negative values are removed.  We are not worried about that though since our learned adjacency map is upstream of this and this simply becomes a probability distribution output from our attention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f6d0c43-e939-4009-8dd0-728ab4f59b55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 2]),\n",
       " tensor([[[0.3429, 0.3389],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3764, 0.3547],\n",
       "          [0.2806, 0.3064],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.3118, 0.3225],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3434, 0.3383],\n",
       "          [0.3448, 0.3392],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3420, 0.3384],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3771, 0.3552],\n",
       "          [0.2809, 0.3064],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3414, 0.3381],\n",
       "          [0.2801, 0.3058],\n",
       "          [0.3785, 0.3561],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.2160, 0.2084],\n",
       "          [0.2153, 0.2079],\n",
       "          [0.2151, 0.2078],\n",
       "          [0.1595, 0.1786],\n",
       "          [0.1941, 0.1973],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.1694, 0.1844],\n",
       "          [0.2279, 0.2140],\n",
       "          [0.1690, 0.1842],\n",
       "          [0.2273, 0.2137],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2063, 0.2037],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3424, 0.3386],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2801, 0.3059],\n",
       "          [0.3775, 0.3554]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3790, 0.3561],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3103, 0.3218],\n",
       "          [0.3107, 0.3221]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3424, 0.3387],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3769, 0.3551],\n",
       "          [0.2806, 0.3063]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3420, 0.3384],\n",
       "          [0.2802, 0.3059],\n",
       "          [0.3778, 0.3557]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.1798, 0.1899],\n",
       "          [0.1797, 0.1898],\n",
       "          [0.2424, 0.2209],\n",
       "          [0.1796, 0.1898],\n",
       "          [0.2185, 0.2096],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2282, 0.2142],\n",
       "          [0.1691, 0.1842],\n",
       "          [0.1691, 0.1842],\n",
       "          [0.2277, 0.2139],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2060, 0.2035]]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = torch.softmax(out, dim=1)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4cdf5c-fc8f-4613-b698-f6c53257b313",
   "metadata": {},
   "source": [
    "##### Attention Dropout\n",
    "\n",
    "Now that we have our attention probabilities calculted, we'll do another round of dropout. Attention dropout is an edge-level / neighbor-level regularization. Because it randomly drops some attention coefficients during training it prevents the model from collapsing onto a single neighbor or a tiny subset of edges, forces heads to consider multiple neighbors and not overfit to one “perfect” regulatory edge and, in our signed version, it’s acts as stochastic pruning of both up- and down-regulation edges during training. The best way to think of it is “don’t over-trust any single edge/neighbor.”  Again, this is Bernoulli based dropout, in addition to zeroing out weights the surviving entries are scaled by $1/(1-p)$.  We'll use a p of 0.05 so you'll see the non-0 values increase by that scalar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e77e16fd-eaed-448e-b57d-a76314ba4516",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_dropout = 0.05\n",
    "gat1_attn_drop = nn.Dropout(attn_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "60cb4a03-178f-45fc-b630-612ca113b4e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 12, 2]),\n",
       " tensor([[[0.3610, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3963, 0.3734],\n",
       "          [0.2954, 0.3225],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.3282, 0.3395],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3614, 0.3561],\n",
       "          [0.3630, 0.3571],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3600, 0.3562],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3969, 0.3739],\n",
       "          [0.2957, 0.3225],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3594, 0.3559],\n",
       "          [0.2948, 0.3219],\n",
       "          [0.3984, 0.3749],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.2193],\n",
       "          [0.2266, 0.2189],\n",
       "          [0.2264, 0.2187],\n",
       "          [0.1679, 0.1880],\n",
       "          [0.2043, 0.2077],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.1783, 0.1942],\n",
       "          [0.0000, 0.2253],\n",
       "          [0.1779, 0.1939],\n",
       "          [0.2393, 0.2249],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.2144],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3604, 0.3565],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2948, 0.3220],\n",
       "          [0.3974, 0.3741]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.3748],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3266, 0.3388],\n",
       "          [0.3270, 0.3390]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3605, 0.3565],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3968, 0.3738],\n",
       "          [0.2954, 0.3224]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.3600, 0.3562],\n",
       "          [0.0000, 0.3220],\n",
       "          [0.3977, 0.3744]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.1893, 0.1999],\n",
       "          [0.0000, 0.1998],\n",
       "          [0.2552, 0.2325],\n",
       "          [0.1891, 0.1998],\n",
       "          [0.2299, 0.2206],\n",
       "          [0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.2402, 0.2255],\n",
       "          [0.1780, 0.1939],\n",
       "          [0.1780, 0.1939],\n",
       "          [0.2396, 0.2252],\n",
       "          [0.0000, 0.0000],\n",
       "          [0.0000, 0.2142]]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = gat1_attn_drop(out) \n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40131fd7-2182-4b1f-bc42-7bff26384dea",
   "metadata": {},
   "source": [
    "**Edge impact aggregation by head**\n",
    "\n",
    "multiply neighbor features by edge sign before summing.\n",
    "\n",
    "---- attention-weighted aggregation: out[i,h,:] = sum_j alpha[i,j,h] * Hproj[j,h,:] -----\n",
    "only score-level sign (not message-level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a5a99d-6afb-43ff-b3b7-4bc314203006",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.permute(2, 0, 1) # [H,M,M]\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04c28c6-bea7-494e-8b72-351154a69c40",
   "metadata": {},
   "source": [
    "pass in specifically edge map for each head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcab328-cd4c-4eed-bc16-bb4e64b1fa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_heads = signed_a.squeeze(-1).unsqueeze(0).expand(heads, -1, -1) # [H,M,M]\n",
    "s_heads.size(), s_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcaec31-8caa-40c6-8a94-04c3e7caab89",
   "metadata": {},
   "source": [
    "**network weights * network map**\n",
    "\n",
    "pass weight impact down to each connection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fcdc96-aa3d-433c-a4d2-bca520a4ca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out * s_heads # [H,M,M]\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57801ef-e400-48bb-a4a3-bae79aa262d4",
   "metadata": {},
   "source": [
    "**Residual pass through of node projection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac981005-dc07-4fea-9980-e56faeec4d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out @ Hproj # [H,M,O]\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457ac89d-d7f0-41dd-ad6d-b4e236d74f69",
   "metadata": {},
   "source": [
    "##### concatenate heads\n",
    "Reshape back down into channels and heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc3ae5-a289-4bac-9e8a-c646d6ae14cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.permute(1, 0, 2) # [M,H,O]\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a28edd-df69-461f-952a-7d4c65d534c9",
   "metadata": {},
   "source": [
    "concatenate heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53177d08-6b0f-4133-92a7-4a57e1ff874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.reshape(B_batch*N_nodes, heads * head_dim)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c80e6ca-764d-4a0e-a1e5-0c143639f8a4",
   "metadata": {},
   "source": [
    "##### Add Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dd3732-a2a3-409a-a8a9-c2af7cd3c9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "gat1_bias = nn.Parameter(torch.zeros(heads*head_dim))\n",
    "with torch.no_grad(): \n",
    "    sign_pattern = torch.tensor([1e-6])\n",
    "    gat1_bias.copy_(sign_pattern)\n",
    "gat1_bias.size(), gat1_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5cded6-55ac-4ee0-a96c-f0d9f875ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out + gat1_bias\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca31eb10-229a-4011-871f-b4243d705037",
   "metadata": {},
   "source": [
    "#### GAT Block - ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372f23d6-d418-4e05-b689-1be01986eb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "gat_elu1 = nn.ELU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bb919e-cb9c-49df-b953-bb4ba68a6ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = gat_elu1(out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59824bf4-3443-48a8-a23f-6ae6ab069fd8",
   "metadata": {},
   "source": [
    "#### GAT Block - Second Attention Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebfc62c-98b6-48a7-9f5e-6268e4b74c4b",
   "metadata": {},
   "source": [
    "* **Short answer:** With **baseline GAT**, you don’t need signed edges to model “up” vs “down” because the *attention* decides **how much** to mix each neighbor (non-negative weights via softmax), while the **message transform** (W h_j) can carry **positive or negative** components. The model can (a) learn to *down-weight* edges that reflect down-regulation, and (b) use negative entries in (W) to make a neighbor’s contribution inhibitory—even though the attention coefficient itself is non-negative.\n",
    "\n",
    "* **What’s happening:**\n",
    "\n",
    "  * GAT learns ( \\alpha_{ij}\\ge 0) (importance) and a content vector (W h_j) (which can have positive/negative channels).\n",
    "  * Aggregation is a convex mix (h_i'=\\sum_j \\alpha_{ij},W h_j). If a neighbor represents “down,” the model can either push (\\alpha_{ij}!\\to!0) or learn (W h_j) with components that **subtract** in the downstream linear heads. Hence, explicit edge signs aren’t strictly required to get inhibitory effects.\n",
    "\n",
    "* **When might you *want* signed edges anyway?**\n",
    "  If you want an **explicit inductive bias** that distinguishes excitatory vs inhibitory relations (rather than relying on attention/weights to discover it), use a **signed GAT** variant—e.g., separate attentions for (A^+) and (A^-) and combine:\n",
    "  [\n",
    "  h_i'=\\sum_{j\\in \\mathcal{N}^+(i)}!!\\alpha_{ij}^+,W_+h_j;-!!\\sum_{j\\in \\mathcal{N}^-(i)}!!\\alpha_{ij}^-,W_-h_j,\n",
    "  ]\n",
    "  or inject the sign as an **edge feature** into the attention score (e.g., concatenate (s_{ij}\\in{-1,+1}) to ([Wh_i|Wh_j])).\n",
    "\n",
    "For an educational baseline, binarizing the adjacency keeps the focus on **how attention learns which neighbors to trust**; the learned message transform supplies the needed **positive/negative** effects. If you later want the model to *explicitly* encode “up” vs “down,” switch to a signed-GAT formulation as above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23313e04-bbea-4999-ad65-00d024e30948",
   "metadata": {},
   "source": [
    "##### Feature Dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6dfbd0-a558-4b71-be21-d8e15afe7de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout on features (standard GAT)\n",
    "gat2_fdrop = nn.Dropout(feat_dropout)\n",
    "out_drop = gat2_fdrop(out)\n",
    "out_drop.size(), out_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7fbce1-d511-4d21-af2b-d88c2d50f712",
   "metadata": {},
   "source": [
    "##### Multi-headed linear weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e4cddd-33a6-4a4e-9664-a38a8ce63407",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- per-head linear projection: [M,H,O] -----\n",
    "##### Broadcast X across heads and multiply by per-head W: [H,M,I] @ [H,I,O] -> [H,M,O]\n",
    "gat2_attn_w = nn.Parameter(torch.empty(heads, n_embd, head_dim))\n",
    "with torch.no_grad(): \n",
    "    vs, d = n_embd, head_dim\n",
    "    rows = torch.arange(vs).unsqueeze(1)  # (vs,1)\n",
    "    cols = torch.arange(d).unsqueeze(0)  # (1,d)\n",
    "    pattern = 0.2*(1 + cols)  # W[i,j] = 0.001*(1+i+j)\n",
    "    gat2_attn_w.copy_(pattern)\n",
    "gat2_attn_w.size(), gat2_attn_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f7f3bf-87df-423d-b41a-9c622651c024",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hproj = out_drop @ gat2_attn_w\n",
    "Hproj.size(), Hproj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ef1cc7-b4ed-4893-a0aa-a74d8f904f7c",
   "metadata": {},
   "source": [
    "##### Directional Node Linear Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3f0d0a-7290-4cce-909f-bab9f54bf4db",
   "metadata": {},
   "source": [
    "**Source**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09a9bb4-3fde-4b35-a8e5-8b8b8c12273a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ----- attention logits e_ij per head -----\n",
    "# Compute node-wise scores for each head, then combine to pairwise:\n",
    "gat2_attn_src = nn.Parameter(torch.empty(heads, head_dim, 1))\n",
    "with torch.no_grad(): \n",
    "    src_pattern = torch.tensor([[[0.03],[0.03],[0.03]],[[0.01],[0.01],[0.01]]])\n",
    "    gat2_attn_src.copy_(src_pattern)\n",
    "gat2_attn_src.size(), gat2_attn_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e0ced6-7f17-470c-ab0a-8d80fcb73cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_src = Hproj @ gat2_attn_src\n",
    "e_src.size(), e_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fab3c08-8cd6-4224-a7c9-f7f7026f24d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_src = e_src.permute(1, 2, 0)  # [M,1,H]\n",
    "e_src.size(), e_src\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc88fc34-0b04-4eb1-8ae8-cabd3571a661",
   "metadata": {},
   "source": [
    "**Destination**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba8c334-3e6a-4086-936b-af60944b1774",
   "metadata": {},
   "outputs": [],
   "source": [
    "gat2_attn_dst = nn.Parameter(torch.empty(heads, head_dim, 1))\n",
    "with torch.no_grad(): \n",
    "    dst_pattern = torch.tensor([[[0.01],[0.01],[0.01]],[[0.02],[0.02],[0.02]]])\n",
    "    gat2_attn_dst.copy_(dst_pattern)\n",
    "gat2_attn_dst.size(), gat2_attn_dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e075d366-cb9f-49a9-b236-1023cd1e8695",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_dst = Hproj @ gat2_attn_dst\n",
    "e_dst.size(), e_dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d308531-7425-4c18-9494-e39328d4e3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_dst = e_dst.permute(2, 1, 0)\n",
    "e_dst.size(), e_dst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f96c52-d1a9-43a1-938f-cdeb79da809d",
   "metadata": {},
   "source": [
    "**Combined directions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3afb520-f5c9-49bc-b32d-66a1dbf18456",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = e_src + e_dst            # [M,M,H\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7de4d3d-6291-434e-ad37-8150ec5fc584",
   "metadata": {},
   "source": [
    "##### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b6438e-21b1-490c-b178-c4371d3ebf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### regularization\n",
    "gat2_attn_lrelu = nn.LeakyReLU(0.5, inplace=True)\n",
    "out = gat2_attn_lrelu(out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dca296-d11f-4929-a356-032209a99c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "signed_a = a_blk.clone()\n",
    "signed_a.fill_diagonal_(0.5)                       # self-loops are positive\n",
    "mask = (signed_a == 0)  \n",
    "mask = mask.unsqueeze(-1)\n",
    "mask.size(), mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4993eedb-9c24-43e6-909a-ac22f88e3156",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.masked_fill(mask, float('-inf'))\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e89a2bc-d0ae-4a76-b32b-d66261d43b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torch.softmax(out, dim=1)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf7209f-0f07-4a52-b09d-9447ee05389a",
   "metadata": {},
   "source": [
    "##### Attention Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64c01da-4c18-4491-b107-75a0a9503db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### dropout\n",
    "gat2_attn_drop = nn.Dropout(attn_dropout)\n",
    "out = gat2_attn_drop(out) \n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdaa690-3434-4844-94e9-6a07a376ceba",
   "metadata": {},
   "source": [
    "##### Nodes by features\n",
    "\n",
    "no signed graph impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb52140-71b9-4218-aa09-37d451b76689",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.permute(2, 0, 1)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a43c2b2-ca16-4ffc-94e1-9ce60864ec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out @ Hproj # [H,M,O]\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2ac74d-4043-4440-8a55-6b2a1edab178",
   "metadata": {},
   "source": [
    "##### Head Concatenation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c068f6a5-0bc4-4912-a66c-c80a14f1dca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.permute(1, 0, 2) # [M,H,O]\n",
    "out.size(), out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9f73b4-6958-4c92-bfa3-a96a81aef33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### concat  heads\n",
    "out = out.reshape(B_batch*N_nodes, heads * head_dim)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cdfae9-cf47-4d5d-919d-e335fa4911fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Bias\n",
    "gat2_bias = nn.Parameter(torch.zeros(heads*head_dim))\n",
    "with torch.no_grad(): \n",
    "    sign_pattern = torch.tensor([1e-6])\n",
    "    gat2_bias.copy_(sign_pattern)\n",
    "gat2_bias.size(), gat2_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1b986b-1101-402a-a393-d831db0c24a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out + gat2_bias\n",
    "out.size(), out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8200c22-c120-4f8b-9609-b0c4b64f7bff",
   "metadata": {},
   "source": [
    "### GAT Block - residual Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514df85f-f04f-4cc7-83a6-58c21eb2fc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out + x\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967de571-bdb2-4fc4-97ae-47e70e136830",
   "metadata": {},
   "source": [
    "### GAT Block - Post Residual ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fbd42a-7ad4-4c0a-9236-e2d1f4396839",
   "metadata": {},
   "outputs": [],
   "source": [
    "elu2 = nn.ELU()\n",
    "out = elu2(out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c87789-b608-4646-9e79-bb1cd57f1879",
   "metadata": {},
   "source": [
    "### Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56160a65-8a24-42d5-8759-2c706ed66706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert batch back\n",
    "out = out.view(B_batch, N_nodes,n_embd)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34732a51-d3a4-45d3-8c8a-2f0f10e746d8",
   "metadata": {},
   "source": [
    "#### Node Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f0fad2-469e-4061-a419-e30acae59754",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nodes logits\n",
    "node_head = nn.Linear(n_embd, 2, bias=True)   # cell-type logits per node\n",
    "with torch.no_grad(): # initilize to W[i,j] = 0.001*(1+i+j) for easy following \n",
    "    vs, d = 2, n_embd\n",
    "    rows = torch.arange(vs).unsqueeze(1)  # (vs,1)\n",
    "    cols = torch.arange(d).unsqueeze(0)  # (1,d)\n",
    "    pattern = 0.005*(1 + rows + cols)  # W[i,j] = 0.001*(1+i+j)\n",
    "    node_head.weight.copy_(pattern)\n",
    "nn.init.constant_(node_head.bias, 1e-6)\n",
    "node_head.weight, node_head.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c97b3eb-b370-4475-a27b-08fc6125a34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_logits = node_head(out)\n",
    "node_logits.size(), node_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0710fb10-e272-44c3-b662-146a3eb96684",
   "metadata": {},
   "source": [
    "#### Graph Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8de88e-4911-43aa-ac38-e9ba06bd912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = gene_mask.float().unsqueeze(0)\n",
    "mask.size(), mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fbb9d8-7fbb-4bdb-a143-8c2d58a2dc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "denom = mask.sum(1, keepdim=True).clamp_min(1.0)\n",
    "denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b762ab9e-5972-4c1e-8727-d51944ccbfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled = (out * mask.unsqueeze(-1)).sum(1) / denom\n",
    "pooled.size(), pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf19414-0272-4890-830c-555c8d7a203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_head = nn.Linear(n_embd, 2, bias=True)  # cancer logits per graph\n",
    "with torch.no_grad(): # initilize to W[i,j] = 0.001*(1+i+j) for easy following \n",
    "    vs, d = 2, n_embd\n",
    "    rows = torch.arange(vs).unsqueeze(1)  # (vs,1)\n",
    "    cols = torch.arange(d).unsqueeze(0)  # (1,d)\n",
    "    pattern = 0.005*(1 + rows + cols)  # W[i,j] = 0.001*(1+i+j)\n",
    "    graph_head.weight.copy_(pattern)\n",
    "nn.init.constant_(graph_head.bias, 1e-6) \n",
    "graph_head.weight, graph_head.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acbdbe4-07f7-4376-ade3-af0c459dea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_logits = graph_head(pooled)  # [B,2]\n",
    "graph_logits.size(), graph_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b3a736-9cd3-425a-8c06-824a3e2089bb",
   "metadata": {},
   "source": [
    "## Loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f2f2ce-03d1-4e78-a49c-ef7f0b4995d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = None\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bbb946-d1e3-4075-a8db-976449e0e946",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = (gene_mask & (y_node>=0))\n",
    "valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234bf14d-107c-422c-b2fe-5e9675e40b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_node_logits = node_logits[valid]\n",
    "valid_y_node = y_node[valid]\n",
    "valid_node_logits, valid_y_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d951dc-52e2-48d5-ae87-6832b7e0f8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = F.cross_entropy(valid_node_logits, valid_y_node)\n",
    "losses.append(nl)\n",
    "nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881d8400-a313-4f6d-a539-6c207814bf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.0\n",
    "graph_logits, y_graph, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e94eec-73dc-4225-9f7d-13eb346db493",
   "metadata": {},
   "outputs": [],
   "source": [
    "gl = F.cross_entropy(graph_logits, y_graph)\n",
    "losses.append(gl)\n",
    "gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3531e3-7026-4b97-9acc-a86f1b394c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = sum(losses) \n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60396340-0006-4024-9d5b-3338dbaf1d68",
   "metadata": {},
   "source": [
    "## Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39f9da4-0ad8-41ae-a20b-7a9c2b854979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output layers\n",
    "graph_head.zero_grad()\n",
    "node_head.zero_grad()\n",
    "\n",
    "#gat block\n",
    "gat2_bias.grad = None # slightly different given it's manually built and not a torch layer. \n",
    "gat2_attn_dst.grad = None\n",
    "gat2_attn_src.grad = None\n",
    "gat2_attn_w.grad = None\n",
    "gat1_bias.grad = None # slightly different given it's manually built and not a torch layer. \n",
    "gat1_attn_sign.grad = None\n",
    "gat1_attn_dst.grad = None\n",
    "gat1_attn_src.grad = None\n",
    "gat1_attn_w.grad = None\n",
    "\n",
    "#input layer\n",
    "tok_emb.zero_grad()\n",
    "\n",
    "# validate gradients\n",
    "graph_head.weight.grad,node_head.weight.grad,tok_emb.weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012ab08d-9791-461f-a183-72eb755582a1",
   "metadata": {},
   "source": [
    "### Back Propagation - Auto Diff\n",
    "\n",
    "Now let's see the magic of the gradients populate.  This magic is called auto-differentiation, or auto-diff for short. This allows us to not have to write many layers of nasty code to do the differentiation for us, but, if you're a sadist, you can surely find people who have written out that code (it's not too bad since you just do one layer at a time). \n",
    "\n",
    "Recall that our loss is actually the sum of the loss across multiple heads. The beauty here is that because we used a sum, the gradient will be distributed across both and join as we reach each layer, helping ensure that the graph is optimizing both heads at the same time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa163c79-eabe-4e2a-b251-b552ff0ccdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb423b4-efdb-42cc-a0f6-cc189e23a9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_head.weight.grad,node_head.weight.grad,tok_emb.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aab446-5d59-4c83-8763-3a35abfb9634",
   "metadata": {},
   "outputs": [],
   "source": [
    "gat1_attn_w.grad,gat1_attn_src.grad,gat1_attn_dst.grad,gat1_attn_sign.grad, gat1_bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2478d1a8-46f7-43eb-801b-e6ef22467bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gat2_attn_w.grad,gat2_attn_src.grad,gat2_attn_dst.grad, gat2_bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0656835-47ca-425e-96e7-bf07297c45f9",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30915151-ddcb-4d69-86f9-accc4ea07865",
   "metadata": {},
   "source": [
    "The process of learning now requires us to update our weights based on this gradient. To really feel the \"back propagation\" we'll start with the last layer and work backwards, though, since we have all of the gradients calculated already, the order does not matter. Recall that our loss function is the negative log likelihood ratio so our gradient signs are flipped.  If a parameter is important, the gradient will be more negative, and vice versa. The gradients are a ratio of importance of each parameter and we need to know how much of that gradient to apply to our weights. This \"how much\" is referred to as the *learning rate*. In modern training learning rate schedulers and optimizers are used to vary the rate and application by layer and by training round with learning rates that are small (e.g. 1e-3) and decaying. \n",
    "\n",
    "We however are trying to learn and if you look at the gradient above in most layers it's tiny (~1e-4).  If we used a typical learning rate scheduler, with our batch size, and just 1 pass, the second pass would just have the same values and we wouldn't learn anything new.  Because of this we'll use a very high learning rate of `5.000` so that we amplify the learning from a single pass and can see the weights change. As a warning, DO NOT DO THIS IN REAL TRAINING. If you did your model would most likely not converge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4248322-d8f4-4c7f-9d26-69c88bacdbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Huge learning rate to emphasize\n",
    "lr = 50.0\n",
    "value_lr = 1e5\n",
    "qk_lr =  1e7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2af494e-d35d-452b-aaad-953e1a886392",
   "metadata": {},
   "source": [
    "### Output Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc5015c-6586-4206-a4e9-d931335e1b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    graph_head.weight -= lr * graph_head.weight.grad\n",
    "    graph_head.bias -= graph_head.bias.grad\n",
    "graph_head.weight, graph_head.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3630314-36ff-461c-982f-e9d5efe18f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    node_head.weight -= lr * node_head.weight.grad\n",
    "    node_head.bias -= node_head.bias.grad\n",
    "node_head.weight, node_head.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c453ffb-b1e0-4037-8487-84a03bcd1fb0",
   "metadata": {},
   "source": [
    "### GAT Unit - Main Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d740a9-b6a7-474a-83a8-2c150e0f3e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    gat2_bias -= gat2_bias.grad\n",
    "gat2_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3585bdb3-4553-43c2-8ea8-7c490ebb36cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    gat2_attn_dst -= qk_lr * gat2_attn_dst.grad\n",
    "    gat2_attn_src -= qk_lr * gat2_attn_src.grad\n",
    "    gat2_attn_w -= value_lr * gat2_attn_w.grad\n",
    "gat2_attn_dst, gat2_attn_src, gat2_attn_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af242aba-5c7d-43ee-8215-1746a48f207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    gat1_bias -= gat1_bias.grad\n",
    "gat1_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd30bb4b-830f-4d8c-b591-70e1df49127d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    gat1_attn_sign -= value_lr * gat1_attn_sign.grad\n",
    "    gat1_attn_dst -= qk_lr * gat1_attn_dst.grad\n",
    "    gat1_attn_src -= qk_lr * gat1_attn_src.grad\n",
    "    gat1_attn_w -= value_lr * gat1_attn_w.grad\n",
    "gat1_attn_sign, gat1_attn_dst, gat1_attn_src, gat2_attn_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c0566-b240-4e07-9849-4162e6a3a4e8",
   "metadata": {},
   "source": [
    "### Input Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb27c2d-071a-4e5f-a9aa-5345b12579f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tok_emb.weight -= lr * tok_emb.weight.grad\n",
    "tok_emb.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b348bae-1a6e-40c6-a7dc-99d690948688",
   "metadata": {},
   "source": [
    "## Forward Pass with Updated Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bfca7b-09a0-4bc4-ac8a-f7ad131d74a2",
   "metadata": {},
   "source": [
    "Now that we have the updated weights for each layer, let's do another forward pass and compare the loss. Since each layer was previously explained we will instead focus on just showing the outputs of the different layers and the final loss. If you want, you can check the previous outputs in the cached cell outputs above and compare them to see how the weight changes impacted the values at each layer. \n",
    "\n",
    "You'll notice that because of our high learning rate we're able to see how each layer now shifts the embedding values as the input passes through them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51085ada-c3d2-41e0-9e9b-a4fb540952a4",
   "metadata": {},
   "source": [
    "### Data Re-loading\n",
    "Since we didn't reuse any input variables, we'll just recall them and use them again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be149e9c-dc80-47e5-8bf9-78c64601ac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tokens, y_node, y_graph, gene_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2639f6-45be-47f9-8168-8795f2783dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_blk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af26df0e-31c7-4d58-aabe-65edd247b2ec",
   "metadata": {},
   "source": [
    "### Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361d1b02-09df-4164-8a08-1dfd32323832",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tok_emb(x_tokens)\n",
    "x = x.view(B_batch*N_nodes,n_embd) # remove batch\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea21ec5-f665-45ed-a0d4-8f6b48c6c171",
   "metadata": {},
   "source": [
    "### GAT Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b844a71f-1db5-436b-8c0d-9ea5841dec39",
   "metadata": {},
   "source": [
    "#### GAT Block - Signed First Attention Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057f6981-ae3d-4b12-8bb1-aa8b5e4431f7",
   "metadata": {},
   "source": [
    "##### Feat Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5938cbe-9786-486b-9d9b-4eaaff5c3c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_drop = gat1_fdrop(x)\n",
    "x_drop.size(), x_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0c5a19-d149-4734-8aec-22691c20d957",
   "metadata": {},
   "source": [
    "##### Per-Head Linear Projection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924fc34a-0b61-41c7-8e25-073a02408764",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hproj = x_drop @ gat1_attn_w\n",
    "Hproj.size(), Hproj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778e0292-700e-4f7c-bbd8-e175810261d1",
   "metadata": {},
   "source": [
    "##### Node Attention "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb6c41e-4292-4183-9c0a-6672cba71a3f",
   "metadata": {},
   "source": [
    "**Source**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67896f1-e813-4480-9be5-cc1a75d5a997",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_src = Hproj @ gat1_attn_src\n",
    "e_src = e_src.permute(1, 2, 0)\n",
    "e_src.size(), e_src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37e682c-f262-4727-8911-fb0dec288a79",
   "metadata": {},
   "source": [
    "**Destination**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525bdd18-9d73-4a6b-a4a0-86d5a77160ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_dst = Hproj @ gat1_attn_dst\n",
    "e_dst = e_dst.permute(2, 1, 0)\n",
    "e_dst.size(), e_dst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c1ad97-05b9-46a3-89fb-35d45b103e07",
   "metadata": {},
   "source": [
    "**Combine Nodes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434efbfb-530b-4d08-b28d-b081b3f23c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = e_src + e_dst\n",
    "e.size(), e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86d840e-f275-495e-a24b-81caa847a831",
   "metadata": {},
   "source": [
    "##### Signed Edge Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa730b03-4c4d-48bc-8660-15992a08dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "signed_a = a_blk.clone()\n",
    "signed_a.fill_diagonal_(0.5)                       # self-loops are positive\n",
    "mask = (signed_a == 0)   \n",
    "mask = mask.unsqueeze(-1)\n",
    "mask.size(), mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9d83b6-5707-418b-9cef-5171c52de435",
   "metadata": {},
   "outputs": [],
   "source": [
    "signed_a = signed_a.unsqueeze(-1) # [M,M,1] sign tensor\n",
    "signed_a.size(), signed_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb2ccb-c599-4964-9062-328a02faa85f",
   "metadata": {},
   "source": [
    "**Edge Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a68252-8ba1-4533-9d01-0ed058a54897",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_sig_a = signed_a * gat1_attn_sign\n",
    "e_sig_a.size(), e_sig_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef05aff6-f081-4090-8f1a-effd37631ba0",
   "metadata": {},
   "source": [
    "**Join Edge and Node attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6badb187-d7fb-4ee6-8585-402dc183a81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = e + e_sig_a \n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d27609d-65cc-4be7-9001-3f8235bbb41a",
   "metadata": {},
   "source": [
    "##### Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108fb1c6-91e0-434b-bc15-f0765648d015",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = gat1_attn_lrelu(out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fe0456-8527-455a-8e22-dda3d79be5fd",
   "metadata": {},
   "source": [
    "##### Node/Edge Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c9493a-180a-49da-a5ef-08993627c89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.masked_fill(mask, float('-inf'))\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346b4fc9-02e2-46de-83c8-4153130133c7",
   "metadata": {},
   "source": [
    "##### Node/Edge SoftMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94629877-54c9-47da-9a35-24fd7db07b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torch.softmax(out, dim=1)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15384f1b-23ee-42d9-99b8-8388c5337314",
   "metadata": {},
   "source": [
    "##### Attention Dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9f58a5-547f-42e5-b905-1c3770d12ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = gat1_attn_drop(out) \n",
    "out.size(), out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3568ece-cb96-4202-ae61-92063cc63997",
   "metadata": {},
   "source": [
    "##### Attention Based Edge Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8240dc47-a8ec-4bb7-bb71-0d6093cf0b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.permute(2, 0, 1) # [H,M,M]\n",
    "s_heads = signed_a.squeeze(-1).unsqueeze(0).expand(heads, -1, -1) # [H,M,M]\n",
    "out = out * s_heads # [H,M,M]\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33272957-8515-437d-94f1-1ac503676728",
   "metadata": {},
   "source": [
    "##### Node Attention with Signed Network Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7829906-fb00-48b0-ba81-5b843d286c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out @ Hproj # [H,M,O]\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d5fc3d-78a8-4387-9d3d-68b2fa8078f8",
   "metadata": {},
   "source": [
    "##### Concat Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f415b8-144d-47d6-9f8e-819f82178885",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.permute(1, 0, 2) # [M,H,O]\n",
    "out = out.reshape(B_batch*N_nodes, heads * head_dim)\n",
    "out.size(), out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304c5c1d-d448-4e8e-900c-609fc19b3754",
   "metadata": {},
   "source": [
    "##### Add Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c25f6ba-6e13-4e53-8d3f-363d1a0128df",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out + gat1_bias\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c188f99b-3287-400a-83db-e99389b1962a",
   "metadata": {},
   "source": [
    "#### GAT Block - ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a0d697-2e7a-4197-88a0-1b9753e8fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = gat_elu1(out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfcd0a9-f2f1-4a61-9cc2-0d6ad3271652",
   "metadata": {},
   "source": [
    "#### GAT Block - Second Attention Layer Default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531916cf-2740-460f-bbad-173fa05d7880",
   "metadata": {},
   "source": [
    "##### Feat Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2073313-c60b-4c41-baf2-d629ddab9395",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_drop = gat2_fdrop(out)\n",
    "out_drop.size(), out_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d04f9e-7fa8-43f7-b91e-cadc105b2648",
   "metadata": {},
   "source": [
    "##### Per-Head Linear Projection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde75e98-5ed8-4876-bc4d-d5e4eaeec732",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hproj = out_drop @ gat2_attn_w\n",
    "Hproj.size(), Hproj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34244438-2af8-42a8-98fb-394ae9c2d7e0",
   "metadata": {},
   "source": [
    "##### Node Attention "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66513c53-3f24-48bd-9a45-aeb99749479d",
   "metadata": {},
   "source": [
    "**Source**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c570a9-7d53-48dd-867a-082317200a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_src = Hproj @ gat2_attn_src\n",
    "e_src = e_src.permute(1, 2, 0)\n",
    "e_src.size(), e_src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67a8807-b0d8-40ff-ab37-9956208dc38a",
   "metadata": {},
   "source": [
    "**Destination**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcec189-75cb-406a-b96f-68d5e6de61bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_dst = Hproj @ gat2_attn_dst\n",
    "e_dst = e_dst.permute(2, 1, 0)\n",
    "e_dst.size(), e_dst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afc6491-889d-4b2f-91ff-658b31d02348",
   "metadata": {},
   "source": [
    "**Combine Nodes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19635443-4e9a-4d77-996a-a949f2b479bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = e_src + e_dst            # [M,M,H\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8f04a0-f3a2-4297-a459-033442f96281",
   "metadata": {},
   "source": [
    "##### Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99a205f-46db-46c4-a8a0-239f7f8cdd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "gat2_attn_lrelu = nn.LeakyReLU(0.5, inplace=True)\n",
    "out = gat2_attn_lrelu(out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619a2819-a9c2-4931-a1ef-4915bb21b853",
   "metadata": {},
   "source": [
    "##### Node/Edge Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb7c040-9571-4e21-ae4b-fe5ec329a84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "signed_a = a_blk.clone()\n",
    "signed_a.fill_diagonal_(0.5)                       # self-loops are positive\n",
    "mask = (signed_a == 0)  \n",
    "mask = mask.unsqueeze(-1)\n",
    "mask.size(), mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d260373-6e3d-49f1-b860-2aba362b2ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.masked_fill(mask, float('-inf'))\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ac0657-451b-4889-bdf3-ce5a60233281",
   "metadata": {},
   "source": [
    "##### Node/Edge SoftMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9adc79a-e531-46f8-92eb-073de8427bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torch.softmax(out, dim=1)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f38401e-7ea9-449d-81a6-4fb5d8872e8a",
   "metadata": {},
   "source": [
    "##### Attention Dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0960046-0949-4a80-8224-9c13612e4613",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = gat2_attn_drop(out) \n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a5d4bc-a9b4-4629-8bd4-63a3aece161b",
   "metadata": {},
   "source": [
    "##### Node Attention with Signed Network Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb149e16-6bdc-4034-88bf-d8550798178d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.permute(2, 0, 1) # [H,M,M]\n",
    "out = out @ Hproj # [H,M,O]\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966c0d48-8056-4ba5-96ac-12d855563949",
   "metadata": {},
   "source": [
    "##### Concat Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88613b15-1b13-4ed5-bf5c-c671cf7f61e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.permute(1, 0, 2) # [M,H,O]\n",
    "out = out.reshape(B_batch*N_nodes, heads * head_dim)\n",
    "out.size(), out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c58948-abb2-4aa4-8956-3115a039295d",
   "metadata": {},
   "source": [
    "##### Add Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09175bd-5d6e-4430-8b70-7226daa6895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gat2_bias = nn.Parameter(torch.zeros(heads*head_dim))\n",
    "out = out + gat2_bias\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1930010-513b-4e53-a7cb-110998b890fd",
   "metadata": {},
   "source": [
    "### GAT Block - residual Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9a4c96-286e-4096-b2b1-b56f99a40a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out + x\n",
    "out.size(), out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052c1e8d-6c09-4c3f-98af-224ddbf64a7c",
   "metadata": {},
   "source": [
    "### GAT Block - Post Residual ELU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa012e5-fc48-4e93-a842-5b117db939a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "elu2 = nn.ELU()\n",
    "out = elu2(out)\n",
    "out.size(), out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ebca7-8ec0-438a-b7d9-20549f679626",
   "metadata": {},
   "source": [
    "### Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a409d82-387e-42fa-981c-ee46a914032b",
   "metadata": {},
   "source": [
    "#### Add Batch Back In\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2213c0-f500-483e-9e20-1d995223e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.view(B_batch, N_nodes,n_embd)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5335861d-9bd2-4a32-b6e8-5e2ff68a8563",
   "metadata": {},
   "source": [
    "#### Node Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23272922-d813-4264-a843-2e95c3f75384",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_logits = node_head(out)\n",
    "node_logits.size(), node_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead55033-7a73-4d94-a536-308ae76bbff1",
   "metadata": {},
   "source": [
    "#### Graph Head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a579d06-cf6d-4e65-a936-ee3751fc9f30",
   "metadata": {},
   "source": [
    "##### Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6337fed5-216e-4002-bd5e-e830a5cb85d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = gene_mask.float().unsqueeze(0)\n",
    "denom = mask.sum(1, keepdim=True).clamp_min(1.0)\n",
    "pooled = (out * mask.unsqueeze(-1)).sum(1) / denom\n",
    "pooled.size(), pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8c9159-e407-4e8c-b324-30cdf3b5b01d",
   "metadata": {},
   "source": [
    "##### Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fd636e-f989-438f-b0db-bf9c7c9a2c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_logits = graph_head(pooled)  # [B,2]\n",
    "graph_logits.size(), graph_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c146ce-eb89-4871-8a88-c2ce7bca7f31",
   "metadata": {},
   "source": [
    "### Updated Loss calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922ab015-7ebb-4675-80b6-e3646bce9b35",
   "metadata": {},
   "source": [
    "Now we'll calculate the updated loss.  Our first pass's total loss was 0.69315 for each of the heads and a total of 1.3863, on par with random. Since we're passing through the same example and used a fairly high learning rate we should see a significant improvement with just 1 learning pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130be022-76de-4392-8a1b-c8d8e0a9ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8b13ef-36d2-4f9c-982b-7b2d856623ff",
   "metadata": {},
   "source": [
    "#### node loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048d0e47-35b7-401b-a3dd-676f474d4488",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "valid = (gene_mask & (y_node>=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53069ebb-a3b3-4848-be45-6ad083348c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = F.cross_entropy(node_logits[valid], y_node[valid])\n",
    "losses.append(nl)\n",
    "nl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8381f6ad-00c6-4e5f-a141-220fdcbd8222",
   "metadata": {},
   "source": [
    "#### graph loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209d0586-a0fb-4bdf-8b55-074a6b22b29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gl = F.cross_entropy(graph_logits, y_graph)\n",
    "losses.append(gl)\n",
    "gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9732dd-1344-4fe0-80bb-d425b8b64c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_loss = sum(losses) \n",
    "updated_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c44979d-4d23-4481-9133-069003bb97ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "f'1 round of training resulted in an loss improvment of {loss.item() - updated_loss.item():.4f}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7ecbd2-60f3-43c3-86c3-f5c66b1f5215",
   "metadata": {},
   "source": [
    "## Training SUCCESS!\n",
    "Our training improved the loss by about **~9%** overall (amount may vary since we didn't set a seed). Interestingly we did see that the loss on the node head got worse but the loss on the graph head outweighed the drop in performance.  If we saw this persist we could change the alpha in our loss sum. There are flaws with this, mainly passing the same example through a second time and a crazy high learning rate, but this helps show the fundamentals of what learning does inside a GAT style GNN model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c840484e-2632-40cc-86f3-a00c030b7e01",
   "metadata": {},
   "source": [
    "## Logit to Token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc3cb90-286a-4e0d-8e1f-310f7cae25d1",
   "metadata": {},
   "source": [
    "For our last piece of code on this notebook, we'll actually now convert our logits for each head into actual class predictions.  For class prediction we apply argmax to the logits. Argmax returns the input index at which an array attains its maximum: \n",
    "$$\n",
    "\\operatorname*{argmax}_x f(x)={x\\mid f(x)\\ge f(y)\\ \\forall y}\n",
    "$$\n",
    "Since we're trying to find the class, argmax will return the index at which the class is maximized. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d98f7-f292-4bda-9f38-aa9c27df4e2b",
   "metadata": {},
   "source": [
    "### Graph class prediction \n",
    "As a reminder our graph head predicts whether a graph is cancerous or not.  We can see here that it overly defaulted to true which makes sense given the input data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6598c4a2-f2d7-49bb-b150-4ec9bdd8e252",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Graph preds:\", graph_logits.argmax(-1).tolist(), \" True:\", y_graph.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e801bab-e011-4510-9de5-2b27da8ed668",
   "metadata": {},
   "source": [
    "### Node class prediction \n",
    "As a reminder our node head predicts the cell type for a node in every single example.  It looks at the gene type and, with the assumption it's upregulated, predicts the cell type it's most commonly upregulated in. We could aggregate across different dimensions using argmax again to get a graph level or gene level predictor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc05b5f-a11d-4a7b-99f9-26096d34b720",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Per-node preds (genes only):\")\n",
    "label = {0: \"T\", 1: \"B\"}\n",
    "pred = node_logits.argmax(-1)\n",
    "ytrue = y_node\n",
    "valid = (ytrue >= 0)\n",
    "names = node_order\n",
    "\n",
    "\n",
    "for b in range(pred.size(0)):\n",
    "    idxs = torch.nonzero(valid[b], as_tuple=False).squeeze(1).tolist()\n",
    "    parts = []\n",
    "    for i in idxs:\n",
    "        p = pred[b, i].item()\n",
    "        t = ytrue[b, i].item()\n",
    "        cor = '' if p ==  t else ' X'\n",
    "        parts.append(f'{names[i]}: pred={label[p]} true={label[t]}{cor}')\n",
    "    print(f'sample{b}: ' + ' | '.join(parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad834d82-68ec-4c7f-b12e-8a1b15aca0ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9fee03-639f-4af6-addf-23fc58affd80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
