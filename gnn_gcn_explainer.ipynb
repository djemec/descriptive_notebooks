{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f90f355-7060-4d8c-8642-e4c8b95e47ce",
   "metadata": {},
   "source": [
    "# GNN GCN Explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28f82b9-4659-4d62-a110-21df41985293",
   "metadata": {},
   "source": [
    "The goal of this notebook is to walk through Graph Convolutional Networks (GCNs) flavor of a GNN (graph neural network). Like CNNs on images, GCNs operate on graphs where nodes are entities  and edges encode relations. A standard GCN layer does two things: it aggregates normalized messages from each node’s neighbors (using the graph’s adjacency) and mixes features with a shared linear transform. Stacking more layers lets information flow across more hops (think 2 connections apart). This makes GCNs effective for node classification, link prediction, and graph classification. In this notebook we'll actually focus on a specific variant called a signed GCNs since it allows for edges to have both positive and negative relationships while keeping the same message-passing idea. Common GNN pitfalls include over-smoothing through the use of deep layers that make nodes look alike, over-squashing that creates long-range info bottlenecked, and numerical issues on irregular/signed graphs. We'll address some fixes to these in our examples. \n",
    "\n",
    "To help display how GCNs work, we'll use a representation of a gene regulatory network where we have 2 specific cell types, a set of genes that are up/down regulated, and then a label for each sample to flag if it's cancerous or not. With our GCN, the goal will be to train it to take in a set of genes and how they're regulated and then predict the cell type as the *node task*, or node level prediction and whether it's cancerous as the *graph task* or graph level prediction. This dual goal will require us to balance 2 loss functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2ef733-c5a3-4b0d-98e0-abeda3f6e4c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Graph Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808a9847-8f90-4c35-a1b0-1c39e4901fd8",
   "metadata": {},
   "source": [
    "We'll start with a common preprocessing step. Instead of a typical tokenizer, we have to create a numerical representation of our graph by enumerating the nodes and edges. This process first starts by creating a series of \"token-like\" IDS for our node vocabulary. In our example that becomes the gene and cell types.  We then use integers to map the relationship of the gene and cell types. This is where our **signed graph** starts since we use `+1` edges for up-regulation and `-1` for down-regulation.   As a result we end up with the following generated:\n",
    "1. **x_tokens** - a list of nodes\n",
    "2. **y_node** - per-node (gene) gene cell type labels. This uses a balance of the up-regulated and down-regulated genes and flags for each sample if the gene is more common with B-cells or T-cells.  We use a `-1` here to flag cells to ignore\n",
    "3. **y_graph** - per-graph (sample) cancer type label. We use `1` for cancerous and `0` for benign\n",
    "4. **a_list** - the nodes in our graph that link the different cell types and genes together.  We pool all the samples together into a large block-diagonal matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41e28a2f-365a-42e4-ac2c-412d614dcb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0551b97a-37db-444b-8ca8-22fadbcdfa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_types = [\"Tcells\",\"Bcells\"]\n",
    "genes = ['CD3D','LCK','ZAP70','CD19'] #only focused on genes present\n",
    "node_order = genes + [f'CT_{ct}' for ct in cell_types]   # genes first, then CT nodes\n",
    "gene_mask = torch.tensor([1,1,1,1,0,0], dtype=torch.bool)\n",
    "N = len(node_order) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef4fc78e-a241-4efb-a7b6-fb69fb61cc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "benign = {\n",
    "    'Tcells':{'up': ['CD3D','LCK','ZAP70'],'down': ['CD19']},\n",
    "    'Bcells':{'up': ['CD19'],'down': ['CD3D','LCK','ZAP70']},\n",
    "  #  'Macrophages':{'up': ['CSF1R'],'down': ['CD3D','MS4A1','CD19']},\n",
    "}\n",
    "\n",
    "cancerous = {\n",
    "    'Tcells':{'up': ['CD3D','ZAP70'],'down': ['CD19','LCK']},\n",
    "    'Bcells':{'up': ['CD19','LCK','CD3D'],'down': ['ZAP70']},\n",
    "   # 'Macrophages':{'up': ['CSF1R'],'down': ['CD3D','MS4A1','CD19']},\n",
    "}\n",
    "\n",
    "cancerous_2 = {\n",
    "    'Tcells':{'up': ['ZAP70'],'down': ['CD19','CD3D','LCK']},\n",
    "    'Bcells':{'up': ['CD19','CD3D'],'down': ['LCK','ZAP70']},\n",
    "   # 'Macrophages':{'up': ['CSF1R'],'down': ['CD3D','MS4A1','CD19']},\n",
    "}\n",
    "\n",
    "cancerous_3 = {\n",
    "    'Tcells':{'up': ['LCK'],'down': ['ZAP70','CD19','CD3D']},\n",
    "    'Bcells':{'up': ['CD19','ZAP70'],'down': ['LCK','CD3D']},\n",
    "   # 'Macrophages':{'up': ['CSF1R'],'down': ['CD3D','MS4A1','CD19']},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c338cdf-3f0b-4975-a7d0-777e9e259103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,\n",
       " {'CD3D': 0, 'LCK': 1, 'ZAP70': 2, 'CD19': 3, 'CT_Tcells': 4, 'CT_Bcells': 5})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_vocab(genes, cell_types):\n",
    "    toks = genes + [f\"CT_{ct}\" for ct in cell_types]\n",
    "    stoi = {t:i for i,t in enumerate(toks)}; itos = {i:t for t,i in stoi.items()}\n",
    "    return stoi, itos\n",
    "\n",
    "stoi, itos = build_vocab(genes, cell_types)\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "vocab_size, stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34a5ea7e-e807-4c4e-9812-3ef8dd4fbac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_signed_adj_and_labels(spec, node_order, genes, cell_types, stoi):\n",
    "    N = len(node_order)\n",
    "    A = torch.zeros(N, N)\n",
    "    def idx(n): return node_order.index(n)\n",
    "    def add_edge(g, ct, s):\n",
    "        i, j = idx(g), idx(f\"CT_{ct}\")\n",
    "        A[i,j] = s; A[j,i] = s\n",
    "    for ct in cell_types:\n",
    "        for g in spec[ct].get('up', []):   add_edge(g, ct, +1)\n",
    "        for g in spec[ct].get('down', []): add_edge(g, ct, -1)\n",
    "    X_tokens = torch.tensor([stoi[n] for n in node_order], dtype=torch.long)\n",
    "    y_node = torch.full((N,), -1, dtype=torch.long)   # -1 = ignore (CT nodes)\n",
    "    # per-gene cell-type label: 0=Tcells, 1=Bcells; rule: prefer UP; tie-break by order T then B; fallback from DOWN (invert)\n",
    "    for g in genes:\n",
    "        def score(ct):\n",
    "            rel = spec.get(ct, {})\n",
    "            return int(g in rel.get('up', [])) - int(g in rel.get('down', []))\n",
    "        t_score = score('Tcells')\n",
    "        b_score = score('Bcells')\n",
    "        i = idx(g)\n",
    "        if t_score > b_score: y_node[i] = 0            # T\n",
    "        elif b_score > t_score: y_node[i] = 1          # B\n",
    "        else:\n",
    "            # option (a) ignore ties/no-signal:\n",
    "            # y_node[i] = -1\n",
    "            # option (b) deterministic tie-break to T:\n",
    "            y_node[i] = 0\n",
    "    return X_tokens, y_node, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84b2754c-0deb-4fc9-af0c-e7fc4a097dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = [benign, cancerous, cancerous_2, cancerous_3]\n",
    "x_list, y_node_list, a_list = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f1d9bff-b2d0-44cf-80a3-7664dc96888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for spec in graphs:\n",
    "    x_i, y_i, a_i = make_signed_adj_and_labels(spec, node_order, genes, cell_types, stoi)\n",
    "    x_list.append(x_i); y_node_list.append(y_i); a_list.append(a_i.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1358655c-9f34-4bf5-9018-7d24d3a7d7dc",
   "metadata": {},
   "source": [
    "**x_tokens** - a list of nodes\n",
    "\n",
    "Notice that all nodes are present in each of our samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e2e3a57-6b0d-401b-8b57-bb3a8be550af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 6]),\n",
       " tensor([[0, 1, 2, 3, 4, 5],\n",
       "         [0, 1, 2, 3, 4, 5],\n",
       "         [0, 1, 2, 3, 4, 5],\n",
       "         [0, 1, 2, 3, 4, 5]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tokens = torch.stack(x_list)   \n",
    "x_tokens.size(), x_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26544e1-dae5-45e5-b88a-35927f42b552",
   "metadata": {},
   "source": [
    "**y_node** - per-node (gene) gene cell type labels. This uses a balance of the up-regulated and down-regulated genes and flags for each sample. If the gene is more commonly upregulated with B-cells we flag it as `0` and if it's more commonly up-regulated in T-cells we flag it as `1`.  This evaluation is done per sample (seen here as per row). \n",
    "\n",
    "The last two columns are embeddings purely for cell type so we flag them as -1 to be masked during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3af282a3-7822-4ecb-a0e9-b7c674698c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 6]),\n",
       " tensor([[ 0,  0,  0,  1, -1, -1],\n",
       "         [ 0,  1,  0,  1, -1, -1],\n",
       "         [ 1,  0,  0,  1, -1, -1],\n",
       "         [ 0,  0,  1,  1, -1, -1]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_node   = torch.stack(y_node_list)\n",
    "y_node.size(), y_node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02778eef-10b7-4a71-afc4-9665a423c51b",
   "metadata": {},
   "source": [
    "**y_graph** - per-graph (sample) cancer type label. We use `1` for cancerous and `0` for benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1de539e1-92bb-4ea6-9901-aa10879d7d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4]), tensor([0, 1, 1, 1]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_graph  = torch.tensor([0,1,1,1]) # 0=benign, 1=cancerous\n",
    "y_graph.size(), y_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba5b018-ab31-4a13-8424-54be07ce0c20",
   "metadata": {},
   "source": [
    "**Graph** \n",
    "\n",
    "Since this is a GNN explainer, let's visualize actually how these graphs look. This is in essence the graph the model is looking at and learning how to read so that if it sees a new one, it can predict the properties we are minimizing loss on (cancerous, cell type). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d67ae0d2-29ed-4034-9761-a8e517bd1aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_positions(genes, cell_types):\n",
    "    # left column (genes), right column (cell types)\n",
    "    gx, cx = 0.1, 0.9\n",
    "    gy = list(range(len(genes)-1, -1, -1))  # top-to-bottom\n",
    "    if len(cell_types) > 1:\n",
    "        # spread CTs across same vertical span as genes\n",
    "        span = (len(genes)-1)\n",
    "        cy = [span * (1 - i/(len(cell_types)-1)) for i in range(len(cell_types))]\n",
    "    else:\n",
    "        cy = [(len(genes)-1)/2]\n",
    "    pos = {g:(gx, gy[i]) for i,g in enumerate(genes)}\n",
    "    pos.update({f\"CT_{ct}\":(cx, cy[j]) for j,ct in enumerate(cell_types)})\n",
    "    return pos\n",
    "\n",
    "def draw_sample(ax, spec, title, genes, cell_types):\n",
    "    pos = compute_positions(genes, cell_types)\n",
    "    # draw nodes\n",
    "    for i,g in enumerate(genes):\n",
    "        x,y = pos[g]\n",
    "        ax.scatter([x],[y], s=300, marker='o', zorder=3)\n",
    "        ax.text(x-0.03, y, g, va='center', ha='right', fontsize=10)\n",
    "    for ct in cell_types:\n",
    "        name = f\"CT_{ct}\"\n",
    "        x,y = pos[name]\n",
    "        ax.scatter([x],[y], s=350, marker='s', zorder=3)\n",
    "        ax.text(x+0.03, y, ct, va='center', ha='left', fontsize=10)\n",
    "    # draw edges: solid = upregulated (+1), dashed = downregulated (-1)\n",
    "    for ct in cell_types:\n",
    "        ct_name = f\"CT_{ct}\"\n",
    "        for g in spec[ct].get('up', []):\n",
    "            if g in genes:\n",
    "                x1,y1 = pos[g]; x2,y2 = pos[ct_name]\n",
    "                ax.plot([x1,x2],[y1,y2], linestyle='-', linewidth=2, zorder=2)\n",
    "        for g in spec[ct].get('down', []):\n",
    "            if g in genes:\n",
    "                x1,y1 = pos[g]; x2,y2 = pos[ct_name]\n",
    "                ax.plot([x1,x2],[y1,y2], linestyle='--', linewidth=2, zorder=2)\n",
    "\n",
    "    # aesthetics\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim(0.0, 1.0)\n",
    "    ys = list(range(len(genes)))\n",
    "    ax.set_ylim(-0.5, len(genes)-0.5)\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    for spine in ax.spines.values(): spine.set_visible(False)\n",
    "\n",
    "    # legend (solid=up, dashed=down)\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_lines = [\n",
    "        Line2D([0],[0], linestyle='-', linewidth=2, label='Upregulated'),\n",
    "        Line2D([0],[0], linestyle='--', linewidth=2, label='Downregulated'),\n",
    "    ]\n",
    "    ax.legend(handles=legend_lines, loc='upper center', frameon=False, fontsize=9)\n",
    "\n",
    "# ----- draw all four samples -----\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "samples = [\n",
    "    ('Sample 1 [benign]',    benign),\n",
    "    ('Sample 2 [cancerous]',    cancerous),\n",
    "    ('Sample 3 [cancerous]',  cancerous_2),\n",
    "    ('Sample 4 [cancerous]',  cancerous_3),\n",
    "]\n",
    "for ax, (name, spec) in zip(axes.ravel(), samples):\n",
    "    draw_sample(ax, spec, name, genes, cell_types)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa14583-b673-48ae-a53d-353275a6b170",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c2576e-4d03-47f5-83c9-5f69bb845bb9",
   "metadata": {},
   "source": [
    "A machine learning model forward pass now uses the data prep information, runs several layers of linear algebra on it, and then \"predicts\" the probability of our different tasks, in this case the cell type based on the node and whether a graph is cancerous. When it is noisy (like you will see in this example), this process results in gibberish.  The training process changes the noise to pattern during the \"backward pass\" as you'll see. We'll show 3 steps that are focused on training:\n",
    "1. **Data Loading** - this step pulls from the raw data enough examples and batches to complete a forward pass and loss calculation.  If the model is inference only, this step is replaced with taking in the inference input and preparing it similarly as the forward pass. \n",
    "2. **Forward Pass** - using the data and the model architecture we run a prediction for the tokens. When training we also compare against the expected to get loss, but in inference, we use the logits to complete the inference task.\n",
    "3. **Back Propagation, aka Backward Pass & Training** - using differentials we can understand what parameters most drive the difference between forward pass' impact on its prediction versus what is actually right based on the data loading step. We compare this based on the loss function and use the partial derivative gradients to make very minor adjustments to the impactful parameters with the hope it improves future predictions.\n",
    "\n",
    "After our back prop, we'll show a final **Forward Pass** with the updated weights we did in #3 and then convert those final weights to a **Model Inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1a64a9-88b9-405f-9076-f26b31d751cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b219fb8-da7b-4d3f-928f-3a3fbb3edaf4",
   "metadata": {},
   "source": [
    "To start, we need to get enough data to run the forward and backward passes.  Since our total dataset in a real experiment is likely too big to be held in memory all at once in real practice, we will read just enough file information into memory so that we can run the passes, leaving memory and compute to be used on the passes instead of static data holding. \n",
    "To start, we have to identify the batch size and the model context length to determine how much data we need.  Consequently, these dimensions also form 2 of the 3 dimensions in the initial matrix.\n",
    "- **Batch Size (B)** - This is the number of examples you'll train on in a single pass. \n",
    "- **Number of Nodes (N)** - This is basically the \"context length\" for a GNN.  This is the max number of nodes that a model can use in a pass.\n",
    "\n",
    "Beyond these, in a GCN, the depth also controls how much context, or complexity, can be learned. This is because each GCN layer learns 1 hop, or 1 relationship of neighbors. This means that after $L$ layers, a model can learn $L$-hops worth of context.\n",
    "\n",
    "In our case we'll set our batch to be our 4 examples, and nodes to the nodes we have configured, 6. As we walk through you'll also see our GCN will have 2 layers to model 2 network hops: gene > cell type > other genes.\n",
    "\n",
    "We'll prepare 2 sets of data. Our **Inputs** will be the `x_token`, or our list of nodes for each example, and `a_list`, our list of node connections.  Our **Outputs** will be `y_node`, our node level cell type identification, and `y_graph`, the graph level cancerous identification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8babb989-efe2-4706-b0b0-13441cafc74c",
   "metadata": {},
   "source": [
    "**x_tokens** — list of nodes for each example. Each entry is an integer token id for the node at that position in node_order (e.g.`['CD3D','LCK','ZAP70','CD19','CT_Tcells','CT_Bcells']`).  In our case you'll notice that each example contains all the nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb0a6c56-3be5-4750-b96b-74539aad3516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,\n",
       " 6,\n",
       " tensor([[0, 1, 2, 3, 4, 5],\n",
       "         [0, 1, 2, 3, 4, 5],\n",
       "         [0, 1, 2, 3, 4, 5],\n",
       "         [0, 1, 2, 3, 4, 5]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_batch, N_nodes = x_tokens.shape\n",
    "B_batch, N_nodes, x_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646b7d0c-2855-49f2-82b8-3db6e33e8e65",
   "metadata": {},
   "source": [
    "**y_node** - per-gene label for cell-type. `0` for T-cell marker and tie, `1` for B-cell marker. As a reminder, this is an aggregation of the up-regulated and down-regulated genes. We focus on which cell type has the gene up-regulated and, if both have it, we use 0. There are ways to handle ties better but we won't get into it.  Since `y_node` also includes the cell types, we'll use -1 to mask them as ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "398f1753-b2cd-4bcc-a782-b7df306cc01b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 6]),\n",
       " tensor([[ 0,  0,  0,  1, -1, -1],\n",
       "         [ 0,  1,  0,  1, -1, -1],\n",
       "         [ 1,  0,  0,  1, -1, -1],\n",
       "         [ 0,  0,  1,  1, -1, -1]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_node.size(), y_node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b436ffa9-9fa8-4048-8576-9184378edfae",
   "metadata": {},
   "source": [
    "**y_graph** - per-graph label to determine if an example is cancerous. `0` is for benign and `1` is for cancerous.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a991b33-e35d-4aae-8cb4-53d27f8ec894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4]), tensor([0, 1, 1, 1]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_graph.size(), y_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7ee2b9-bc4b-4863-8d97-0aba83e0e227",
   "metadata": {},
   "source": [
    "**A_list** - The relationships for each of our cells.  You'll notice here that only the last two rows and columns are used.  In this tensor `+1` is for **upregulated** gene per cell type and `-1` is for **downregulated**. `0` is for not in the network.  We also include here a **Gene_mask** that will act in our loss function as a flag to suppress the gene x gene portions of the matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e192adb-bbf8-45bf-8a8d-bc58fbdf7ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ True,  True,  True,  True, False, False]),\n",
       " [tensor([[ 0.,  0.,  0.,  0.,  1., -1.],\n",
       "          [ 0.,  0.,  0.,  0.,  1., -1.],\n",
       "          [ 0.,  0.,  0.,  0.,  1., -1.],\n",
       "          [ 0.,  0.,  0.,  0., -1.,  1.],\n",
       "          [ 1.,  1.,  1., -1.,  0.,  0.],\n",
       "          [-1., -1., -1.,  1.,  0.,  0.]]),\n",
       "  tensor([[ 0.,  0.,  0.,  0.,  1.,  1.],\n",
       "          [ 0.,  0.,  0.,  0., -1.,  1.],\n",
       "          [ 0.,  0.,  0.,  0.,  1., -1.],\n",
       "          [ 0.,  0.,  0.,  0., -1.,  1.],\n",
       "          [ 1., -1.,  1., -1.,  0.,  0.],\n",
       "          [ 1.,  1., -1.,  1.,  0.,  0.]]),\n",
       "  tensor([[ 0.,  0.,  0.,  0., -1.,  1.],\n",
       "          [ 0.,  0.,  0.,  0., -1., -1.],\n",
       "          [ 0.,  0.,  0.,  0.,  1., -1.],\n",
       "          [ 0.,  0.,  0.,  0., -1.,  1.],\n",
       "          [-1., -1.,  1., -1.,  0.,  0.],\n",
       "          [ 1., -1., -1.,  1.,  0.,  0.]]),\n",
       "  tensor([[ 0.,  0.,  0.,  0., -1., -1.],\n",
       "          [ 0.,  0.,  0.,  0.,  1., -1.],\n",
       "          [ 0.,  0.,  0.,  0., -1.,  1.],\n",
       "          [ 0.,  0.,  0.,  0., -1.,  1.],\n",
       "          [-1.,  1., -1., -1.,  0.,  0.],\n",
       "          [-1., -1.,  1.,  1.,  0.,  0.]])])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_mask, a_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a919d5c-671a-4afe-88fb-cce01d5a3834",
   "metadata": {},
   "source": [
    "### Data Loading - Signed Symmetric Graph Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb1cce1-d1c6-474b-8cca-3f1e0df00f24",
   "metadata": {},
   "source": [
    "With `a_list` we now have all of our relationships but they're not in the most training conducive structure.  For our training, since we actually have 2 types of relationships, we need to split that into two different graphs that the model can learn across. This split creates 2 channels, 1 for the excitatory regulation, and one for the inhibitory.  We then run normalization, and a self-loop on the excitatory, to improve numerical stability, scaling, and preserve node signals to themself. Finally, once we've split and normalized each of the samples, we'll combine all of the same-signed tensors together into 1 large one using block-diagnola (which basically puts the graphs iteratively one after another diagonally into one big tensor).  \n",
    "\n",
    "Symmetric degree normalization bounds the propagation operator and removes degree bias. the split $A^{+}/A^{-}$ prevents destructive cancellation by a model trying to learn up and down regulation together, and lets the model learn different filters for up- vs down-regulation, self-loops in the positive channel allow each node to retain its identity. Finally block-diagonal stacking gives clean batching without cross-graph leakage. Fundamentally we apply:\n",
    "\n",
    "$$\n",
    "\\tilde A = D^{-\\tfrac12},\\hat A,D^{-\\tfrac12} \\newline\n",
    "\\text{so}\\\\\n",
    "\\quad w_{ij}=\\frac{\\hat A_{ij}}{\\sqrt{\\deg_i,\\deg_j}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60950adb-ccb7-4337-9b5a-2c7184b563fa",
   "metadata": {},
   "source": [
    "#### Signed Symmetric Graph Norm - First Example\n",
    "\n",
    "Let's start with our first example. This one is our benign.  The first step is to extract the network from the list.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4200c9e-5211-44d3-a484-da85b849c4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 6]),\n",
       " tensor([[ 0.,  0.,  0.,  0.,  1., -1.],\n",
       "         [ 0.,  0.,  0.,  0.,  1., -1.],\n",
       "         [ 0.,  0.,  0.,  0.,  1., -1.],\n",
       "         [ 0.,  0.,  0.,  0., -1.,  1.],\n",
       "         [ 1.,  1.,  1., -1.,  0.,  0.],\n",
       "         [-1., -1., -1.,  1.,  0.,  0.]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = a_list[0]\n",
    "A.size(), A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d961fb59-81d1-409c-9946-311087eade19",
   "metadata": {},
   "source": [
    "**Splitting** \n",
    "\n",
    "Now we'll split the network into the two different tensors, one for the positives and one for the negatives.  Recall that 0s are non-information for our training so we do not care where they go. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40fb7d17-67e5-4bd3-99e8-d285a7781b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 1.],\n",
       "         [1., 1., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1., 0., 0.],\n",
       "         [1., 1., 1., 0., -0., -0.]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_pos = torch.clamp(A, min=0)  \n",
    "A_neg = torch.clamp(-A, min=0)  \n",
    "A_pos, A_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291d98ca-9d51-4eb5-8867-5bc817111f3b",
   "metadata": {},
   "source": [
    "**Self loop**\n",
    "\n",
    "We'll now add the self-loop in. Adding a self-loop lets each node pass some of its own features forward, so its identity isn’t washed out by neighbors, especially when it has few or noisy connections. It acts like a tiny skip connection, stabilizing message passing and improving learning and gradient flow. We do need to be careful though to only include it in one split. If we put the self-loop in both channels, when we do our subtraction during our forward pass, we'd add, then subtract a copy of the node’s own features every pass, which fights the residual/identity signal, shrinks or flips activations, and destabilizes training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ad3f31b-b686-4c26-82b7-b41cefbfd0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 0., 0., 1., 0.],\n",
       "         [0., 1., 0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0., 1., 0.],\n",
       "         [0., 0., 0., 1., 0., 1.],\n",
       "         [1., 1., 1., 0., 1., 0.],\n",
       "         [0., 0., 0., 1., 0., 1.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1., 0., 0.],\n",
       "         [1., 1., 1., 0., -0., -0.]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I = torch.eye(N_nodes)\n",
    "A_pos_hat = A_pos + I # add for self looops \n",
    "A_neg_hat = A_neg\n",
    "A_pos_hat, A_neg_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e462c622-cee4-440b-b82b-dc42d992cf5a",
   "metadata": {},
   "source": [
    "**Degree scaling**\n",
    "\n",
    "As part of our normalization we divide by the square root of the product of the row and column sums.  We'll also add in a small epsilon to make sure we're not dividing by 0. We'll take this sum by graph then take the square roots. After that we'll create a diagonal so that we can take the dot product instead of having to worry about dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95329101-d070-4084-9283-ce26af8513a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 2., 2., 2., 4., 2.]), tensor([1., 1., 1., 1., 1., 3.]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deg_pos = torch.clamp(torch.abs(A_pos_hat).sum(1), min=1e-6)\n",
    "deg_neg = torch.clamp(torch.abs(A_neg_hat).sum(1), min=1e-6)\n",
    "deg_pos, deg_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df87462e-879c-45ce-9a5d-eda9d0594b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.7071, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.7071, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.7071, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.7071, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7071]]),\n",
       " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5774]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_pos_inv_sqrt = torch.diag(deg_pos.rsqrt())\n",
    "D_neg_inv_sqrt = torch.diag(deg_neg.rsqrt())\n",
    "D_pos_inv_sqrt, D_neg_inv_sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5716fb-e00d-4a7f-9b1e-6d069e2ca286",
   "metadata": {},
   "source": [
    "**Normalization**\n",
    "\n",
    "Finally we'll do our normalization through our dot prod of the network with our inverse square root. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f4630e3-933c-4f64-8936-744b6d8445c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5000, 0.0000, 0.0000, 0.0000, 0.3536, 0.0000],\n",
       "         [0.0000, 0.5000, 0.0000, 0.0000, 0.3536, 0.0000],\n",
       "         [0.0000, 0.0000, 0.5000, 0.0000, 0.3536, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000],\n",
       "         [0.3536, 0.3536, 0.3536, 0.0000, 0.2500, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000]]),\n",
       " tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5774],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5774],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5774],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
       "         [0.5774, 0.5774, 0.5774, 0.0000, 0.0000, 0.0000]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Apos_n = D_pos_inv_sqrt @ A_pos_hat @ D_pos_inv_sqrt\n",
    "Aneg_n = D_neg_inv_sqrt @ A_neg_hat @ D_neg_inv_sqrt\n",
    "Apos_n, Aneg_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c29ad28-2acc-479e-9928-27fcc5093bb9",
   "metadata": {},
   "source": [
    "**Bulk prep** \n",
    "\n",
    "Since we have 3 other samples to process, we'll create a list for aggregating the other normalizations and add these into them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afbd95f4-584d-405e-bf1c-caaddf48bfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Apos_blk, Aneg_blk = [], []\n",
    "Apos_blk.append(Apos_n)\n",
    "Aneg_blk.append(Aneg_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da193711-03eb-428e-b7ce-b3c629c613b4",
   "metadata": {},
   "source": [
    "#### Signed Symmetric Graph Norm - Remaining Example\n",
    "\n",
    "Now that we've stepped through the first example, we'll do the same for the remaining three examples.  At the end we'll have normalized entries for all the examples split by sign. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fae058d-f396-444a-ad6f-812ba8285103",
   "metadata": {},
   "outputs": [],
   "source": [
    "for A in a_list[1:]:\n",
    "    A_pos = torch.clamp(A, min=0)  \n",
    "    A_neg = torch.clamp(-A, min=0)  \n",
    "    I = torch.eye(N_nodes)\n",
    "    A_pos_hat = A_pos + I\n",
    "    A_neg_hat = A_neg\n",
    "    deg_pos = torch.clamp(torch.abs(A_pos_hat).sum(1), min=1e-6)\n",
    "    deg_neg = torch.clamp(torch.abs(A_neg_hat).sum(1), min=1e-6)\n",
    "    D_pos_inv_sqrt = torch.diag(deg_pos.rsqrt())\n",
    "    D_neg_inv_sqrt = torch.diag(deg_neg.rsqrt())\n",
    "    Apos_n = D_pos_inv_sqrt @ A_pos_hat @ D_pos_inv_sqrt\n",
    "    Aneg_n = D_neg_inv_sqrt @ A_neg_hat @ D_neg_inv_sqrt\n",
    "    Apos_blk.append(Apos_n)\n",
    "    Aneg_blk.append(Aneg_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39e14fa9-7d81-426c-967c-b40d2735b4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.5000, 0.0000, 0.0000, 0.0000, 0.3536, 0.0000],\n",
       "         [0.0000, 0.5000, 0.0000, 0.0000, 0.3536, 0.0000],\n",
       "         [0.0000, 0.0000, 0.5000, 0.0000, 0.3536, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000],\n",
       "         [0.3536, 0.3536, 0.3536, 0.0000, 0.2500, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000]]),\n",
       " tensor([[0.3333, 0.0000, 0.0000, 0.0000, 0.3333, 0.2887],\n",
       "         [0.0000, 0.5000, 0.0000, 0.0000, 0.0000, 0.3536],\n",
       "         [0.0000, 0.0000, 0.5000, 0.0000, 0.4082, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.3536],\n",
       "         [0.3333, 0.0000, 0.4082, 0.0000, 0.3333, 0.0000],\n",
       "         [0.2887, 0.3536, 0.0000, 0.3536, 0.0000, 0.2500]]),\n",
       " tensor([[0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4082],\n",
       "         [0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.5000, 0.0000, 0.5000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.4082],\n",
       "         [0.0000, 0.0000, 0.5000, 0.0000, 0.5000, 0.0000],\n",
       "         [0.4082, 0.0000, 0.0000, 0.4082, 0.0000, 0.3333]]),\n",
       " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.5000, 0.0000, 0.0000, 0.5000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.5000, 0.0000, 0.0000, 0.4082],\n",
       "         [0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.4082],\n",
       "         [0.0000, 0.5000, 0.0000, 0.0000, 0.5000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.4082, 0.4082, 0.0000, 0.3333]])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Apos_blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "798923a9-66b8-4d82-836f-d2a9474fd16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5774],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5774],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5774],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
       "         [0.5774, 0.5774, 0.5774, 0.0000, 0.0000, 0.0000]]),\n",
       " tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.7071, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.7071, 0.0000],\n",
       "         [0.0000, 0.7071, 0.0000, 0.7071, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000]]),\n",
       " tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.5774, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.4082, 0.5000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7071],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.5774, 0.0000],\n",
       "         [0.5774, 0.4082, 0.0000, 0.5774, 0.0000, 0.0000],\n",
       "         [0.0000, 0.5000, 0.7071, 0.0000, 0.0000, 0.0000]]),\n",
       " tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.4082, 0.5000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7071],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.5774, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.5774, 0.0000],\n",
       "         [0.4082, 0.0000, 0.5774, 0.5774, 0.0000, 0.0000],\n",
       "         [0.5000, 0.7071, 0.0000, 0.0000, 0.0000, 0.0000]])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Aneg_blk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f433ba38-caf3-46a5-9b57-8f6d86f96364",
   "metadata": {},
   "source": [
    "#### Signed Symmetric Graph Norm - Block Diagonal\n",
    "\n",
    "As our final step, we now join our networks together. We do this using a block diagonal join that creates a large tensor out of the inputs by simply sliding each new tensor to start at `[i+1,j+1]`.  In our case this results in a `[24,24]` matrix since we have 4 examples with 6 nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7285c32e-3cd7-4ac9-a185-4ae15b1af3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 24]),\n",
       " tensor([[0.5000, 0.0000, 0.0000, 0.0000, 0.3536, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.5000, 0.0000, 0.0000, 0.3536, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.5000, 0.0000, 0.3536, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3536, 0.3536, 0.3536, 0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000,\n",
       "          0.0000, 0.3333, 0.2887, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000,\n",
       "          0.0000, 0.0000, 0.3536, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000,\n",
       "          0.0000, 0.4082, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.5000, 0.0000, 0.3536, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.4082,\n",
       "          0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2887, 0.3536, 0.0000,\n",
       "          0.3536, 0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4082,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.4082,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.4082, 0.0000, 0.0000, 0.4082, 0.0000, 0.3333,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.5000, 0.0000, 0.0000, 0.5000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.5000, 0.0000, 0.0000, 0.4082],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.4082],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.5000, 0.0000, 0.0000, 0.5000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.4082, 0.4082, 0.0000, 0.3333]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Apos_blk =  torch.block_diag(*Apos_blk)\n",
    "Apos_blk.size(), Apos_blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68ffb621-2e08-4d77-8f90-0f2d163b7330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 24]),\n",
       " tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5774, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5774, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5774, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5774, 0.5774, 0.5774, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.7071, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.7071, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7071, 0.0000,\n",
       "          0.7071, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5774, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4082, 0.5000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7071,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5774, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.5774, 0.4082, 0.0000, 0.5774, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.7071, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.4082, 0.5000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7071],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.5774, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.5774, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.4082, 0.0000, 0.5774, 0.5774, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.5000, 0.7071, 0.0000, 0.0000, 0.0000, 0.0000]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Aneg_blk =  torch.block_diag(*Aneg_blk)\n",
    "Aneg_blk.size(), Aneg_blk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86c1bd9-c69d-4fbf-ba37-cce872c8c085",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87db81ef-4ef5-489f-95fd-09347c67f246",
   "metadata": {},
   "source": [
    "<img src=\"explainer_screenshots/gcn/full_network.png\" width=\"300\">\n",
    "\n",
    "During training, in the GCN we've built, the forward pass takes in the set of nodes per example and a signed network map and uses this to predict the cell type by node and the cancerous level of the graph. This is quite different from other models we've explored where we're doing things like next token prediction. An additional difference here is that our \"batch\" and \"token\" dimensions are one in the same, being that we just have 1 dimension to signify our examples, which we'll call batch.  We could layer in batch based learning but it would make things super complicated. \n",
    "\n",
    "Our walkthrough of the forward pass is focused on training where we'll pass in the input nodes `x_tokens` and signed graph `Apos_blk, Aneg_blk`, carry that input through the layers, and generate 2 matrices of the probability for the node and graph level prediction. These predictions will be two different sets of `logits`. During the forward pass, after embedding our input nodes we'll pass through two different graph convolutions to help learn our two hops in the graph while including in common normalization and residual steps. \n",
    "\n",
    "At the end of the forward pass we then compare the probability in the logits to the actual next token in `y_node, y_graph` and calculate `loss` based on the difference. You'll see that we calculate the loss on each head, then sum it for a final loss (and so that we can distribute across both pathways in backprop). This difference is what we'll then use in the backprop/training steps.  \n",
    "\n",
    "*Note that we will do some layer initialization to simplify following along.  In reality, layers are often initialized to normal distribution with some adjustments made for parameter sizes, called Kaiming normal, to keep the weights properly noisy.  We will not cover initialization in this series*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e25071-34cf-47ee-a392-7b49ef73499c",
   "metadata": {},
   "source": [
    "### Input Layer - Embedding projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca67b56-abc3-4ffa-b057-c2a930d3f5f3",
   "metadata": {},
   "source": [
    "<img src=\"explainer_screenshots/gcn/input_layer.png\" width=\"400\">\n",
    "\n",
    "We'll first create an initial **embedding layer** for our sample level node tokens. Recall that this is the layer that will add the second dimension to our node list. We start with supplying only the nodes. In parallel our network graphs will also be kept on the side as they'll be inputs to the next layer. Generally, by using an embedding on the node, we give the graph a chance to learn how important the different nodes are per example and how to use them, and the node order, in our output prediction task.  We'll also use a small embedding dimension to allow the network to learn a deeper representation of our nodes. After doing the embedding, we'll then remove our batch dimensions for the remaining training to simplify our training.  \n",
    "\n",
    "We'll start by initializing the weight to a sliding scale so that we can quickly see each layer's impact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f64eb292-1169-454f-9e0f-688a9d325e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_embd = 3 # level of embedding of input tokens\n",
    "vocab_size, n_embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a8577c0-d108-4fab-9d41-6b9b8dbb5543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.0010, 0.0020, 0.0030],\n",
       "        [0.0020, 0.0030, 0.0040],\n",
       "        [0.0030, 0.0040, 0.0050],\n",
       "        [0.0040, 0.0050, 0.0060],\n",
       "        [0.0050, 0.0060, 0.0070],\n",
       "        [0.0060, 0.0070, 0.0080]], requires_grad=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_emb = nn.Embedding(vocab_size, n_embd)\n",
    "with torch.no_grad(): # initilize to W[i,j] = 0.001*(1+i+j) for easy following \n",
    "    vs, d = tok_emb.num_embeddings, tok_emb.embedding_dim\n",
    "    rows = torch.arange(vs).unsqueeze(1)  # (vs,1)\n",
    "    cols = torch.arange(d).unsqueeze(0)  # (1,d)\n",
    "    pattern = 0.001*(1 + rows + cols)  # W[i,j] = 0.001*(1+i+j)\n",
    "    tok_emb.weight.copy_(pattern)\n",
    "tok_emb.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85e1292-3edf-4ecf-876d-e6f7786fcfb5",
   "metadata": {},
   "source": [
    "**Embedding projection**\n",
    "\n",
    "Remember that each of our samples includes all the nodes so we expect that all the weights will be used repeatedly for each sample.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39f1fba0-f4ea-4d69-babc-428e284b01dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0010, 0.0020, 0.0030],\n",
       "         [0.0020, 0.0030, 0.0040],\n",
       "         [0.0030, 0.0040, 0.0050],\n",
       "         [0.0040, 0.0050, 0.0060],\n",
       "         [0.0050, 0.0060, 0.0070],\n",
       "         [0.0060, 0.0070, 0.0080]],\n",
       "\n",
       "        [[0.0010, 0.0020, 0.0030],\n",
       "         [0.0020, 0.0030, 0.0040],\n",
       "         [0.0030, 0.0040, 0.0050],\n",
       "         [0.0040, 0.0050, 0.0060],\n",
       "         [0.0050, 0.0060, 0.0070],\n",
       "         [0.0060, 0.0070, 0.0080]],\n",
       "\n",
       "        [[0.0010, 0.0020, 0.0030],\n",
       "         [0.0020, 0.0030, 0.0040],\n",
       "         [0.0030, 0.0040, 0.0050],\n",
       "         [0.0040, 0.0050, 0.0060],\n",
       "         [0.0050, 0.0060, 0.0070],\n",
       "         [0.0060, 0.0070, 0.0080]],\n",
       "\n",
       "        [[0.0010, 0.0020, 0.0030],\n",
       "         [0.0020, 0.0030, 0.0040],\n",
       "         [0.0030, 0.0040, 0.0050],\n",
       "         [0.0040, 0.0050, 0.0060],\n",
       "         [0.0050, 0.0060, 0.0070],\n",
       "         [0.0060, 0.0070, 0.0080]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tok_emb(x_tokens)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cced203-0a47-42b1-8620-bfe1256e5924",
   "metadata": {},
   "source": [
    "**Collapse the batch**\n",
    "\n",
    "Now we'll remove the batch for further training and learning. If you recall we combined our networks using blocked diagonal so the batch removal now aligns the \"sample\"/\"node\" dimension  with the dimension in the network tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8bd3ea54-13f6-40ac-ae42-8410f2f461c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 3]),\n",
       " tensor([[0.0010, 0.0020, 0.0030],\n",
       "         [0.0020, 0.0030, 0.0040],\n",
       "         [0.0030, 0.0040, 0.0050],\n",
       "         [0.0040, 0.0050, 0.0060],\n",
       "         [0.0050, 0.0060, 0.0070],\n",
       "         [0.0060, 0.0070, 0.0080],\n",
       "         [0.0010, 0.0020, 0.0030],\n",
       "         [0.0020, 0.0030, 0.0040],\n",
       "         [0.0030, 0.0040, 0.0050],\n",
       "         [0.0040, 0.0050, 0.0060],\n",
       "         [0.0050, 0.0060, 0.0070],\n",
       "         [0.0060, 0.0070, 0.0080],\n",
       "         [0.0010, 0.0020, 0.0030],\n",
       "         [0.0020, 0.0030, 0.0040],\n",
       "         [0.0030, 0.0040, 0.0050],\n",
       "         [0.0040, 0.0050, 0.0060],\n",
       "         [0.0050, 0.0060, 0.0070],\n",
       "         [0.0060, 0.0070, 0.0080],\n",
       "         [0.0010, 0.0020, 0.0030],\n",
       "         [0.0020, 0.0030, 0.0040],\n",
       "         [0.0030, 0.0040, 0.0050],\n",
       "         [0.0040, 0.0050, 0.0060],\n",
       "         [0.0050, 0.0060, 0.0070],\n",
       "         [0.0060, 0.0070, 0.0080]], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.view(B_batch*N_nodes,n_embd)\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4175d404-23d4-4866-a72e-8bb4deb125f5",
   "metadata": {},
   "source": [
    "### GCN Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc3b52d-93f6-41b1-9e16-100f5962a9f3",
   "metadata": {},
   "source": [
    "<img src=\"explainer_screenshots/gcn/block_details.png\" width=\"400\">\n",
    "\n",
    "\n",
    "To support our two hops of learning (Cell type > gene > cell type) we stack two signed graph-convolution layers with normalization and nonlinearity to learn increasingly expressive node features. As a reminder, we have split our edges into two different graphs so that we can model them with separate channels and weights. This separation allows the network to learn how to amplify or suppress information along each relation while remaining numerically stable.\n",
    "\n",
    "\n",
    "Each of our graph convolution layers uses a different linear layer for our positive `+` and negative `-` graph connections.  The convolution layer compute the weight as:\n",
    "$$\n",
    "Out =\\hat A^{pos} X_{embd} W_{pos} - \\hat A^{neg} X_{embd} W_{neg}\n",
    "$$\n",
    "where the two linear $W$ layers are learned and shared across nodes.\n",
    "\n",
    "\n",
    "We interleave batch normalization and leaky ReLU to stabilize activations, improve gradient flow, and add nonlinearity. We did choose to use leaky RELU instead of standard ReLU as we want to learn both positive and negative edges so we don't want to just cut off negative values. After our final graph convolution layer and normalization we then use a residual connection to allow for passing through node embeddings and use a final leaky ReLU for normalization. With this design we are able to retain directional regulation(up vs. down) through separate channels, keeps propagation spectrally bounded via normalization, and uses the residual path to support deeper stacks without oversmoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d53c6c-547c-4a04-846d-7215045a655f",
   "metadata": {},
   "source": [
    "#### GCN Block - First Node Convolution\n",
    "\n",
    "Our first convolutional block uses two separate weights for our positive and negative network. We'll start by configuring our weights.  To really highlight the impact we'll use a pyramidal weight structure based on increments of `0.01`. We'll need to initialize both the positive and negative kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22ede61a-d791-4875-a2d0-6b027e8eb69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyramid_weight(h, w, base=0.01, dtype=torch.float32):\n",
    "    r = torch.arange(h, dtype=dtype)\n",
    "    c = torch.arange(w, dtype=dtype)\n",
    "    drow = torch.minimum(r, (h - 1) - r)\n",
    "    dcol = torch.minimum(c, (w - 1) - c)\n",
    "    return base * (1 + drow)[:, None] * (1 + dcol)[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "09ab3c51-5247-409d-a5d5-64e9635abae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[0.0100, 0.0200, 0.0100],\n",
       "         [0.0200, 0.0400, 0.0200],\n",
       "         [0.0100, 0.0200, 0.0100]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1.0000e-06, 1.0000e-06, 1.0000e-06], requires_grad=True))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn_1_pos = torch.nn.Linear(n_embd, n_embd, bias=True)\n",
    "with torch.no_grad(): \n",
    "    gcn_1_pos.weight.copy_(pyramid_weight(n_embd, n_embd))\n",
    "nn.init.constant_(gcn_1_pos.bias, 1e-6)\n",
    "gcn_1_pos.weight, gcn_1_pos.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca56e180-efc0-4a24-a5d4-ae06a507ff47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[0.0100, 0.0200, 0.0100],\n",
       "         [0.0200, 0.0400, 0.0200],\n",
       "         [0.0100, 0.0200, 0.0100]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1.0000e-06, 1.0000e-06, 1.0000e-06], requires_grad=True))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn_1_neg = torch.nn.Linear(n_embd, n_embd, bias=True)\n",
    "with torch.no_grad(): \n",
    "    gcn_1_neg.weight.copy_(pyramid_weight(n_embd, n_embd))\n",
    "nn.init.constant_(gcn_1_neg.bias, 1e-6)\n",
    "gcn_1_neg.weight, gcn_1_neg.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d46498-fcf7-40a2-a4f1-1f1ba7adfe58",
   "metadata": {},
   "source": [
    "**Positive Convolution**\n",
    "\n",
    "We'll start by taking the dot product of the network and positive node embeddings $\\hat A^{pos} X_{embd}$.  With this you'll notice that even though our embedded input has a repetition of `[6,3]` for each sample, because of the network tensor multiplication. If you look closely though you will notice that the difference between columns is consistent for each row.  \n",
    "\n",
    "After taking our dot product we then multiply our weights completing the positive product $\\hat A^{pos} X_{embd} W_{pos}$.  This linearization allows the network to learn how much to learn from the positive edge.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3cabdbe-8b34-4bb4-9b7f-65f884c1d85f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0023, 0.0031, 0.0040],\n",
       "        [0.0028, 0.0036, 0.0045],\n",
       "        [0.0033, 0.0041, 0.0050],\n",
       "        [0.0050, 0.0060, 0.0070],\n",
       "        [0.0034, 0.0047, 0.0060],\n",
       "        [0.0050, 0.0060, 0.0070],\n",
       "        [0.0037, 0.0047, 0.0056],\n",
       "        [0.0031, 0.0040, 0.0048],\n",
       "        [0.0035, 0.0044, 0.0054],\n",
       "        [0.0041, 0.0050, 0.0058],\n",
       "        [0.0032, 0.0043, 0.0054],\n",
       "        [0.0039, 0.0052, 0.0064],\n",
       "        [0.0029, 0.0039, 0.0048],\n",
       "        [0.0020, 0.0030, 0.0040],\n",
       "        [0.0040, 0.0050, 0.0060],\n",
       "        [0.0044, 0.0054, 0.0063],\n",
       "        [0.0040, 0.0050, 0.0060],\n",
       "        [0.0040, 0.0052, 0.0063],\n",
       "        [0.0010, 0.0020, 0.0030],\n",
       "        [0.0035, 0.0045, 0.0055],\n",
       "        [0.0039, 0.0049, 0.0058],\n",
       "        [0.0044, 0.0054, 0.0063],\n",
       "        [0.0035, 0.0045, 0.0055],\n",
       "        [0.0049, 0.0060, 0.0072]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_pos = Apos_blk @ x\n",
    "H_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b506aa8-0481-4a7d-b67b-5504b6f2960e",
   "metadata": {},
   "source": [
    "When we multiply our pyramidal weights you can see that that brings a similar pyramidal pattern to our data where the middle column tends to be elevated 2x above the side columns since the  weights are 2x in the middle column compared to the edge columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48fdd038-773d-4b0b-b515-2b132bba05e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2585e-04, 2.5071e-04, 1.2585e-04],\n",
       "        [1.4585e-04, 2.9071e-04, 1.4585e-04],\n",
       "        [1.6585e-04, 3.3071e-04, 1.6585e-04],\n",
       "        [2.4100e-04, 4.8100e-04, 2.4100e-04],\n",
       "        [1.8828e-04, 3.7556e-04, 1.8828e-04],\n",
       "        [2.4100e-04, 4.8100e-04, 2.4100e-04],\n",
       "        [1.8850e-04, 3.7599e-04, 1.8850e-04],\n",
       "        [1.5999e-04, 3.1899e-04, 1.5999e-04],\n",
       "        [1.7898e-04, 3.5696e-04, 1.7898e-04],\n",
       "        [1.9999e-04, 3.9899e-04, 1.9999e-04],\n",
       "        [1.7299e-04, 3.4497e-04, 1.7299e-04],\n",
       "        [2.0723e-04, 4.1346e-04, 2.0723e-04],\n",
       "        [1.5531e-04, 3.0962e-04, 1.5531e-04],\n",
       "        [1.2100e-04, 2.4100e-04, 1.2100e-04],\n",
       "        [2.0100e-04, 4.0100e-04, 2.0100e-04],\n",
       "        [2.1531e-04, 4.2962e-04, 2.1531e-04],\n",
       "        [2.0100e-04, 4.0100e-04, 2.0100e-04],\n",
       "        [2.0864e-04, 4.1629e-04, 2.0864e-04],\n",
       "        [8.1000e-05, 1.6100e-04, 8.1000e-05],\n",
       "        [1.8100e-04, 3.6100e-04, 1.8100e-04],\n",
       "        [1.9531e-04, 3.8962e-04, 1.9531e-04],\n",
       "        [2.1531e-04, 4.2962e-04, 2.1531e-04],\n",
       "        [1.8100e-04, 3.6100e-04, 1.8100e-04],\n",
       "        [2.4130e-04, 4.8161e-04, 2.4130e-04]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_pos =  gcn_1_pos(H_pos)\n",
    "H_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2a4a68-38a3-4406-be45-f5c921f797f1",
   "metadata": {},
   "source": [
    "**Negative Convolution**\n",
    "\n",
    "We'll next take the dot product of the network and negative node embeddings $\\hat A^{neg} X_{embd}$.  With this you'll notice that like the positive multiplication, even though our embedded input has a repetition of `[6,3]` for each sample, because of the network tensor multiplication. If you look closely though you will notice that the difference between columns is consistent for each row.  Another thing you'll also notice is a couple `0` rows. These are nodes where there is no down-regulation. \n",
    "\n",
    "After taking our dot product we then multiply our weights completing the negative product $\\hat A^{neg} X_{embd} W_{neg}$.  This linearization allows the network to learn how much to learn from the negative edge.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6376c13-6c16-4b4b-b1dc-5cd80a312796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0035, 0.0040, 0.0046],\n",
       "        [0.0035, 0.0040, 0.0046],\n",
       "        [0.0035, 0.0040, 0.0046],\n",
       "        [0.0050, 0.0060, 0.0070],\n",
       "        [0.0040, 0.0050, 0.0060],\n",
       "        [0.0035, 0.0052, 0.0069],\n",
       "        [0.0000, 0.0000, 0.0000],\n",
       "        [0.0035, 0.0042, 0.0049],\n",
       "        [0.0060, 0.0070, 0.0080],\n",
       "        [0.0035, 0.0042, 0.0049],\n",
       "        [0.0042, 0.0057, 0.0071],\n",
       "        [0.0030, 0.0040, 0.0050],\n",
       "        [0.0029, 0.0035, 0.0040],\n",
       "        [0.0050, 0.0059, 0.0069],\n",
       "        [0.0042, 0.0049, 0.0057],\n",
       "        [0.0029, 0.0035, 0.0040],\n",
       "        [0.0037, 0.0053, 0.0068],\n",
       "        [0.0031, 0.0043, 0.0055],\n",
       "        [0.0050, 0.0059, 0.0069],\n",
       "        [0.0042, 0.0049, 0.0057],\n",
       "        [0.0029, 0.0035, 0.0040],\n",
       "        [0.0029, 0.0035, 0.0040],\n",
       "        [0.0044, 0.0060, 0.0076],\n",
       "        [0.0019, 0.0031, 0.0043]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_neg = Aneg_blk @ x\n",
    "H_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d993ece7-93ff-41d7-b790-aa3010914188",
   "metadata": {},
   "source": [
    "Again, when we multiply our pyramidal weights you can see that that brings a similar pyramidal pattern to our data where the middle column tends to be elevated 2x above the side columns since the  weights are 2x in the middle column compared to the edge columns.  Additionally you'll see the impact of bias come in here.  Recall that we set our bias to 1e-6 to prevent zeroing out.  With this, our 0 rows become the epsilon.  This helps the model learn about the nodes even if this particular learning batch does not have them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3989df0d-ccfc-42c1-8e2f-e2ba23e89560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6266e-04, 3.2432e-04, 1.6266e-04],\n",
       "        [1.6266e-04, 3.2432e-04, 1.6266e-04],\n",
       "        [1.6266e-04, 3.2432e-04, 1.6266e-04],\n",
       "        [2.4100e-04, 4.8100e-04, 2.4100e-04],\n",
       "        [2.0100e-04, 4.0100e-04, 2.0100e-04],\n",
       "        [2.0885e-04, 4.1669e-04, 2.0885e-04],\n",
       "        [1.0000e-06, 1.0000e-06, 1.0000e-06],\n",
       "        [1.7071e-04, 3.4041e-04, 1.7071e-04],\n",
       "        [2.8100e-04, 5.6100e-04, 2.8100e-04],\n",
       "        [1.7071e-04, 3.4041e-04, 1.7071e-04],\n",
       "        [2.2727e-04, 4.5355e-04, 2.2727e-04],\n",
       "        [1.6100e-04, 3.2100e-04, 1.6100e-04],\n",
       "        [1.3956e-04, 2.7813e-04, 1.3956e-04],\n",
       "        [2.3898e-04, 4.7696e-04, 2.3898e-04],\n",
       "        [1.9899e-04, 3.9698e-04, 1.9899e-04],\n",
       "        [1.3956e-04, 2.7813e-04, 1.3956e-04],\n",
       "        [2.1165e-04, 4.2230e-04, 2.1165e-04],\n",
       "        [1.7414e-04, 3.4727e-04, 1.7414e-04],\n",
       "        [2.3898e-04, 4.7696e-04, 2.3898e-04],\n",
       "        [1.9899e-04, 3.9698e-04, 1.9899e-04],\n",
       "        [1.3956e-04, 2.7813e-04, 1.3956e-04],\n",
       "        [1.3956e-04, 2.7813e-04, 1.3956e-04],\n",
       "        [2.4151e-04, 4.8201e-04, 2.4151e-04],\n",
       "        [1.2585e-04, 2.5071e-04, 1.2585e-04]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_neg =  gcn_1_neg(H_neg)\n",
    "H_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178463f7-b4e3-4bba-82b7-ace48ac0d0e5",
   "metadata": {},
   "source": [
    "**Combine weights**\n",
    "\n",
    "Now that we have each sign's convolution, we need to add them together.  To emphasize the excitatory and inhibitory nature of the two different networks we subtract the negative weight from the positive as follows:\n",
    "\n",
    "$Out =\\hat A^{pos} X_{embd} W_{pos} - \\hat A^{neg} X_{embd} W_{neg}$\n",
    "\n",
    "Notice that our values are a mix of positive and negative values. This shows that our model, even in its initiation state, has started building a model of genes and cell relationships. This is likely not correct but backprop will fix it.  Also that the pattern of the middle channel being 2x the side channels is maintained. This will become important in the normalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b32d879b-d83f-4a7a-85f0-3747953445cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 3]),\n",
       " tensor([[-3.6805e-05, -7.3611e-05, -3.6805e-05],\n",
       "         [-1.6805e-05, -3.3611e-05, -1.6805e-05],\n",
       "         [ 3.1947e-06,  6.3895e-06,  3.1947e-06],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [-1.2721e-05, -2.5442e-05, -1.2721e-05],\n",
       "         [ 3.2154e-05,  6.4308e-05,  3.2154e-05],\n",
       "         [ 1.8750e-04,  3.7499e-04,  1.8750e-04],\n",
       "         [-1.0711e-05, -2.1421e-05, -1.0711e-05],\n",
       "         [-1.0202e-04, -2.0404e-04, -1.0202e-04],\n",
       "         [ 2.9289e-05,  5.8579e-05,  2.9289e-05],\n",
       "         [-5.4288e-05, -1.0858e-04, -5.4288e-05],\n",
       "         [ 4.6231e-05,  9.2462e-05,  4.6231e-05],\n",
       "         [ 1.5745e-05,  3.1491e-05,  1.5745e-05],\n",
       "         [-1.1798e-04, -2.3596e-04, -1.1798e-04],\n",
       "         [ 2.0101e-06,  4.0202e-06,  2.0101e-06],\n",
       "         [ 7.5745e-05,  1.5149e-04,  7.5745e-05],\n",
       "         [-1.0648e-05, -2.1296e-05, -1.0648e-05],\n",
       "         [ 3.4506e-05,  6.9012e-05,  3.4506e-05],\n",
       "         [-1.5798e-04, -3.1596e-04, -1.5798e-04],\n",
       "         [-1.7990e-05, -3.5980e-05, -1.7990e-05],\n",
       "         [ 5.5745e-05,  1.1149e-04,  5.5745e-05],\n",
       "         [ 7.5745e-05,  1.5149e-04,  7.5745e-05],\n",
       "         [-6.0506e-05, -1.2101e-04, -6.0506e-05],\n",
       "         [ 1.1545e-04,  2.3090e-04,  1.1545e-04]], grad_fn=<SubBackward0>))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = H_pos - H_neg\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba4d34e-9b7a-4011-93e6-94a5bfa06818",
   "metadata": {},
   "source": [
    "#### GCN Block - First Layer Norm\n",
    "\n",
    "With Layer normalization, we review the row and adjust based on how far away it is from the mean. This means an array of `[1,2,3]` and `[2,4,6]` will actually have the same normalized entries after layer normalization.  This layer adds regularization which helps with overall learning speed. The formula applied is:\n",
    "\n",
    "$y = \\frac{x - \\mathbb{E}[x]}{\\sqrt{\\operatorname{Var}[x] + \\epsilon}}$\n",
    "\n",
    "Layer normalization here is applied on the output of the first graph convolution layer across each node.  You'll see that because our middle node is 2 the side nodes that create an interesting pattern emerge, when the node is negative, the middle becomes more negative and the side positive.  The inverse happens when it is positive.  This helps show how normalization works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea70b76b-1aa7-4984-8a65-001a77833b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1.0000e-06, 1.0000e-06, 1.0000e-06], requires_grad=True))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_a = nn.LayerNorm(n_embd)\n",
    "nn.init.constant_(bn_a.bias, 1e-6) # prevent 0 crashout\n",
    "bn_a.weight, bn_a.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "082584d3-04d6-48d9-a5b9-d151ebf8ca04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 3]),\n",
       " tensor([[ 3.8806e-03, -7.7581e-03,  3.8806e-03],\n",
       "         [ 1.7724e-03, -3.5419e-03,  1.7724e-03],\n",
       "         [-3.3576e-04,  6.7451e-04, -3.3576e-04],\n",
       "         [ 1.0000e-06,  1.0000e-06,  1.0000e-06],\n",
       "         [ 1.3419e-03, -2.6808e-03,  1.3419e-03],\n",
       "         [-3.3883e-03,  6.7796e-03, -3.3883e-03],\n",
       "         [-1.9755e-02,  3.9513e-02, -1.9755e-02],\n",
       "         [ 1.1300e-03, -2.2570e-03,  1.1300e-03],\n",
       "         [ 1.0754e-02, -2.1504e-02,  1.0754e-02],\n",
       "         [-3.0863e-03,  6.1757e-03, -3.0863e-03],\n",
       "         [ 5.7232e-03, -1.1443e-02,  5.7232e-03],\n",
       "         [-4.8721e-03,  9.7471e-03, -4.8721e-03],\n",
       "         [-1.6587e-03,  3.3204e-03, -1.6587e-03],\n",
       "         [ 1.2435e-02, -2.4867e-02,  1.2435e-02],\n",
       "         [-2.1088e-04,  4.2477e-04, -2.1088e-04],\n",
       "         [-7.9828e-03,  1.5969e-02, -7.9828e-03],\n",
       "         [ 1.1234e-03, -2.2438e-03,  1.1234e-03],\n",
       "         [-3.6362e-03,  7.2754e-03, -3.6362e-03],\n",
       "         [ 1.6649e-02, -3.3295e-02,  1.6649e-02],\n",
       "         [ 1.8973e-03, -3.7916e-03,  1.8973e-03],\n",
       "         [-5.8749e-03,  1.1753e-02, -5.8749e-03],\n",
       "         [-7.9828e-03,  1.5969e-02, -7.9828e-03],\n",
       "         [ 6.3786e-03, -1.2754e-02,  6.3786e-03],\n",
       "         [-1.2167e-02,  2.4336e-02, -1.2167e-02]],\n",
       "        grad_fn=<NativeLayerNormBackward0>))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = bn_a(out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64bccc5-6c49-4685-91b2-a0a9e58e58bf",
   "metadata": {},
   "source": [
    "#### GCN Block - Leaky ReLU\n",
    "Next we apply leaky ReLU (rectified linear units), an element-wise nonlinearity that leaves positive activations unchanged and scales negative activations by a small factor, in our case `0.1`. In graph networks it is commonly used after normalization to introduce nonlinearity while preserving gradient flow through negative values and avoiding “dying units.” Compared to ReLU, LeakyReLU produces less sparse activations but better retains informative negative signals (useful when inhibitory effects matter).  We also prefer LeakyRELU in this graph since we are building a signed graph.  \n",
    "\n",
    "The formula applied is:\n",
    "$$\n",
    "y=\\max(\\alpha x,x)=\n",
    "\\begin{cases}\n",
    "x,& x>0\\\\\n",
    "\\alpha x,& x\\leq 0\n",
    "\\end{cases} \\quad\n",
    "\\frac{\\partial y}{\\partial x}=\n",
    "\\begin{cases}\n",
    "1,& x>0 \\\\\n",
    "\\alpha,& x\\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "LeakyReLU is stateless, meaning it has no learnable parameters. The only control is the hyperparameter (\\alpha))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2172634c-3dfc-48d6-b751-df5e972f007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrelu_1= nn.LeakyReLU(negative_slope=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e6d9fdbe-f361-4e1a-8813-2c67a7492b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 3]),\n",
       " tensor([[ 3.8806e-03, -7.7581e-04,  3.8806e-03],\n",
       "         [ 1.7724e-03, -3.5419e-04,  1.7724e-03],\n",
       "         [-3.3576e-05,  6.7451e-04, -3.3576e-05],\n",
       "         [ 1.0000e-06,  1.0000e-06,  1.0000e-06],\n",
       "         [ 1.3419e-03, -2.6808e-04,  1.3419e-03],\n",
       "         [-3.3883e-04,  6.7796e-03, -3.3883e-04],\n",
       "         [-1.9755e-03,  3.9513e-02, -1.9755e-03],\n",
       "         [ 1.1300e-03, -2.2570e-04,  1.1300e-03],\n",
       "         [ 1.0754e-02, -2.1504e-03,  1.0754e-02],\n",
       "         [-3.0863e-04,  6.1757e-03, -3.0863e-04],\n",
       "         [ 5.7232e-03, -1.1443e-03,  5.7232e-03],\n",
       "         [-4.8721e-04,  9.7471e-03, -4.8721e-04],\n",
       "         [-1.6587e-04,  3.3204e-03, -1.6587e-04],\n",
       "         [ 1.2435e-02, -2.4867e-03,  1.2435e-02],\n",
       "         [-2.1088e-05,  4.2477e-04, -2.1088e-05],\n",
       "         [-7.9828e-04,  1.5969e-02, -7.9828e-04],\n",
       "         [ 1.1234e-03, -2.2438e-04,  1.1234e-03],\n",
       "         [-3.6362e-04,  7.2754e-03, -3.6362e-04],\n",
       "         [ 1.6649e-02, -3.3295e-03,  1.6649e-02],\n",
       "         [ 1.8973e-03, -3.7916e-04,  1.8973e-03],\n",
       "         [-5.8749e-04,  1.1753e-02, -5.8749e-04],\n",
       "         [-7.9828e-04,  1.5969e-02, -7.9828e-04],\n",
       "         [ 6.3786e-03, -1.2754e-03,  6.3786e-03],\n",
       "         [-1.2167e-03,  2.4336e-02, -1.2167e-03]], grad_fn=<LeakyReluBackward0>))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = lrelu_1(out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2b1898-8e4f-45b0-822d-437e2f1f00ec",
   "metadata": {},
   "source": [
    "#### GCN Block - Dropout\n",
    "\n",
    "Finally, before our next node convolution, we'll perform dropout. Dropout will randomly zero out any value effectively removing that specific node from impacting prediction. Since this is Bernoulli based dropout, in addition to zeroing out weights the surviving entries are scaled by $1/(1-p)$. During training this helps with generalization and fights fixation. You can quickly see the dropout's impact on the embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b3f77a00-b595-4246-bcc7-9fc01081921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = torch.nn.Dropout(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ffb72b23-dca0-48d3-a165-fa739c284d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 3]),\n",
       " tensor([[ 0.0000e+00, -8.1664e-04,  4.0848e-03],\n",
       "         [ 0.0000e+00, -3.7283e-04,  1.8657e-03],\n",
       "         [-0.0000e+00,  7.1001e-04, -3.5343e-05],\n",
       "         [ 1.0526e-06,  1.0526e-06,  1.0526e-06],\n",
       "         [ 1.4125e-03, -2.8219e-04,  1.4125e-03],\n",
       "         [-3.5666e-04,  7.1364e-03, -0.0000e+00],\n",
       "         [-2.0795e-03,  4.1593e-02, -2.0795e-03],\n",
       "         [ 1.1895e-03, -0.0000e+00,  1.1895e-03],\n",
       "         [ 1.1320e-02, -2.2636e-03,  0.0000e+00],\n",
       "         [-3.2488e-04,  6.5007e-03, -3.2488e-04],\n",
       "         [ 6.0245e-03, -0.0000e+00,  6.0245e-03],\n",
       "         [-5.1285e-04,  1.0260e-02, -5.1285e-04],\n",
       "         [-1.7460e-04,  3.4952e-03, -1.7460e-04],\n",
       "         [ 1.3090e-02, -2.6176e-03,  1.3090e-02],\n",
       "         [-2.2198e-05,  4.4712e-04, -2.2198e-05],\n",
       "         [-8.4029e-04,  1.6809e-02, -8.4029e-04],\n",
       "         [ 1.1825e-03, -2.3619e-04,  1.1825e-03],\n",
       "         [-3.8276e-04,  7.6583e-03, -3.8276e-04],\n",
       "         [ 1.7525e-02, -3.5047e-03,  1.7525e-02],\n",
       "         [ 1.9972e-03, -3.9911e-04,  1.9972e-03],\n",
       "         [-6.1841e-04,  1.2371e-02, -6.1841e-04],\n",
       "         [-8.4029e-04,  1.6809e-02, -8.4029e-04],\n",
       "         [ 6.7143e-03, -1.3426e-03,  6.7143e-03],\n",
       "         [-1.2807e-03,  2.5617e-02, -1.2807e-03]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = drop(out) \n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f13cfb3-a77c-4d3d-9860-4d05742d5bab",
   "metadata": {},
   "source": [
    "#### GCN Block - Second Node Convolution\n",
    "\n",
    "Our next convolutional block has the same dimensions as our first and still uses two separate weights for our positive and negative network. Instead of passing in the input embedding though we'll pass in the output of dropout allowing the model to learn the \"next hop\". Just like in the first convolution, to really highlight the impact we'll use a pyramidal weight structure based on increments of `0.05`. We'll need to initialize both the positive and negative weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dd69a287-748e-4624-bb92-afea08e877ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[0.0500, 0.1000, 0.0500],\n",
       "         [0.1000, 0.2000, 0.1000],\n",
       "         [0.0500, 0.1000, 0.0500]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1.0000e-06, 1.0000e-06, 1.0000e-06], requires_grad=True))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn_2_pos = torch.nn.Linear(n_embd, n_embd, bias=True)\n",
    "with torch.no_grad(): \n",
    "    gcn_2_pos.weight.copy_(pyramid_weight(n_embd, n_embd, base=0.05))\n",
    "nn.init.constant_(gcn_2_pos.bias, 1e-6)\n",
    "gcn_2_pos.weight, gcn_2_pos.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d768529f-4907-4112-ae4d-4ef3214b482d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[0.0500, 0.1000, 0.0500],\n",
       "         [0.1000, 0.2000, 0.1000],\n",
       "         [0.0500, 0.1000, 0.0500]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1.0000e-06, 1.0000e-06, 1.0000e-06], requires_grad=True))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn_2_neg = torch.nn.Linear(n_embd, n_embd, bias=True)\n",
    "with torch.no_grad(): \n",
    "    gcn_2_neg.weight.copy_(pyramid_weight(n_embd, n_embd, base=0.05))\n",
    "nn.init.constant_(gcn_2_neg.bias, 1e-6)\n",
    "gcn_2_neg.weight, gcn_2_neg.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e230afd5-6de1-497b-86de-d611fb6c6579",
   "metadata": {},
   "source": [
    "**Positive Convolution**\n",
    "\n",
    "We'll again start by taking the dot product of the positive node embeddings and the current inflight weights $\\hat A^{pos} out$.  At this point you'll notice that the patterns in our weights have largely disappeared though in our dot product, because of the layer normalization, we still see the last embedding channel is still negative. This shows just how different this convolution layer is over the previous one.  \n",
    "\n",
    "Just as before, after taking our dot product we then multiply our weights completing the positive product $\\hat A^{pos} X_{embd} W_{pos}$.  This linearization allows the network to learn how much to learn from the positive edge.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8f1a06c9-d6e9-4659-ab6f-80bbcdd7713a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 3]),\n",
       " tensor([[ 4.9940e-04, -5.0809e-04,  2.5418e-03],\n",
       "         [ 4.9940e-04, -2.8618e-04,  1.4323e-03],\n",
       "         [ 4.9940e-04,  2.5524e-04,  4.8173e-04],\n",
       "         [-1.7780e-04,  3.5687e-03,  5.2632e-07],\n",
       "         [ 3.5313e-04, -2.4006e-04,  2.4445e-03],\n",
       "         [-1.7780e-04,  3.5687e-03,  5.2632e-07],\n",
       "         [ 1.1669e-03,  1.6826e-02,  1.1669e-03],\n",
       "         [ 4.1342e-04,  3.6275e-03,  4.1342e-04],\n",
       "         [ 8.1193e-03, -1.1318e-03,  2.4595e-03],\n",
       "         [-3.4376e-04,  6.8779e-03, -3.4376e-04],\n",
       "         [ 5.9362e-03,  1.2940e-02,  1.3150e-03],\n",
       "         [-4.2282e-04,  1.6870e-02, -4.2282e-04],\n",
       "         [-2.4356e-04,  4.8741e-03, -2.4356e-04],\n",
       "         [ 1.3090e-02, -2.6176e-03,  1.3090e-02],\n",
       "         [ 5.8015e-04,  1.0547e-04,  5.8015e-04],\n",
       "         [-5.7640e-04,  1.1531e-02, -5.7640e-04],\n",
       "         [ 5.8015e-04,  1.0547e-04,  5.8015e-04],\n",
       "         [-5.4191e-04,  1.0842e-02, -5.4191e-04],\n",
       "         [ 1.7525e-02, -3.5047e-03,  1.7525e-02],\n",
       "         [ 4.3558e-03, -8.7083e-04,  4.3558e-03],\n",
       "         [-8.3205e-04,  1.6644e-02, -8.3205e-04],\n",
       "         [-9.4299e-04,  1.8863e-02, -9.4299e-04],\n",
       "         [ 4.3558e-03, -8.7083e-04,  4.3558e-03],\n",
       "         [-1.0224e-03,  2.0452e-02, -1.0224e-03]], grad_fn=<MmBackward0>))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H2_pos = Apos_blk @ out\n",
    "H2_pos.size(), H2_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864f9c08-4702-4e71-adbc-c30758e3f98d",
   "metadata": {},
   "source": [
    "Now, when we multiply our pyramidal weights you can see that that despite the previous weights looking all over the place, the output returns a similar pyramidal pattern to our data where the middle column tends to be elevated 2x above the side columns since the  weights are 2x in the middle column compared to the edge columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1bd3a8fb-5d1c-4d27-ae15-7c5e42a700ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0225e-04, 2.0350e-04, 1.0225e-04],\n",
       "        [6.8965e-05, 1.3693e-04, 6.8965e-05],\n",
       "        [7.5580e-05, 1.5016e-04, 7.5580e-05],\n",
       "        [3.4901e-04, 6.9702e-04, 3.4901e-04],\n",
       "        [1.1687e-04, 2.3275e-04, 1.1687e-04],\n",
       "        [3.4901e-04, 6.9702e-04, 3.4901e-04],\n",
       "        [1.8003e-03, 3.5996e-03, 1.8003e-03],\n",
       "        [4.0509e-04, 8.0919e-04, 4.0509e-04],\n",
       "        [4.1676e-04, 8.3252e-04, 4.1676e-04],\n",
       "        [6.5441e-04, 1.3078e-03, 6.5441e-04],\n",
       "        [1.6576e-03, 3.3141e-03, 1.6576e-03],\n",
       "        [1.6457e-03, 3.2905e-03, 1.6457e-03],\n",
       "        [4.6405e-04, 9.2710e-04, 4.6405e-04],\n",
       "        [1.0482e-03, 2.0954e-03, 1.0482e-03],\n",
       "        [6.9562e-05, 1.3812e-04, 6.9562e-05],\n",
       "        [1.0965e-03, 2.1919e-03, 1.0965e-03],\n",
       "        [6.9562e-05, 1.3812e-04, 6.9562e-05],\n",
       "        [1.0310e-03, 2.0610e-03, 1.0310e-03],\n",
       "        [1.4030e-03, 2.8051e-03, 1.4030e-03],\n",
       "        [3.4949e-04, 6.9798e-04, 3.4949e-04],\n",
       "        [1.5822e-03, 3.1634e-03, 1.5822e-03],\n",
       "        [1.7930e-03, 3.5849e-03, 1.7930e-03],\n",
       "        [3.4949e-04, 6.9798e-04, 3.4949e-04],\n",
       "        [1.9439e-03, 3.8869e-03, 1.9439e-03]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H2_pos =  gcn_2_pos(H2_pos)\n",
    "H2_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2516d6c6-b18c-454a-bead-ed2a80631f43",
   "metadata": {},
   "source": [
    "**Negative Convolution**\n",
    "\n",
    "We'll next take the dot product of the negative node embeddings and the current inflight weights $\\hat A^{neg} X_{embd}$.  With this you'll notice that like the positive multiplication, you'll notice that the patterns in our weights have largely disappeared though in our dot product, because of the layer normalization, we still see the last embedding channel is still negative. This shows just how different this convolution layer is over the previous one.  Another thing you'll also notice is the return of `0` rows. These return for the same reason as before, there are nodes where there is no down-regulation. \n",
    "\n",
    "Just as before, after taking our dot product we then multiply our weights completing the negative product $\\hat A^{neg} X_{embd} W_{neg}$.  This linearization allows the network to learn how much to learn from the negative edge.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "647fe12e-ea0a-43de-8a2d-aeabe682f3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 3]),\n",
       " tensor([[-2.0592e-04,  4.1202e-03,  0.0000e+00],\n",
       "         [-2.0592e-04,  4.1202e-03,  0.0000e+00],\n",
       "         [-2.0592e-04,  4.1202e-03,  0.0000e+00],\n",
       "         [ 1.4125e-03, -2.8219e-04,  1.4125e-03],\n",
       "         [ 1.0526e-06,  1.0526e-06,  1.0526e-06],\n",
       "         [ 0.0000e+00, -2.7682e-04,  3.4151e-03],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 4.2599e-03,  0.0000e+00,  4.2599e-03],\n",
       "         [-5.1285e-04,  1.0260e-02, -5.1285e-04],\n",
       "         [ 4.2599e-03,  0.0000e+00,  4.2599e-03],\n",
       "         [ 6.1137e-04,  4.5967e-03,  6.1137e-04],\n",
       "         [ 1.1320e-02, -2.2636e-03,  0.0000e+00],\n",
       "         [ 6.8272e-04, -1.3636e-04,  6.8272e-04],\n",
       "         [ 2.9138e-04,  3.7327e-03,  2.9138e-04],\n",
       "         [-2.7065e-04,  5.4152e-03, -2.7065e-04],\n",
       "         [ 6.8272e-04, -1.3636e-04,  6.8272e-04],\n",
       "         [ 4.7579e-03,  1.0654e-02,  4.7579e-03],\n",
       "         [ 6.5292e-03, -9.9265e-04,  6.5292e-03],\n",
       "         [ 2.1008e-03,  1.2261e-02,  2.1008e-03],\n",
       "         [-9.0559e-04,  1.8114e-02, -9.0559e-04],\n",
       "         [ 3.8765e-03, -7.7512e-04,  3.8765e-03],\n",
       "         [ 3.8765e-03, -7.7512e-04,  3.8765e-03],\n",
       "         [ 6.3124e-03,  1.5416e-02,  6.3124e-03],\n",
       "         [ 1.0175e-02, -2.0346e-03,  1.0175e-02]], grad_fn=<MmBackward0>))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H2_neg = Aneg_blk @ out\n",
    "H2_neg.size(), H2_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b55b27-2e61-49e5-8dee-659ba1431361",
   "metadata": {},
   "source": [
    "Again, when we multiply our pyramidal weights you can see that that brings a similar pyramidal pattern to our data where the middle column tends to be elevated 2x above the side columns since the  weights are 2x in the middle column compared to the edge columns.  Additionally you'll see the impact of bias come in here.  Recall that we set our bias to 1e-6 to prevent zeroing out.  With this, our 0 rows become the epsilon.  This helps the model learn about the nodes even if this particular learning batch does not have them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1d110dd1-5ec8-4118-8547-16986b91e6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.0272e-04, 8.0445e-04, 4.0272e-04],\n",
       "        [4.0272e-04, 8.0445e-04, 4.0272e-04],\n",
       "        [4.0272e-04, 8.0445e-04, 4.0272e-04],\n",
       "        [1.1403e-04, 2.2707e-04, 1.1403e-04],\n",
       "        [1.2105e-06, 1.4211e-06, 1.2105e-06],\n",
       "        [1.4407e-04, 2.8715e-04, 1.4407e-04],\n",
       "        [1.0000e-06, 1.0000e-06, 1.0000e-06],\n",
       "        [4.2699e-04, 8.5299e-04, 4.2699e-04],\n",
       "        [9.7573e-04, 1.9505e-03, 9.7573e-04],\n",
       "        [4.2699e-04, 8.5299e-04, 4.2699e-04],\n",
       "        [5.2181e-04, 1.0426e-03, 5.2181e-04],\n",
       "        [3.4062e-04, 6.8024e-04, 3.4062e-04],\n",
       "        [5.5636e-05, 1.1027e-04, 5.5636e-05],\n",
       "        [4.0341e-04, 8.0582e-04, 4.0341e-04],\n",
       "        [5.1546e-04, 1.0299e-03, 5.1546e-04],\n",
       "        [5.5636e-05, 1.1027e-04, 5.5636e-05],\n",
       "        [1.5422e-03, 3.0834e-03, 1.5422e-03],\n",
       "        [5.5465e-04, 1.1083e-03, 5.5465e-04],\n",
       "        [1.4371e-03, 2.8733e-03, 1.4371e-03],\n",
       "        [1.7219e-03, 3.4427e-03, 1.7219e-03],\n",
       "        [3.1114e-04, 6.2128e-04, 3.1114e-04],\n",
       "        [3.1114e-04, 6.2128e-04, 3.1114e-04],\n",
       "        [2.1739e-03, 4.3468e-03, 2.1739e-03],\n",
       "        [8.1502e-04, 1.6290e-03, 8.1502e-04]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H2_neg =  gcn_2_neg(H2_neg)\n",
    "H2_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a9e89a-2285-43d7-a8c2-911501d70a1d",
   "metadata": {},
   "source": [
    "**Combine weights**\n",
    "\n",
    "Now that we have each sign's convolution output, we again add them together.  To emphasize the excitatory and inhibitory nature of the two different networks we subtract the negative weight from the positive as follows:\n",
    "\n",
    "$Out =\\hat A^{pos} X_{embd} W_{pos} - \\hat A^{neg} X_{embd} W_{neg}$\n",
    "\n",
    "Notice that we repeat the same patters we saw before. Our values are a mix of positive and negative values. This shows that our model, even in its initiation state, has started building a model of genes and cell relationships. This is likely not correct but backprop will fix it.  Also that the pattern of the middle channel being 2x the side channels is maintained. This will become important in the normalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fd78b77f-b662-438e-8cec-ed85fd5ff832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 3]),\n",
       " tensor([[-3.0047e-04, -6.0094e-04, -3.0047e-04],\n",
       "         [-3.3376e-04, -6.6752e-04, -3.3376e-04],\n",
       "         [-3.2714e-04, -6.5429e-04, -3.2714e-04],\n",
       "         [ 2.3497e-04,  4.6995e-04,  2.3497e-04],\n",
       "         [ 1.1566e-04,  2.3133e-04,  1.1566e-04],\n",
       "         [ 2.0493e-04,  4.0987e-04,  2.0493e-04],\n",
       "         [ 1.7993e-03,  3.5986e-03,  1.7993e-03],\n",
       "         [-2.1901e-05, -4.3803e-05, -2.1901e-05],\n",
       "         [-5.5897e-04, -1.1179e-03, -5.5897e-04],\n",
       "         [ 2.2742e-04,  4.5483e-04,  2.2742e-04],\n",
       "         [ 1.1358e-03,  2.2715e-03,  1.1358e-03],\n",
       "         [ 1.3051e-03,  2.6102e-03,  1.3051e-03],\n",
       "         [ 4.0841e-04,  8.1683e-04,  4.0841e-04],\n",
       "         [ 6.4480e-04,  1.2896e-03,  6.4480e-04],\n",
       "         [-4.4589e-04, -8.9179e-04, -4.4589e-04],\n",
       "         [ 1.0408e-03,  2.0816e-03,  1.0408e-03],\n",
       "         [-1.4726e-03, -2.9453e-03, -1.4726e-03],\n",
       "         [ 4.7635e-04,  9.5269e-04,  4.7635e-04],\n",
       "         [-3.4085e-05, -6.8170e-05, -3.4085e-05],\n",
       "         [-1.3724e-03, -2.7447e-03, -1.3724e-03],\n",
       "         [ 1.2710e-03,  2.5421e-03,  1.2710e-03],\n",
       "         [ 1.4818e-03,  2.9637e-03,  1.4818e-03],\n",
       "         [-1.8244e-03, -3.6488e-03, -1.8244e-03],\n",
       "         [ 1.1289e-03,  2.2579e-03,  1.1289e-03]], grad_fn=<SubBackward0>))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = H2_pos - H2_neg\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940314cd-a4f7-4399-9720-e18342e153c6",
   "metadata": {},
   "source": [
    "#### GCN Block - Second Layer Norm\n",
    "\n",
    "Again we run layer normalization, Recall that we maintained a pattern where our middle channel was 2x the side channels.  During layer normalization we'll see this balance out\n",
    "\n",
    "Again layer normalization here is applied on the output of the second graph convolution layer across each node.  You'll see that because our middle node is 2 the side nodes that create an interesting pattern emerge, when the node is negative, the middle becomes more negative and the side positive.  The inverse happens when it is positive.  This helps reemphasize how normalization works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8fb8a26b-1dc2-4ec8-97a4-5f227abf9cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1.0000e-06, 1.0000e-06, 1.0000e-06], requires_grad=True))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_b = nn.LayerNorm(n_embd)\n",
    "nn.init.constant_(bn_b.bias, 1e-6) # prevent 0 crashout\n",
    "bn_b.weight, bn_b.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b02c6118-c5ff-4661-9bd8-e358b933736b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 3]),\n",
       " tensor([[ 0.0316, -0.0633,  0.0316],\n",
       "         [ 0.0351, -0.0703,  0.0351],\n",
       "         [ 0.0344, -0.0689,  0.0344],\n",
       "         [-0.0248,  0.0495, -0.0248],\n",
       "         [-0.0122,  0.0244, -0.0122],\n",
       "         [-0.0216,  0.0432, -0.0216],\n",
       "         [-0.1832,  0.3664, -0.1832],\n",
       "         [ 0.0023, -0.0046,  0.0023],\n",
       "         [ 0.0587, -0.1174,  0.0587],\n",
       "         [-0.0240,  0.0479, -0.0240],\n",
       "         [-0.1180,  0.2361, -0.1180],\n",
       "         [-0.1350,  0.2701, -0.1350],\n",
       "         [-0.0430,  0.0859, -0.0430],\n",
       "         [-0.0677,  0.1353, -0.0677],\n",
       "         [ 0.0469, -0.0938,  0.0469],\n",
       "         [-0.1084,  0.2168, -0.1084],\n",
       "         [ 0.1516, -0.3032,  0.1516],\n",
       "         [-0.0501,  0.1002, -0.0501],\n",
       "         [ 0.0036, -0.0072,  0.0036],\n",
       "         [ 0.1417, -0.2834,  0.1417],\n",
       "         [-0.1316,  0.2633, -0.1316],\n",
       "         [-0.1525,  0.3050, -0.1525],\n",
       "         [ 0.1856, -0.3711,  0.1856],\n",
       "         [-0.1173,  0.2347, -0.1173]], grad_fn=<NativeLayerNormBackward0>))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = bn_b(out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37206f1-9ef8-42b0-8307-28ac4c8a0845",
   "metadata": {},
   "source": [
    "#### GCN Block - Residual Connection\n",
    "\n",
    "\n",
    "In GCNs, residual connections create a clean path for each node’s own features and gradients to combat vanishing gradients that can come from the convolution layers and makes deeper message-passing stacks trainable. The residual connection also counters oversmoothing by re-injecting the identity at every layer so the node embeddings are less likely to collapse toward a common subspace, especially under strong degree normalization. Functionally this is represented as\n",
    "\n",
    "$y = f(x) + x$\n",
    "\n",
    "To achieve this we simply sum the projection matrix `x` with the convolution layer outputs `out`.  As a reminder our input embedding uses an incremental initiation so this addition removes the predictable pattern of the middle being 2x the outside channels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2dd2b7cc-762c-4465-978f-41fc638bcd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 3]),\n",
       " tensor([[ 0.0326, -0.0613,  0.0346],\n",
       "         [ 0.0371, -0.0673,  0.0391],\n",
       "         [ 0.0374, -0.0649,  0.0394],\n",
       "         [-0.0208,  0.0545, -0.0188],\n",
       "         [-0.0072,  0.0304, -0.0052],\n",
       "         [-0.0156,  0.0502, -0.0136],\n",
       "         [-0.1822,  0.3684, -0.1802],\n",
       "         [ 0.0043, -0.0016,  0.0063],\n",
       "         [ 0.0617, -0.1134,  0.0637],\n",
       "         [-0.0200,  0.0529, -0.0180],\n",
       "         [-0.1130,  0.2421, -0.1110],\n",
       "         [-0.1290,  0.2771, -0.1270],\n",
       "         [-0.0420,  0.0879, -0.0400],\n",
       "         [-0.0657,  0.1383, -0.0637],\n",
       "         [ 0.0499, -0.0898,  0.0519],\n",
       "         [-0.1044,  0.2218, -0.1024],\n",
       "         [ 0.1566, -0.2972,  0.1586],\n",
       "         [-0.0441,  0.1072, -0.0421],\n",
       "         [ 0.0046, -0.0052,  0.0066],\n",
       "         [ 0.1437, -0.2804,  0.1457],\n",
       "         [-0.1286,  0.2673, -0.1266],\n",
       "         [-0.1485,  0.3100, -0.1465],\n",
       "         [ 0.1906, -0.3651,  0.1926],\n",
       "         [-0.1113,  0.2417, -0.1093]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = out + x\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d347794-6944-4c6d-970f-cfd7f74fa3aa",
   "metadata": {},
   "source": [
    "#### GCN Block - Final LeakyReLU\n",
    "Now that we've summed up our values we apply a final layer of ReLU. This applies the same formulas and benefits on the overall graph output. At this stage, applying LeakyReLU nonlinearly gates the residual connection output letting the model suppress noisy aggregate messages while still preserving informative inhibitory, negative, responses via the non-zero negative slope. With GCNs, this again helps deeper message-passing remain stable and expressive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3d4b99e0-3aa8-4536-a60a-b3e8b794bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrelu_res = nn.LeakyReLU(negative_slope=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ca85ec78-c5b9-459b-8c1c-7fbb41716e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 3]),\n",
       " tensor([[ 3.2642e-02, -6.1281e-03,  3.4642e-02],\n",
       "         [ 3.7139e-02, -6.7275e-03,  3.9139e-02],\n",
       "         [ 3.7444e-02, -6.4885e-03,  3.9444e-02],\n",
       "         [-2.0752e-03,  5.4508e-02, -1.8752e-03],\n",
       "         [-7.1891e-04,  3.0381e-02, -5.1891e-04],\n",
       "         [-1.5591e-03,  5.0185e-02, -1.3591e-03],\n",
       "         [-1.8219e-02,  3.6838e-01, -1.8019e-02],\n",
       "         [ 4.3096e-03, -1.6162e-04,  6.3096e-03],\n",
       "         [ 6.1718e-02, -1.1343e-02,  6.3718e-02],\n",
       "         [-1.9957e-03,  5.2917e-02, -1.7957e-03],\n",
       "         [-1.1304e-02,  2.4208e-01, -1.1104e-02],\n",
       "         [-1.2904e-02,  2.7708e-01, -1.2704e-02],\n",
       "         [-4.1970e-03,  8.7943e-02, -3.9970e-03],\n",
       "         [-6.5655e-03,  1.3831e-01, -6.3655e-03],\n",
       "         [ 4.9899e-02, -8.9795e-03,  5.1899e-02],\n",
       "         [-1.0441e-02,  2.2183e-01, -1.0241e-02],\n",
       "         [ 1.5662e-01, -2.9723e-02,  1.5862e-01],\n",
       "         [-4.4084e-03,  1.0717e-01, -4.2084e-03],\n",
       "         [ 4.5938e-03, -5.1847e-04,  6.5938e-03],\n",
       "         [ 1.4373e-01, -2.8045e-02,  1.4573e-01],\n",
       "         [-1.2864e-02,  2.6728e-01, -1.2664e-02],\n",
       "         [-1.4852e-02,  3.1004e-01, -1.4652e-02],\n",
       "         [ 1.9057e-01, -3.6514e-02,  1.9257e-01],\n",
       "         [-1.1135e-02,  2.4170e-01, -1.0935e-02]], grad_fn=<LeakyReluBackward0>))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = lrelu_res(x)\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cf162c-c5c3-4864-aa74-8d78efacc76c",
   "metadata": {},
   "source": [
    "**Reinsert our batch** \n",
    "\n",
    "Finally now that we have our graph convolutions complete, we'll reinsert our batch dimension.  This will not change values, simply split the incoming tensor down into each batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cb970fdf-029e-472d-b1f3-b5b43bafbe94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 6, 3]),\n",
       " tensor([[[ 3.2642e-02, -6.1281e-03,  3.4642e-02],\n",
       "          [ 3.7139e-02, -6.7275e-03,  3.9139e-02],\n",
       "          [ 3.7444e-02, -6.4885e-03,  3.9444e-02],\n",
       "          [-2.0752e-03,  5.4508e-02, -1.8752e-03],\n",
       "          [-7.1891e-04,  3.0381e-02, -5.1891e-04],\n",
       "          [-1.5591e-03,  5.0185e-02, -1.3591e-03]],\n",
       " \n",
       "         [[-1.8219e-02,  3.6838e-01, -1.8019e-02],\n",
       "          [ 4.3096e-03, -1.6162e-04,  6.3096e-03],\n",
       "          [ 6.1718e-02, -1.1343e-02,  6.3718e-02],\n",
       "          [-1.9957e-03,  5.2917e-02, -1.7957e-03],\n",
       "          [-1.1304e-02,  2.4208e-01, -1.1104e-02],\n",
       "          [-1.2904e-02,  2.7708e-01, -1.2704e-02]],\n",
       " \n",
       "         [[-4.1970e-03,  8.7943e-02, -3.9970e-03],\n",
       "          [-6.5655e-03,  1.3831e-01, -6.3655e-03],\n",
       "          [ 4.9899e-02, -8.9795e-03,  5.1899e-02],\n",
       "          [-1.0441e-02,  2.2183e-01, -1.0241e-02],\n",
       "          [ 1.5662e-01, -2.9723e-02,  1.5862e-01],\n",
       "          [-4.4084e-03,  1.0717e-01, -4.2084e-03]],\n",
       " \n",
       "         [[ 4.5938e-03, -5.1847e-04,  6.5938e-03],\n",
       "          [ 1.4373e-01, -2.8045e-02,  1.4573e-01],\n",
       "          [-1.2864e-02,  2.6728e-01, -1.2664e-02],\n",
       "          [-1.4852e-02,  3.1004e-01, -1.4652e-02],\n",
       "          [ 1.9057e-01, -3.6514e-02,  1.9257e-01],\n",
       "          [-1.1135e-02,  2.4170e-01, -1.0935e-02]]], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.view(B_batch, N_nodes, n_embd)\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8dedd0-1230-400f-86b3-180e6e8b7da3",
   "metadata": {},
   "source": [
    "### Output Layers AKA Model Heads.\n",
    "We've now shown a common pattern for a GCN block including two layers that handle our signed graph and the residual connection. Once those layers are complete during the forward pass we then start the output process that results in our node level and graph level logits which represent the probability of the different classes we're predicting.  \n",
    "\n",
    "<img src=\"explainer_screenshots/gcn/output_layer.png\" width=\"300\">\n",
    "\n",
    "This layer is also known as the model **head**, or in this case \"heads\" and the weights learned are more task specific than the general model itself. What we'll uniquely do in this example is train two heads at once, \n",
    "\n",
    "Once a model is trained, there are processes to swap out the \"head\" and learn new tasks, e.g. go from next token prediction to classification.  In our example case, this is a linear layer mapping the backbone to vocab logits.\n",
    "\n",
    "Recall in the beginning that we discussed that we have 2 goals: predict the cell type and predict if a network is cancerous.  Because we have 2 goals we actually need to train 2 separate heads.  Given GCNs are graphs at the core, there's 2 ways to look at the probabilities, per node level or for a graph.  Per node level creates logit predictions at each node while the graph uses pooling to create graph level logits.  In our case we'll actually create a head for each type.  Our cell type will predict at the node level and cancerous level will predict at the graph level. \n",
    "\n",
    "One thing you'll notice is that our head for GNNs includes bias, something we haven't done in other models. In a GNN, the bias in the heads acts as a learnable intercept that soaks up systematic offsets coming from graph degree normalized aggregation, variable graph sizes, and, in signed graphs, partial cancellation of positive/negative messages. In other models where you have sequence/vision heads, those heads often use weight tying or the head sits after a normalization layer that does similar levels of offset and normalization that bias does. GCN heads often follow pooling or raw node features without a final centering step so the bias therefore captures class priors and size/structure-dependent baselines, improving calibration when labels are sparse and graphs are small.\n",
    "\n",
    "We'll first start with creating our **Node Head** or our cell type logits through a linear projection on the graph convolution output.  From there we'll then create a parallel **Graph Head** using pooling, masking, and then finally a linear projection.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04cca28-117e-4f4c-85d7-6acdfc86a298",
   "metadata": {},
   "source": [
    "#### Output Layer - Node Head\n",
    "Our node level head will predict if a node is more commonly associated with a \"T cell\" or \"B cell\".  We can think of this task as a classification task where the model has to pick for each sample the association.  Since this is classification, we need 2 dimensions for the cell types, `0` for T cell, and `1` for B cell.  This node level head will be shared across all examples and be  the `[classes,n_embed]` dimensionality.  We'll initialize with an incremental stepping of `0.005` similar to how we initialized the embedding layer. This will create a degree of bias to the second class initially but training should help adjust these weights. \n",
    "\n",
    "Also remember that since this is a GNN we also need to initialize bias in our head, in this case we'll start with our epsilon value. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9912594a-0ef1-4b2d-bf96-a71491f20de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[0.0050, 0.0100, 0.0150],\n",
       "         [0.0100, 0.0150, 0.0200]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1.0000e-06, 1.0000e-06], requires_grad=True))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_head = nn.Linear(n_embd, 2, bias=True)   # cell-type logits per node\n",
    "with torch.no_grad(): # initilize to W[i,j] = 0.001*(1+i+j) for easy following \n",
    "    vs, d = 2, n_embd\n",
    "    rows = torch.arange(vs).unsqueeze(1)  # (vs,1)\n",
    "    cols = torch.arange(d).unsqueeze(0)  # (1,d)\n",
    "    pattern = 0.005*(1 + rows + cols)  # W[i,j] = 0.001*(1+i+j)\n",
    "    node_head.weight.copy_(pattern)\n",
    "nn.init.constant_(node_head.bias, 1e-6)\n",
    "node_head.weight, node_head.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e52ae8-5c26-422d-bbbe-fbc22ffc273b",
   "metadata": {},
   "source": [
    "**Node Head - Output Projection**\n",
    "\n",
    "Now we're ready for our final projection. Since our head is initialized having the incremental values we're automatically skewing towards class `1` instead of `0`, meaning it might be shit. Luckily backpropagation has a way of updating this so that with enough data and time the probabilities change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3aae275c-d291-4786-a44c-5641ae0251c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 6, 2]),\n",
       " tensor([[[0.0006, 0.0009],\n",
       "          [0.0007, 0.0011],\n",
       "          [0.0007, 0.0011],\n",
       "          [0.0005, 0.0008],\n",
       "          [0.0003, 0.0004],\n",
       "          [0.0005, 0.0007]],\n",
       " \n",
       "         [[0.0033, 0.0050],\n",
       "          [0.0001, 0.0002],\n",
       "          [0.0012, 0.0017],\n",
       "          [0.0005, 0.0007],\n",
       "          [0.0022, 0.0033],\n",
       "          [0.0025, 0.0038]],\n",
       " \n",
       "         [[0.0008, 0.0012],\n",
       "          [0.0013, 0.0019],\n",
       "          [0.0009, 0.0014],\n",
       "          [0.0020, 0.0030],\n",
       "          [0.0029, 0.0043],\n",
       "          [0.0010, 0.0015]],\n",
       " \n",
       "         [[0.0001, 0.0002],\n",
       "          [0.0026, 0.0039],\n",
       "          [0.0024, 0.0036],\n",
       "          [0.0028, 0.0042],\n",
       "          [0.0035, 0.0052],\n",
       "          [0.0022, 0.0033]]], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_logits = node_head(x)   \n",
    "node_logits.size(), node_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5bdb1c-0d05-4a3b-998b-aeb4fffc37c1",
   "metadata": {},
   "source": [
    "#### Output Layer - Graph Head\n",
    "Our graph level head will predict if a network (connection of nodes) is more benign or cancerous.  We can think of this task also as a classification task where the model has to pick for each sample the association.  Since this is classification, we need 2 dimensions for the cell types, `0` for Benign, and `1` for Cancerous.  This node level head will be shared across all examples and be  the `[classes,n_embed]` dimensionality.\n",
    "\n",
    "Before we can run the graph level prediction though, we have to collapse our current graph convolution output. Currently the output `x` is `[4,6,3]` representing our batch, nodes, and embeddings.  Recall that our nodes are all not genes, there's actually two nodes that are cell types. To do our graph level predictions and only focus on the parts of the network focused on the gene edges, we'll use a mask to zero-out the cell dimensions.  That will then be applied to our graph convolution output, then we'll get the node level average, or sum of the embedding channels divided by the number of genes. This is computed by applying the following:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}=\\frac{1}{|G|}\\sum_{i\\in G}\\mathbf{h}_i\n",
    "$$\n",
    "\n",
    "This results in an output of embeddings for each example, or a `[b_batch,n_embd]` tensor.  We then do a final linear projection against our class dimension head to get a final `logits` output. Also remember that since this is a GNN we also need to initialize bias in our head, in this case we'll start with our epsilon value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceced0c-f942-47b8-badf-85dcf8917db5",
   "metadata": {},
   "source": [
    "**Graph Head - Creating the mask**  \n",
    "\n",
    "We first create a simple mask that shows only our first 4 dimensions on our tensor pertaining to the gene nodes.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "69874254-1dfa-48cc-90f0-ec90464f3a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6]), tensor([[1., 1., 1., 1., 0., 0.]]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = gene_mask.float().unsqueeze(0)\n",
    "mask.size(), mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3af1c88-0559-4882-b4f8-35d49ecc8f92",
   "metadata": {},
   "source": [
    "**Graph Head - Masked Sum**  \n",
    "\n",
    "We now apply the mask to the graph convolution output.  You can see that this zeros out the tensor entries pertaining to the cell type dimension. After we have that zeroed out we then sum our node dimensions resulting in embeddings for each of our cell types. You can see that at this point we have differing values across all three embedding channels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "429945ae-4c77-4e86-9553-7278b82180b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 6, 3]),\n",
       " tensor([[[ 3.2642e-02, -6.1281e-03,  3.4642e-02],\n",
       "          [ 3.7139e-02, -6.7275e-03,  3.9139e-02],\n",
       "          [ 3.7444e-02, -6.4885e-03,  3.9444e-02],\n",
       "          [-2.0752e-03,  5.4508e-02, -1.8752e-03],\n",
       "          [-0.0000e+00,  0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00, -0.0000e+00]],\n",
       " \n",
       "         [[-1.8219e-02,  3.6838e-01, -1.8019e-02],\n",
       "          [ 4.3096e-03, -1.6162e-04,  6.3096e-03],\n",
       "          [ 6.1718e-02, -1.1343e-02,  6.3718e-02],\n",
       "          [-1.9957e-03,  5.2917e-02, -1.7957e-03],\n",
       "          [-0.0000e+00,  0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00, -0.0000e+00]],\n",
       " \n",
       "         [[-4.1970e-03,  8.7943e-02, -3.9970e-03],\n",
       "          [-6.5655e-03,  1.3831e-01, -6.3655e-03],\n",
       "          [ 4.9899e-02, -8.9795e-03,  5.1899e-02],\n",
       "          [-1.0441e-02,  2.2183e-01, -1.0241e-02],\n",
       "          [ 0.0000e+00, -0.0000e+00,  0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00, -0.0000e+00]],\n",
       " \n",
       "         [[ 4.5938e-03, -5.1847e-04,  6.5938e-03],\n",
       "          [ 1.4373e-01, -2.8045e-02,  1.4573e-01],\n",
       "          [-1.2864e-02,  2.6728e-01, -1.2664e-02],\n",
       "          [-1.4852e-02,  3.1004e-01, -1.4652e-02],\n",
       "          [ 0.0000e+00, -0.0000e+00,  0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00, -0.0000e+00]]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_graph_sum = (x * mask.unsqueeze(-1))\n",
    "x_graph_sum.size(), x_graph_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2fd6b21c-1b0b-44e6-af84-ee9c8897b6e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3]),\n",
       " tensor([[0.1051, 0.0352, 0.1113],\n",
       "         [0.0458, 0.4098, 0.0502],\n",
       "         [0.0287, 0.4391, 0.0313],\n",
       "         [0.1206, 0.5488, 0.1250]], grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_graph_sum = x_graph_sum.sum(1)\n",
    "x_graph_sum.size(), x_graph_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716e3923-8185-493c-9d1d-58b4b0e01bcf",
   "metadata": {},
   "source": [
    "**Graph Head - Average Pooling**  \n",
    "\n",
    "To finish our pooling we now have to divide our sums to get a node average per example. This simply requires dividing each row by the number of nodes.  Lucky for us all our entries have the same number so it's a simple division across all entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a944671d-0196-45f1-b561-849bcdf3c85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denom = mask.sum(1, keepdim=True).clamp_min(1.0) \n",
    "denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "673b8f72-54ff-4eee-a605-a151d5290b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3]),\n",
       " tensor([[0.0263, 0.0088, 0.0278],\n",
       "         [0.0115, 0.1024, 0.0126],\n",
       "         [0.0072, 0.1098, 0.0078],\n",
       "         [0.0302, 0.1372, 0.0313]], grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled = x_graph_sum / denom   \n",
    "pooled.size(), pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb03805e-e9c2-4886-99ee-28bb3ab3dd21",
   "metadata": {},
   "source": [
    "**Graph Head - Output Projection**\n",
    "\n",
    "Now we're ready for our final projection. Since our head is initialized having the incremental values we're automatically skewing towards class `1` instead of `0`, meaning it might be shit. Luckily backpropagation has a way of updating this so that with enough data and time the probabilities change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dfb7b240-1533-4911-a897-49db877c9d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[0.0010, 0.0020, 0.0030],\n",
       "         [0.0020, 0.0030, 0.0040]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1.0000e-06, 1.0000e-06], requires_grad=True))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_head = nn.Linear(n_embd, 2, bias=True)  # cancer logits per graph\n",
    "with torch.no_grad(): # initilize to W[i,j] = 0.001*(1+i+j) for easy following \n",
    "    vs, d = 2, n_embd\n",
    "    rows = torch.arange(vs).unsqueeze(1)  # (vs,1)\n",
    "    cols = torch.arange(d).unsqueeze(0)  # (1,d)\n",
    "    pattern = 0.001*(1 + rows + cols)  # W[i,j] = 0.001*(1+i+j)\n",
    "    graph_head.weight.copy_(pattern)\n",
    "nn.init.constant_(graph_head.bias, 1e-6) \n",
    "graph_head.weight, graph_head.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "31b7f10e-d109-4f97-8fe4-a555fe0c797f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 2]),\n",
       " tensor([[0.0001, 0.0002],\n",
       "         [0.0003, 0.0004],\n",
       "         [0.0003, 0.0004],\n",
       "         [0.0004, 0.0006]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_logits = graph_head(pooled)  # [B,2]\n",
    "graph_logits.size(), graph_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b342640c-c90c-40a9-9be1-2ad7a70cf5e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Loss calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb1ae70-d061-40fd-9377-3e9f9878d5bf",
   "metadata": {},
   "source": [
    "Now we have to see how good our ~shit~ prediction is.  Since we haven't done training and we saw that regardless of class, for an example we had the same exact logit values, so we can expect it's bad, basically random. That said, we need to know how bad. For this example we'll use cross entropy, also known as the negative log likelihood of the softmax.  Our loss calculates\n",
    "\n",
    "$$\n",
    "\\ell_i=-\\log\\big(\\mathrm{argmax}(z_i)\\_{y_i}\\big)\n",
    "= -z_{i,y_i}+\\log\\!\\sum_{c=1}^C e^{z_{i,c}},\n",
    "$$\n",
    "\n",
    "What's unique in this case is that we actually have 2 heads.  You might wonder how we'll do backprop, but it's simple, we'll just calculate the loss on each head.  For now we'll make it easy and assume that the both losses are equally important to us.  This is tunable though depending on what you value in a graph.  If you value one of the losses more, you can upweight it before summing and, during backprop, that will impact the gradient flow to prioritize that loss. \n",
    "\n",
    "$$\n",
    "\\ell_{total}= \\ell_{node}+\\alpha \\ell_{graph}\n",
    "$$\n",
    "\n",
    "We'll first calculate our node loss and then our graph loss and sum them together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "87a2fb89-258c-42c9-92a0-c5a9f34b9ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a908ce7c-5c6b-4117-8dda-1206870ff30d",
   "metadata": {},
   "source": [
    "**Node Loss**\n",
    "\n",
    "For our node loss, we again need to employ a mask.  This mask helps with our loss to avoid including the cell type level nodes in our prediction as we primarily want to understand which node, or gene, predicts which cell type.  After we create the mask, we slice our node logits (nodes for each example) and label `y_node` for those and then calculate the loss. You'll see because our logits are equal, our loss is random. You'll also notice that the valid masking removes the batch dimension to simplify the loss calculation as it does not care about batches. Finally we also append our loss to `losses` so we can do the final summation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f7521890-e6bb-4841-87af-3e809427dbce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True, False, False],\n",
       "        [ True,  True,  True,  True, False, False],\n",
       "        [ True,  True,  True,  True, False, False],\n",
       "        [ True,  True,  True,  True, False, False]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid = (gene_mask & (y_node>=0))\n",
    "valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "480f7beb-bf12-45ef-a7db-262e8fc3df06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0006, 0.0009],\n",
       "         [0.0007, 0.0011],\n",
       "         [0.0007, 0.0011],\n",
       "         [0.0005, 0.0008],\n",
       "         [0.0033, 0.0050],\n",
       "         [0.0001, 0.0002],\n",
       "         [0.0012, 0.0017],\n",
       "         [0.0005, 0.0007],\n",
       "         [0.0008, 0.0012],\n",
       "         [0.0013, 0.0019],\n",
       "         [0.0009, 0.0014],\n",
       "         [0.0020, 0.0030],\n",
       "         [0.0001, 0.0002],\n",
       "         [0.0026, 0.0039],\n",
       "         [0.0024, 0.0036],\n",
       "         [0.0028, 0.0042]], grad_fn=<IndexBackward0>),\n",
       " tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_node_logits = node_logits[valid]\n",
    "valid_y_node = y_node[valid]\n",
    "valid_node_logits, valid_y_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3c1a5843-c2db-4340-9ece-8d8207146ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6932, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nl = F.cross_entropy(valid_node_logits, valid_y_node)\n",
    "losses.append(nl)\n",
    "nl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56d9cc2-95d9-4209-aee0-5bc79aeffa74",
   "metadata": {},
   "source": [
    "**Graph Loss**\n",
    "\n",
    "For our graph loss, we do not need a mask since we included it during pooling and we've already collapsed our dimensions down.  At this point we  just pass in the graph logits and our labels stored in `y_graph`.  We'll also append the loss to `losses`. Notice that our current logits are equal per example so we will expect our loss to be close to random.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b2484471-c5a1-4e4b-9a4f-2b9f41422614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0001, 0.0002],\n",
       "         [0.0003, 0.0004],\n",
       "         [0.0003, 0.0004],\n",
       "         [0.0004, 0.0006]], grad_fn=<AddmmBackward0>),\n",
       " tensor([0, 1, 1, 1]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_logits, y_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "58fda360-8684-4610-9af3-cd202e8f1ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6931, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gl = F.cross_entropy(graph_logits, y_graph)\n",
    "losses.append(gl)\n",
    "gl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d7ecdb-367c-4422-b167-929367e3c2e1",
   "metadata": {},
   "source": [
    "**Sum Loss**\n",
    "\n",
    "Now that we have each loss we finally have to complete the sum:\n",
    "\n",
    "$$\n",
    "\\ell_{total}= \\ell_{node}+\\alpha \\ell_{graph}\n",
    "$$\n",
    "\n",
    "For this example we'll weigh each loss equally so we can ignore the alpha. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "be018565-25a6-4011-b993-adce11a09ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3863, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = sum(losses) \n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e9f666-5438-4a74-b3d7-142da1c078f8",
   "metadata": {},
   "source": [
    "## Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070b9c23-837c-4824-922d-c50b07bbaeaa",
   "metadata": {},
   "source": [
    "We now know just how ~well~ terribly the current model, with its weights and biases, predicts the next token given the input context. We now need to know how to change the different weights and biases to improve the formula.  We could do this by guessing through making minor changes and seeing what improves, or we can think through this more critically.\n",
    "\n",
    "If you review the chain of layers above, you can see that it's a series of formulas.  We can think of this as $f(g(x))$, except with many many more layers and complexities.  Since this is a formula, we can dig into our math toolbox and find a better way to determine what parts need to update.  Recall that in our calculus we learned that differentiation tells us the rate of change in a graph.  So if we treat the loss function $\\mathcal{L}$ as $\\mathcal{L}(f(g(x)))$ taking the partial differential \n",
    "\n",
    "$\\delta=\\partial \\mathcal{L}/\\partial h$\n",
    "\n",
    "at each layer will give us the impact of each weight/bias on our final out (albeit the inverse since our loss function is the negative log likelihood). \n",
    "\n",
    "Lucky for us, each layer of our model already has a placeholder for the partial differential called the **Gradient**. We'll use this field to store it.  We'll start by first zeroing out the gradients. We do this because of the nature of handling partial differentials for multiple dependencies. Recall that in multiple places we had a formula structure of \n",
    "\n",
    "$a+b=c ; a+c= d$\n",
    "\n",
    "In this case $a$ has 2 dependencies and determining the partial derivative of $\\partial d / \\partial a$ requires understanding both the path from $d$ and $c$.  To determine the true impact of a we would sum both partial derivatives together.  Because of this property, the tool we use, the built in `.backwards()` automatically sums gradients, `+=`, so if we do not set the gradient to `0` we then end up with erroneous gradients. \n",
    "\n",
    "Finally, we start `.backwards` from the `loss`, not `logits` as our goal is to minimize loss, we need to ensure we are looking at the calculations that impact loss which requires the whole forward pass to be able to generate the prediction `logits_flat`.  If we think of it as $\\mathcal{L}(f(x))$ where $f(x)$ is the forward pass to generate logits, then a simple chain rule is applied:\n",
    "\n",
    "${\\partial}/{\\partial x} =  \\mathcal{L}'(f(x)) f'(x)$\n",
    "\n",
    "Lets start by zeroing the gradients and leaning on pytorch to calculate the gradients for us. We'll also validate the gradients were `none`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0044f323-bd0f-4d09-a0db-0e0d8ddc5f2a",
   "metadata": {},
   "source": [
    "### Back Propagation - Zero out gradients\n",
    "\n",
    "Before we start we have to zero our gradients to make sure there's nothing in them.  Recall that when we run  `.backwards()` it automatically sums gradients if there are multiple paths so zeroing out ensure no erroneous measures.  \n",
    "\n",
    "Two things to notice: \n",
    "1. Our pooling step is not included.  While pooling does pass through gradients, it is stateless, meaning there are no learnable parameters, so the layer does not have gradient buffers to clear. The step itself is differentiable though so gradients flow through it back to upstream parameters.\n",
    "2. Our convolutional layers use a special reset.  Since we manually built the layers using `nn.Parameter` they do not have the typical gradient and weight functions that layers have, but the layers are learnable.  Because of this they do have gradient buffers but no magic function to clear them so we have to clear them manually.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "69a68b2e-2bbd-4809-b8eb-4031ed23ce5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output layers\n",
    "graph_head.zero_grad()\n",
    "node_head.zero_grad()\n",
    "\n",
    "#GCN block\n",
    "bn_b.zero_grad()\n",
    "gcn_2_neg.zero_grad()\n",
    "gcn_2_pos.zero_grad()\n",
    "bn_a.zero_grad()\n",
    "gcn_1_neg.zero_grad()\n",
    "gcn_1_pos.zero_grad()\n",
    "\n",
    "#input layer\n",
    "tok_emb.zero_grad()\n",
    "\n",
    "# validate gradients\n",
    "graph_head.weight.grad,node_head.weight.grad,tok_emb.weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5776d30c-045c-4db4-8018-64b98b558570",
   "metadata": {},
   "source": [
    "### Back Propagation - Auto Diff\n",
    "\n",
    "Now let's see the magic of the gradients populate.  This magic is called auto-differentiation, or auto-diff for short. This allows us to not have to write many layers of nasty code to do the differentiation for us, but, if you're a sadist, you can surely find people who have written out that code (it's not too bad since you just do one layer at a time). \n",
    "\n",
    "Recall that our loss is actually the sum of the loss across multiple heads. The beauty here is that because we used a sum, the gradient will be distributed across both and join as we reach each layer, helping ensure that the graph is optimizing both heads at the same time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fa249655-cce1-4cf1-bbd9-ac6033513ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fdca7a-ec26-483d-b98c-3a7a490ce660",
   "metadata": {},
   "source": [
    "Now we will revisualize our gradients and see that they contain values, just like we wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b833418f-6a5d-4d6a-8c81-d9ba6320c321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0028,  0.0426,  0.0030],\n",
       "         [-0.0028, -0.0426, -0.0030]]),\n",
       " tensor([[-0.0120,  0.0173, -0.0124],\n",
       "         [ 0.0120, -0.0173,  0.0124]]),\n",
       " tensor([[-0.0023, -0.0052, -0.0023],\n",
       "         [-0.0044, -0.0090, -0.0044],\n",
       "         [-0.0116, -0.0242, -0.0116],\n",
       "         [ 0.0009,  0.0012,  0.0009],\n",
       "         [ 0.0105,  0.0211,  0.0105],\n",
       "         [-0.0006, -0.0013, -0.0006]]),\n",
       " tensor([ 5.5718e-05, -1.2564e-04,  5.5718e-05]),\n",
       " tensor([[ 0.0002,  0.0008,  0.0001],\n",
       "         [-0.0003, -0.0015, -0.0003],\n",
       "         [ 0.0002,  0.0008,  0.0001]]),\n",
       " tensor([ 9.9687e-08, -1.6229e-05,  2.3857e-06]),\n",
       " tensor([[ 0.0032,  0.0040,  0.0047],\n",
       "         [-0.0051, -0.0062, -0.0073],\n",
       "         [ 0.0019,  0.0022,  0.0026]]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(graph_head.weight.grad,node_head.weight.grad,tok_emb.weight.grad, \n",
    "bn_b.weight.grad,gcn_2_pos.weight.grad,bn_a.weight.grad,gcn_1_pos.weight.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b2fa4d-c2f4-4a85-844d-bf1165b64699",
   "metadata": {},
   "source": [
    "## Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc4db83-db37-414a-8828-2991456e7534",
   "metadata": {},
   "source": [
    "In GNNs, backpropagated signals traverse the graph via repeated multiplications by the adjacency (A), so when we want to model distant node interactions through the use of deeper stacks, gradients can spike rapidly for hubs/high-degree regions, or poorly scaled signed edges. Additionally, irregular graph topology (variable degrees, component sizes) and graph-level pooling also make gradient magnitudes highly sample-dependent, so a single complex, or \"hard\" graph can cause catastrophic updates. When graphs add the signed layer like ours, subtractive positive/negative channels can create oscillatory dynamics that further amplify bursts. \n",
    "\n",
    "Gradient clipping caps these rare but dangerous surges, stabilizing training, preserving a usable learning rate, and reducing sensitivity to graph heterogeneity without masking genuine signals. Gradient clipping caps the global gradient, in our case to `1.0`, to prevent runaway steps, numerical overflow, and unstable updates.\n",
    "\n",
    "Since none of our gradients are currently above 1, we will not see an impact of gradient clipping but I did want to introduce the concept as it is an important component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6d66ffe8-0345-4fea-9bc5-ec29a8e386f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0028,  0.0426,  0.0030],\n",
       "         [-0.0028, -0.0426, -0.0030]]),\n",
       " tensor([[-0.0120,  0.0173, -0.0124],\n",
       "         [ 0.0120, -0.0173,  0.0124]]),\n",
       " tensor([[-0.0023, -0.0052, -0.0023],\n",
       "         [-0.0044, -0.0090, -0.0044],\n",
       "         [-0.0116, -0.0242, -0.0116],\n",
       "         [ 0.0009,  0.0012,  0.0009],\n",
       "         [ 0.0105,  0.0211,  0.0105],\n",
       "         [-0.0006, -0.0013, -0.0006]]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.utils.clip_grad_norm_(graph_head.parameters(), 1.0)\n",
    "nn.utils.clip_grad_norm_(node_head.parameters(), 1.0)\n",
    "\n",
    "nn.utils.clip_grad_norm_(bn_b.parameters(), 1.0)\n",
    "nn.utils.clip_grad_norm_(gcn_2_neg.parameters(), 1.0)\n",
    "nn.utils.clip_grad_norm_(gcn_2_pos.parameters(), 1.0)\n",
    "nn.utils.clip_grad_norm_(bn_a.parameters(), 1.0)\n",
    "nn.utils.clip_grad_norm_(gcn_1_neg.parameters(), 1.0)\n",
    "nn.utils.clip_grad_norm_(gcn_1_pos.parameters(), 1.0)\n",
    "\n",
    "nn.utils.clip_grad_norm_(tok_emb.parameters(), 1.0)\n",
    "\n",
    "graph_head.weight.grad,node_head.weight.grad,tok_emb.weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ea5429-9572-4c68-8d24-fde989a1290a",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f733e31-7577-452c-b158-de94c7572688",
   "metadata": {},
   "source": [
    "The process of learning now requires us to update our weights based on this gradient. To really feel the \"back propagation\" we'll start with the last layer and work backwards, though, since we have all of the gradients calculated already, the order does not matter. Recall that our loss function is the negative log likelihood ratio so our gradient signs are flipped.  If a parameter is important, the gradient will be more negative, and vice versa. The gradients are a ratio of importance of each parameter and we need to know how much of that gradient to apply to our weights. This \"how much\" is referred to as the *learning rate*. In modern training learning rate schedulers and optimizers are used to vary the rate and application by layer and by training round with learning rates that are small (e.g. 1e-3) and decaying. \n",
    "\n",
    "We however are trying to learn and if you look at the gradient above in most layers it's tiny (~1e-4).  If we used a typical learning rate scheduler, with our batch size, and just 1 pass, the second pass would just have the same values and we wouldn't learn anything new.  Because of this we'll use a very high learning rate of `5.000` so that we amplify the learning from a single pass and can see the weights change. As a warning, DO NOT DO THIS IN REAL TRAINING. If you did your model would most likely not converge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "867a19fb-3ab8-4f7a-b74b-f956a5a94b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Huge learning rate to emphasize\n",
    "lr = 5.000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0ed666-e343-4a88-9160-e7885c8823e8",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Let's start with our output layer.  Recall that we initialized the weights to increments of `0.001` for the graph_head and `0.005` for the node_head so we can quickly see the impact of the gradient update on the weights. This layer has some of the largest gradients so using the high learning rate radically shifts the values.  You can see also that the model is adjusting to downweight the first class and upweight the second class in the graph_head while having more ambiguity on the node_head.  Also the model seems to be learning from the different channels as we can see the weights shifting across the `n_embd` dimensions.  \n",
    "\n",
    "On the bias though we're seeing interesting symmetry, hard to explain why, but it could be acting almost as an overwrite for the network.  This could highlight the smoothing power of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3609d4b6-5e30-4125-85b4-a6d335332e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[-0.0131, -0.2109, -0.0119],\n",
       "         [ 0.0161,  0.2159,  0.0189]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.2500,  0.2500], requires_grad=True))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    graph_head.weight -= lr * graph_head.weight.grad\n",
    "    graph_head.bias -= graph_head.bias.grad\n",
    "graph_head.weight, graph_head.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "33da3813-fecd-40aa-b867-2841839146e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[ 0.0651, -0.0767,  0.0768],\n",
       "         [-0.0501,  0.1017, -0.0418]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0627, -0.0627], requires_grad=True))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    node_head.weight -= lr * node_head.weight.grad\n",
    "    node_head.bias -= node_head.bias.grad\n",
    "node_head.weight, node_head.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2711556-2090-4a5e-86d5-d0b86535028a",
   "metadata": {},
   "source": [
    "### GCN Unit - Main Pass\n",
    "Now we'll update our graph convolution layers.  Recall we initiated our layer norm layers to `1.000` and used pyramidal initiation for our graph convolution layers.   We can see that with our learning, all of the layers were pushed away from their initiation.  \n",
    "\n",
    "For our layer norm we can see some of the symmetry of the first and last column compared to the middle column propagating into the weights.  It would be interesting to see if this persists with many training loops as it is likely an artifact of our initiation of the graph convolution layers. \n",
    "\n",
    "For the graph convolution layers we can start seeing a push away from the pyramidal structure.  The second layer has far smaller shifts at this point with larger shifts in the first layer. This shows that the graph is favoring information from the first connection (Cell type > gene) more than the gene back to the cell type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2681132e-396e-4e2d-a631-c16549db366f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([0.9997, 1.0006, 0.9997], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0008,  0.0007, -0.0008], requires_grad=True))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    bn_b.weight -= lr * bn_b.weight.grad\n",
    "    bn_b.bias -= bn_b.bias.grad\n",
    "bn_b.weight, bn_b.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "044c0888-2f58-4ab6-a9cb-4d2cc9ca2d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[0.0508, 0.1034, 0.0509],\n",
       "         [0.0984, 0.1933, 0.0983],\n",
       "         [0.0508, 0.1034, 0.0509]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.1574, -0.3148,  0.1574], requires_grad=True))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    gcn_2_neg.weight -= lr * gcn_2_neg.weight.grad\n",
    "    gcn_2_neg.bias -= gcn_2_neg.bias.grad\n",
    "gcn_2_neg.weight, gcn_2_neg.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "84eb3371-cc12-47de-982f-839cada1289a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[0.0492, 0.0962, 0.0493],\n",
       "         [0.1016, 0.2075, 0.1015],\n",
       "         [0.0492, 0.0962, 0.0493]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.1574,  0.3148, -0.1574], requires_grad=True))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    gcn_2_pos.weight -= lr * gcn_2_pos.weight.grad\n",
    "    gcn_2_pos.bias -= gcn_2_pos.bias.grad\n",
    "gcn_2_pos.weight, gcn_2_pos.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a35a25f5-f4a9-43c0-a14c-a22490d09a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([1.0000, 1.0001, 1.0000], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0010,  0.0069,  0.0006], requires_grad=True))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    bn_a.weight -= lr * bn_a.weight.grad\n",
    "    bn_a.bias -= bn_a.bias.grad\n",
    "bn_a.weight, bn_a.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3a0f84a2-47b5-429b-a01f-20d9327fe57b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[0.0191, 0.0299, 0.0207],\n",
       "         [0.0052, 0.0244, 0.0036],\n",
       "         [0.0157, 0.0257, 0.0157]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.5361, -0.8014,  0.2652], requires_grad=True))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    gcn_1_neg.weight -= lr * gcn_1_neg.weight.grad\n",
    "    gcn_1_neg.bias -= gcn_1_neg.bias.grad\n",
    "gcn_1_neg.weight, gcn_1_neg.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ac89824a-ed26-4561-a382-906b2b96fb2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[ 0.0015,  0.0094, -0.0027],\n",
       "         [ 0.0335,  0.0566,  0.0396],\n",
       "         [ 0.0050,  0.0140,  0.0031]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.5361,  0.8014, -0.2652], requires_grad=True))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    gcn_1_pos.weight -= lr * gcn_1_pos.weight.grad\n",
    "    gcn_1_pos.bias -= gcn_1_pos.bias.grad\n",
    "gcn_1_pos.weight, gcn_1_pos.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308b1ce4-bfc0-4c50-b97b-972f9ccc5081",
   "metadata": {},
   "source": [
    "### Input Layer\n",
    "Finally we'll update our input layer.  Recall that this was incremented with a sliding increment as well and has no bias.  Here we can again see that the layer seems to be learning the pyramidal structure of our convolution layers where the middle channel is higher than the two outside.  This shows the dangers of poor initiation but it does show the model overall balancing and learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "26c61496-e0da-4de1-8ebb-e7fbf3514c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0123,  0.0278,  0.0143],\n",
       "        [ 0.0241,  0.0478,  0.0261],\n",
       "        [ 0.0611,  0.1252,  0.0631],\n",
       "        [-0.0004, -0.0010,  0.0016],\n",
       "        [-0.0477, -0.0994, -0.0457],\n",
       "        [ 0.0092,  0.0133,  0.0112]], requires_grad=True)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    tok_emb.weight -= lr * tok_emb.weight.grad\n",
    "tok_emb.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c558eb69-c42c-4505-97cf-66b0b4c91270",
   "metadata": {},
   "source": [
    "## Forward Pass with Updated Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0a4619-31e5-4a48-a779-505a511be125",
   "metadata": {},
   "source": [
    "Now that we have the updated weights for each layer, let's do another forward pass and compare the loss. Since each layer was previously explained we will instead focus on just showing the outputs of the different layers and the final loss. If you want, you can check the previous outputs in the cached cell outputs above and compare them to see how the weight changes impacted the values at each layer. \n",
    "\n",
    "You'll notice that because of our high learning rate we're able to see how each layer now shifts the embedding values as the input passes through them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a2e1b1-b65a-430d-ade3-a3de1dfb9557",
   "metadata": {},
   "source": [
    "### Data Re-loading\n",
    "Since we didn't reuse any input variables, we'll just recall them and use them again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1963276c-fbb9-4ec1-8735-6dce7686efaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2, 3, 4, 5],\n",
       "         [0, 1, 2, 3, 4, 5],\n",
       "         [0, 1, 2, 3, 4, 5],\n",
       "         [0, 1, 2, 3, 4, 5]]),\n",
       " tensor([[ 0,  0,  0,  1, -1, -1],\n",
       "         [ 0,  1,  0,  1, -1, -1],\n",
       "         [ 1,  0,  0,  1, -1, -1],\n",
       "         [ 0,  0,  1,  1, -1, -1]]),\n",
       " tensor([0, 1, 1, 1]),\n",
       " tensor([ True,  True,  True,  True, False, False]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tokens, y_node, y_graph, gene_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "87864fa9-37f9-456b-8ac4-9f57ad582fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5000, 0.0000, 0.0000, 0.0000, 0.3536, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.5000, 0.0000, 0.0000, 0.3536, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.5000, 0.0000, 0.3536, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3536, 0.3536, 0.3536, 0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000,\n",
       "          0.0000, 0.3333, 0.2887, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000,\n",
       "          0.0000, 0.0000, 0.3536, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000,\n",
       "          0.0000, 0.4082, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.5000, 0.0000, 0.3536, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.4082,\n",
       "          0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2887, 0.3536, 0.0000,\n",
       "          0.3536, 0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4082,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.4082,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.5000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.4082, 0.0000, 0.0000, 0.4082, 0.0000, 0.3333,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.5000, 0.0000, 0.0000, 0.5000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.5000, 0.0000, 0.0000, 0.4082],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.4082],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.5000, 0.0000, 0.0000, 0.5000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.4082, 0.4082, 0.0000, 0.3333]]),\n",
       " tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5774, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5774, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5774, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5774, 0.5774, 0.5774, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.7071, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.7071, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7071, 0.0000,\n",
       "          0.7071, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5774, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4082, 0.5000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7071,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5774, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.5774, 0.4082, 0.0000, 0.5774, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.7071, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.4082, 0.5000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7071],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.5774, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.5774, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.4082, 0.0000, 0.5774, 0.5774, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.5000, 0.7071, 0.0000, 0.0000, 0.0000, 0.0000]]))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Apos_blk, Aneg_blk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef18795-d926-4d8c-b53b-659d8992cde6",
   "metadata": {},
   "source": [
    "### Input Layer\n",
    "Note that in `tok_embedding` we ended up with a large shift away from our initial values.  because of this we'll see a shift in our embedding layer though it will still reflect the weights since our input is the iteration of nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f28806a2-24e8-4bd2-b5b4-e8e0cc284d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 3]),\n",
       " tensor([[ 0.0123,  0.0278,  0.0143],\n",
       "         [ 0.0241,  0.0478,  0.0261],\n",
       "         [ 0.0611,  0.1252,  0.0631],\n",
       "         [-0.0004, -0.0010,  0.0016],\n",
       "         [-0.0477, -0.0994, -0.0457],\n",
       "         [ 0.0092,  0.0133,  0.0112],\n",
       "         [ 0.0123,  0.0278,  0.0143],\n",
       "         [ 0.0241,  0.0478,  0.0261],\n",
       "         [ 0.0611,  0.1252,  0.0631],\n",
       "         [-0.0004, -0.0010,  0.0016],\n",
       "         [-0.0477, -0.0994, -0.0457],\n",
       "         [ 0.0092,  0.0133,  0.0112],\n",
       "         [ 0.0123,  0.0278,  0.0143],\n",
       "         [ 0.0241,  0.0478,  0.0261],\n",
       "         [ 0.0611,  0.1252,  0.0631],\n",
       "         [-0.0004, -0.0010,  0.0016],\n",
       "         [-0.0477, -0.0994, -0.0457],\n",
       "         [ 0.0092,  0.0133,  0.0112],\n",
       "         [ 0.0123,  0.0278,  0.0143],\n",
       "         [ 0.0241,  0.0478,  0.0261],\n",
       "         [ 0.0611,  0.1252,  0.0631],\n",
       "         [-0.0004, -0.0010,  0.0016],\n",
       "         [-0.0477, -0.0994, -0.0457],\n",
       "         [ 0.0092,  0.0133,  0.0112]], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tok_emb(x_tokens)\n",
    "x = x.view(B_batch*N_nodes,n_embd)\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5258d8d7-db81-4a8c-bd2c-808a0aa46072",
   "metadata": {},
   "source": [
    "### GCN Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c942e8-992a-4d44-9502-386019b7a7a9",
   "metadata": {},
   "source": [
    "#### GCN Block - First Node Convolution\n",
    "Now we'll dive into our node convolutions.  Recall that this layer had significant changes away from the pyramidal structure including negative weights in the `gcn_1_pos` layer.  This should lead to seeing layers now that have less predictable outputs. Because we do still have the repetition in X though we will see some repetitions in the outputs of these layers. \n",
    "\n",
    "One interesting note is the inversion of signs for the positive and negative layers. with similar weights. Since we subtract the weights this actually becomes additive meaning the model is respecting the two layers similarly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "932f0253-b8e6-451b-89f2-99192dc86585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 3]),\n",
       " tensor([[-0.5363,  0.7994, -0.2656],\n",
       "         [-0.5362,  0.8004, -0.2654],\n",
       "         [-0.5359,  0.8040, -0.2647],\n",
       "         [-0.5361,  0.8021, -0.2651],\n",
       "         [-0.5357,  0.8057, -0.2644],\n",
       "         [-0.5361,  0.8021, -0.2651],\n",
       "         [-0.5363,  0.7996, -0.2656],\n",
       "         [-0.5359,  0.8042, -0.2647],\n",
       "         [-0.5359,  0.8035, -0.2648],\n",
       "         [-0.5361,  0.8019, -0.2651],\n",
       "         [-0.5359,  0.8039, -0.2647],\n",
       "         [-0.5359,  0.8041, -0.2647],\n",
       "         [-0.5360,  0.8032, -0.2649],\n",
       "         [-0.5357,  0.8059, -0.2644],\n",
       "         [-0.5360,  0.8027, -0.2650],\n",
       "         [-0.5361,  0.8020, -0.2651],\n",
       "         [-0.5360,  0.8027, -0.2650],\n",
       "         [-0.5360,  0.8029, -0.2649],\n",
       "         [-0.5359,  0.8039, -0.2647],\n",
       "         [-0.5364,  0.7991, -0.2657],\n",
       "         [-0.5355,  0.8078, -0.2640],\n",
       "         [-0.5361,  0.8020, -0.2651],\n",
       "         [-0.5364,  0.7991, -0.2657],\n",
       "         [-0.5356,  0.8066, -0.2642]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_pos = Apos_blk @ x\n",
    "H_pos =  gcn_1_pos(H_pos)\n",
    "H_pos.size(), H_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "496e0616-1c33-4714-8039-0f11cbff0d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 3]),\n",
       " tensor([[ 0.5366, -0.8011,  0.2656],\n",
       "         [ 0.5366, -0.8011,  0.2656],\n",
       "         [ 0.5366, -0.8011,  0.2656],\n",
       "         [ 0.5313, -0.8042,  0.2612],\n",
       "         [ 0.5361, -0.8014,  0.2652],\n",
       "         [ 0.5419, -0.7980,  0.2700],\n",
       "         [ 0.5361, -0.8014,  0.2652],\n",
       "         [ 0.5327, -0.8034,  0.2624],\n",
       "         [ 0.5369, -0.8009,  0.2659],\n",
       "         [ 0.5327, -0.8034,  0.2624],\n",
       "         [ 0.5378, -0.8004,  0.2667],\n",
       "         [ 0.5423, -0.7978,  0.2704],\n",
       "         [ 0.5333, -0.8030,  0.2629],\n",
       "         [ 0.5346, -0.8023,  0.2639],\n",
       "         [ 0.5367, -0.8011,  0.2657],\n",
       "         [ 0.5333, -0.8030,  0.2629],\n",
       "         [ 0.5379, -0.8003,  0.2667],\n",
       "         [ 0.5417, -0.7981,  0.2699],\n",
       "         [ 0.5346, -0.8023,  0.2639],\n",
       "         [ 0.5367, -0.8011,  0.2657],\n",
       "         [ 0.5333, -0.8030,  0.2629],\n",
       "         [ 0.5333, -0.8030,  0.2629],\n",
       "         [ 0.5403, -0.7990,  0.2687],\n",
       "         [ 0.5385, -0.8000,  0.2672]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_neg = Aneg_blk @ x\n",
    "H_neg =  gcn_1_neg(H_neg)\n",
    "H_neg.size(), H_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ec6737cf-ca1c-4b16-ba6d-13a4955479f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 3]),\n",
       " tensor([[-1.0729,  1.6006, -0.5312],\n",
       "         [-1.0728,  1.6016, -0.5310],\n",
       "         [-1.0725,  1.6051, -0.5304],\n",
       "         [-1.0674,  1.6063, -0.5263],\n",
       "         [-1.0718,  1.6071, -0.5296],\n",
       "         [-1.0780,  1.6001, -0.5352],\n",
       "         [-1.0724,  1.6010, -0.5308],\n",
       "         [-1.0686,  1.6075, -0.5271],\n",
       "         [-1.0729,  1.6044, -0.5307],\n",
       "         [-1.0688,  1.6052, -0.5275],\n",
       "         [-1.0737,  1.6043, -0.5314],\n",
       "         [-1.0782,  1.6018, -0.5351],\n",
       "         [-1.0693,  1.6062, -0.5278],\n",
       "         [-1.0703,  1.6082, -0.5283],\n",
       "         [-1.0727,  1.6037, -0.5307],\n",
       "         [-1.0694,  1.6050, -0.5280],\n",
       "         [-1.0739,  1.6030, -0.5317],\n",
       "         [-1.0777,  1.6010, -0.5349],\n",
       "         [-1.0704,  1.6062, -0.5287],\n",
       "         [-1.0730,  1.6002, -0.5314],\n",
       "         [-1.0689,  1.6108, -0.5269],\n",
       "         [-1.0694,  1.6050, -0.5280],\n",
       "         [-1.0766,  1.5981, -0.5344],\n",
       "         [-1.0742,  1.6066, -0.5315]], grad_fn=<SubBackward0>))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = H_pos - H_neg\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3db9f17-b4cf-4787-aab9-f21b90ca8dd4",
   "metadata": {},
   "source": [
    "#### GCN Block - First Layer Norm\n",
    "Now we'll run our normalization.  because our entries have such high similarity you'll see that our weights are starting to look very similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8a159882-193a-4c34-9be2-bf8579536348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 3]),\n",
       " tensor([[-0.9297,  1.3950, -0.4587],\n",
       "         [-0.9296,  1.3950, -0.4587],\n",
       "         [-0.9295,  1.3951, -0.4589],\n",
       "         [-0.9294,  1.3951, -0.4590],\n",
       "         [-0.9295,  1.3951, -0.4590],\n",
       "         [-0.9298,  1.3950, -0.4586],\n",
       "         [-0.9297,  1.3950, -0.4587],\n",
       "         [-0.9294,  1.3951, -0.4591],\n",
       "         [-0.9296,  1.3951, -0.4589],\n",
       "         [-0.9295,  1.3951, -0.4590],\n",
       "         [-0.9296,  1.3951, -0.4588],\n",
       "         [-0.9297,  1.3950, -0.4586],\n",
       "         [-0.9294,  1.3951, -0.4590],\n",
       "         [-0.9294,  1.3951, -0.4591],\n",
       "         [-0.9296,  1.3951, -0.4588],\n",
       "         [-0.9295,  1.3951, -0.4589],\n",
       "         [-0.9296,  1.3951, -0.4588],\n",
       "         [-0.9297,  1.3950, -0.4586],\n",
       "         [-0.9295,  1.3951, -0.4590],\n",
       "         [-0.9297,  1.3950, -0.4587],\n",
       "         [-0.9293,  1.3951, -0.4592],\n",
       "         [-0.9295,  1.3951, -0.4589],\n",
       "         [-0.9298,  1.3950, -0.4585],\n",
       "         [-0.9295,  1.3951, -0.4589]], grad_fn=<NativeLayerNormBackward0>))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = bn_a(out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4bc8ec-0330-4a34-a191-a12dd4d064e5",
   "metadata": {},
   "source": [
    "#### GCN Block - Leaky ReLU\n",
    "LeakyReLU is our first non-learnable layer.  This again will scale our negative values to have less impact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "466836fd-b208-441c-a24b-bcc34316c73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 3]),\n",
       " tensor([[-0.0930,  1.3950, -0.0459],\n",
       "         [-0.0930,  1.3950, -0.0459],\n",
       "         [-0.0930,  1.3951, -0.0459],\n",
       "         [-0.0929,  1.3951, -0.0459],\n",
       "         [-0.0929,  1.3951, -0.0459],\n",
       "         [-0.0930,  1.3950, -0.0459],\n",
       "         [-0.0930,  1.3950, -0.0459],\n",
       "         [-0.0929,  1.3951, -0.0459],\n",
       "         [-0.0930,  1.3951, -0.0459],\n",
       "         [-0.0929,  1.3951, -0.0459],\n",
       "         [-0.0930,  1.3951, -0.0459],\n",
       "         [-0.0930,  1.3950, -0.0459],\n",
       "         [-0.0929,  1.3951, -0.0459],\n",
       "         [-0.0929,  1.3951, -0.0459],\n",
       "         [-0.0930,  1.3951, -0.0459],\n",
       "         [-0.0929,  1.3951, -0.0459],\n",
       "         [-0.0930,  1.3951, -0.0459],\n",
       "         [-0.0930,  1.3950, -0.0459],\n",
       "         [-0.0929,  1.3951, -0.0459],\n",
       "         [-0.0930,  1.3950, -0.0459],\n",
       "         [-0.0929,  1.3951, -0.0459],\n",
       "         [-0.0929,  1.3951, -0.0459],\n",
       "         [-0.0930,  1.3950, -0.0459],\n",
       "         [-0.0930,  1.3951, -0.0459]], grad_fn=<LeakyReluBackward0>))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = lrelu_1(out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff7c35b-04b8-4467-a1f9-d582b8713653",
   "metadata": {},
   "source": [
    "#### GCN Block - Dropout\n",
    "Dropout is also a non-learnable layer.  We'll execute this to add some noise into our data that looks increasingly more similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "30974a81-91d3-4a6b-b8fd-6d131dde2800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 3]),\n",
       " tensor([[-0.0000,  1.4685, -0.0483],\n",
       "         [-0.0979,  1.4685, -0.0483],\n",
       "         [-0.0978,  1.4685, -0.0483],\n",
       "         [-0.0978,  1.4685, -0.0483],\n",
       "         [-0.0978,  1.4685, -0.0483],\n",
       "         [-0.0979,  1.4684, -0.0483],\n",
       "         [-0.0979,  1.4685, -0.0483],\n",
       "         [-0.0978,  1.4685, -0.0483],\n",
       "         [-0.0978,  1.4685, -0.0483],\n",
       "         [-0.0978,  1.4685, -0.0483],\n",
       "         [-0.0978,  1.4685, -0.0483],\n",
       "         [-0.0979,  1.4684, -0.0483],\n",
       "         [-0.0978,  1.4685, -0.0483],\n",
       "         [-0.0978,  0.0000, -0.0483],\n",
       "         [-0.0978,  1.4685, -0.0483],\n",
       "         [-0.0978,  1.4685, -0.0483],\n",
       "         [-0.0000,  1.4685, -0.0483],\n",
       "         [-0.0000,  1.4684, -0.0483],\n",
       "         [-0.0978,  1.4685, -0.0483],\n",
       "         [-0.0000,  1.4685, -0.0483],\n",
       "         [-0.0978,  1.4686, -0.0483],\n",
       "         [-0.0978,  1.4685, -0.0483],\n",
       "         [-0.0979,  1.4684, -0.0483],\n",
       "         [-0.0978,  1.4685, -0.0483]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = drop(out) \n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb7cd62-940a-4000-9552-9ed059ab5867",
   "metadata": {},
   "source": [
    "#### GCN Block - Second Node Convolution\n",
    "Our second node convolution had less of a drastic impact compared to our first convolution.  We should see some interesting repetition as the layers are pushing our data into a more uniform pattern. If we ran more training loops with varied data this would likely shift with time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bdd713e9-4b49-450b-97f2-905640be5cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 3]),\n",
       " tensor([[-0.0405,  0.5672, -0.0405],\n",
       "         [-0.0429,  0.5623, -0.0429],\n",
       "         [-0.0429,  0.5623, -0.0429],\n",
       "         [-0.0233,  0.6047, -0.0233],\n",
       "         [ 0.0201,  0.6983,  0.0201],\n",
       "         [-0.0233,  0.6047, -0.0233],\n",
       "         [-0.0293,  0.5918, -0.0293],\n",
       "         [-0.0429,  0.5623, -0.0429],\n",
       "         [-0.0356,  0.5781, -0.0356],\n",
       "         [-0.0429,  0.5623, -0.0429],\n",
       "         [-0.0132,  0.6264, -0.0132],\n",
       "         [ 0.0097,  0.6760,  0.0097],\n",
       "         [-0.0336,  0.5822, -0.0336],\n",
       "         [-0.1646,  0.2999, -0.1646],\n",
       "         [-0.0209,  0.6097, -0.0209],\n",
       "         [-0.0336,  0.5822, -0.0336],\n",
       "         [-0.0209,  0.6097, -0.0209],\n",
       "         [-0.0016,  0.6515, -0.0016],\n",
       "         [-0.0233,  0.6047, -0.0233],\n",
       "         [-0.0209,  0.6097, -0.0209],\n",
       "         [-0.0356,  0.5781, -0.0356],\n",
       "         [-0.0356,  0.5781, -0.0356],\n",
       "         [-0.0209,  0.6097, -0.0209],\n",
       "         [-0.0032,  0.6482, -0.0032]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H2_pos = Apos_blk @ out\n",
    "H2_pos =  gcn_2_pos(H2_pos)\n",
    "H2_pos.size(), H2_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9d06cf0c-ec58-4b2c-9052-37953de4195f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 3]),\n",
       " tensor([[ 0.2407, -0.1592,  0.2407],\n",
       "         [ 0.2407, -0.1592,  0.2407],\n",
       "         [ 0.2407, -0.1592,  0.2407],\n",
       "         [ 0.3018, -0.0453,  0.3018],\n",
       "         [ 0.3018, -0.0453,  0.3018],\n",
       "         [ 0.4103,  0.1574,  0.4103],\n",
       "         [ 0.1574, -0.3148,  0.1574],\n",
       "         [ 0.2595, -0.1243,  0.2595],\n",
       "         [ 0.3018, -0.0454,  0.3018],\n",
       "         [ 0.2595, -0.1243,  0.2595],\n",
       "         [ 0.3616,  0.0663,  0.3616],\n",
       "         [ 0.3018, -0.0454,  0.3018],\n",
       "         [ 0.2436, -0.1537,  0.2436],\n",
       "         [ 0.2930, -0.0613,  0.2930],\n",
       "         [ 0.2630, -0.1175,  0.2630],\n",
       "         [ 0.2436, -0.1537,  0.2436],\n",
       "         [ 0.3211, -0.0095,  0.3211],\n",
       "         [ 0.2558, -0.1315,  0.2558],\n",
       "         [ 0.2885, -0.0701,  0.2885],\n",
       "         [ 0.2595, -0.1243,  0.2595],\n",
       "         [ 0.2407, -0.1592,  0.2407],\n",
       "         [ 0.2407, -0.1592,  0.2407],\n",
       "         [ 0.3830,  0.1063,  0.3830],\n",
       "         [ 0.3352,  0.0173,  0.3352]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H2_neg = Aneg_blk @ out\n",
    "H2_neg =  gcn_2_neg(H2_neg)\n",
    "H2_neg.size(), H2_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5a8e3c6b-cf6c-4469-84ac-b06894d2a265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 3]),\n",
       " tensor([[-0.2813,  0.7265, -0.2813],\n",
       "         [-0.2837,  0.7215, -0.2837],\n",
       "         [-0.2837,  0.7215, -0.2837],\n",
       "         [-0.3250,  0.6501, -0.3250],\n",
       "         [-0.2817,  0.7437, -0.2817],\n",
       "         [-0.4336,  0.4473, -0.4336],\n",
       "         [-0.1867,  0.9066, -0.1867],\n",
       "         [-0.3024,  0.6865, -0.3024],\n",
       "         [-0.3373,  0.6235, -0.3373],\n",
       "         [-0.3024,  0.6865, -0.3024],\n",
       "         [-0.3748,  0.5602, -0.3748],\n",
       "         [-0.2921,  0.7213, -0.2921],\n",
       "         [-0.2772,  0.7359, -0.2772],\n",
       "         [-0.4576,  0.3613, -0.4576],\n",
       "         [-0.2839,  0.7272, -0.2839],\n",
       "         [-0.2772,  0.7359, -0.2772],\n",
       "         [-0.3419,  0.6192, -0.3419],\n",
       "         [-0.2573,  0.7829, -0.2573],\n",
       "         [-0.3118,  0.6748, -0.3118],\n",
       "         [-0.2804,  0.7340, -0.2804],\n",
       "         [-0.2763,  0.7374, -0.2763],\n",
       "         [-0.2763,  0.7374, -0.2763],\n",
       "         [-0.4039,  0.5033, -0.4039],\n",
       "         [-0.3384,  0.6309, -0.3384]], grad_fn=<SubBackward0>))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = H2_pos - H2_neg\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb945e-5f0f-44f8-9da3-bcf95e52fdef",
   "metadata": {},
   "source": [
    "#### GCN Block - Second Layer Norm\n",
    "Our final layer norm similarly had minor shifts in the weights so we shouldn't expect major deviation as the weighting mirrored the increase in the middle channel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bef38a6b-0e3d-4703-b11d-d4ed0daba564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 3]),\n",
       " tensor([[-0.7077,  1.4158, -0.7077],\n",
       "         [-0.7077,  1.4158, -0.7077],\n",
       "         [-0.7077,  1.4158, -0.7077],\n",
       "         [-0.7077,  1.4158, -0.7077],\n",
       "         [-0.7077,  1.4158, -0.7077],\n",
       "         [-0.7077,  1.4158, -0.7077],\n",
       "         [-0.7077,  1.4158, -0.7077],\n",
       "         [-0.7077,  1.4158, -0.7077],\n",
       "         [-0.7077,  1.4158, -0.7077],\n",
       "         [-0.7077,  1.4158, -0.7077],\n",
       "         [-0.7077,  1.4158, -0.7077],\n",
       "         [-0.7077,  1.4158, -0.7077],\n",
       "         [-0.7077,  1.4158, -0.7077],\n",
       "         [-0.7077,  1.4158, -0.7077],\n",
       "         [-0.7077,  1.4158, -0.7077],\n",
       "         [-0.7077,  1.4158, -0.7077],\n",
       "         [-0.7077,  1.4158, -0.7077],\n",
       "         [-0.7077,  1.4158, -0.7077],\n",
       "         [-0.7077,  1.4158, -0.7077],\n",
       "         [-0.7077,  1.4158, -0.7077],\n",
       "         [-0.7077,  1.4158, -0.7077],\n",
       "         [-0.7077,  1.4158, -0.7077],\n",
       "         [-0.7077,  1.4158, -0.7077],\n",
       "         [-0.7077,  1.4158, -0.7077]], grad_fn=<NativeLayerNormBackward0>))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = bn_b(out)\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7544d22-5c6f-465e-bc7f-bd7a4283ce32",
   "metadata": {},
   "source": [
    "#### GCN Block - Residual Connection\n",
    "The residual connection will bring our input embedding back in with the main channel and shift the data profile so that the first and last channel are no longer as uniform.  This helps again highlight the value of the residual connection and how it can help smooth out impact from the convolution layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b19c0f4c-a6b8-410f-9b13-ff90fcaed51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 3]),\n",
       " tensor([[-0.6954,  1.4436, -0.6934],\n",
       "         [-0.6836,  1.4636, -0.6816],\n",
       "         [-0.6465,  1.5410, -0.6445],\n",
       "         [-0.7080,  1.4148, -0.7060],\n",
       "         [-0.7554,  1.3164, -0.7534],\n",
       "         [-0.6985,  1.4291, -0.6965],\n",
       "         [-0.6954,  1.4436, -0.6934],\n",
       "         [-0.6836,  1.4636, -0.6816],\n",
       "         [-0.6465,  1.5410, -0.6445],\n",
       "         [-0.7080,  1.4148, -0.7060],\n",
       "         [-0.7554,  1.3164, -0.7534],\n",
       "         [-0.6985,  1.4291, -0.6965],\n",
       "         [-0.6954,  1.4436, -0.6934],\n",
       "         [-0.6836,  1.4636, -0.6816],\n",
       "         [-0.6465,  1.5410, -0.6445],\n",
       "         [-0.7080,  1.4148, -0.7060],\n",
       "         [-0.7554,  1.3164, -0.7534],\n",
       "         [-0.6985,  1.4291, -0.6965],\n",
       "         [-0.6954,  1.4436, -0.6934],\n",
       "         [-0.6836,  1.4636, -0.6816],\n",
       "         [-0.6465,  1.5410, -0.6445],\n",
       "         [-0.7080,  1.4148, -0.7060],\n",
       "         [-0.7554,  1.3164, -0.7534],\n",
       "         [-0.6985,  1.4291, -0.6965]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = out + x\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a07e0a-2903-4a9c-bbda-62e0fa24ea7b",
   "metadata": {},
   "source": [
    "#### GCN Block - Final Leaky ReLU\n",
    "Again the leaky ReLU layer is not learnable but we can see the scaling impact on the negative values as it shrinks them to 1/10th the overall size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "74ddacc8-c1c8-4399-8595-3f06ed7aaec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 6, 3]),\n",
       " tensor([[[-0.0695,  1.4436, -0.0693],\n",
       "          [-0.0684,  1.4636, -0.0682],\n",
       "          [-0.0647,  1.5410, -0.0645],\n",
       "          [-0.0708,  1.4148, -0.0706],\n",
       "          [-0.0755,  1.3164, -0.0753],\n",
       "          [-0.0699,  1.4291, -0.0697]],\n",
       " \n",
       "         [[-0.0695,  1.4436, -0.0693],\n",
       "          [-0.0684,  1.4636, -0.0682],\n",
       "          [-0.0647,  1.5410, -0.0645],\n",
       "          [-0.0708,  1.4148, -0.0706],\n",
       "          [-0.0755,  1.3164, -0.0753],\n",
       "          [-0.0699,  1.4291, -0.0697]],\n",
       " \n",
       "         [[-0.0695,  1.4436, -0.0693],\n",
       "          [-0.0684,  1.4636, -0.0682],\n",
       "          [-0.0647,  1.5410, -0.0645],\n",
       "          [-0.0708,  1.4148, -0.0706],\n",
       "          [-0.0755,  1.3164, -0.0753],\n",
       "          [-0.0699,  1.4291, -0.0697]],\n",
       " \n",
       "         [[-0.0695,  1.4436, -0.0693],\n",
       "          [-0.0684,  1.4636, -0.0682],\n",
       "          [-0.0647,  1.5410, -0.0645],\n",
       "          [-0.0708,  1.4148, -0.0706],\n",
       "          [-0.0755,  1.3164, -0.0753],\n",
       "          [-0.0699,  1.4291, -0.0697]]], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = lrelu_res(x)\n",
    "x = x.view(B_batch, N_nodes, n_embd)\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aabdf5-2805-4f33-89c6-88e38f095f73",
   "metadata": {},
   "source": [
    "### Output Layers AKA Model Heads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d07d5f-3640-43e6-997b-f7edbc354bbb",
   "metadata": {},
   "source": [
    "The output layer saw some of the largest gradients that pushed it away from the incremental initiation.  Given how close the layer is to the loss and that we just have 4 examples and 2 classes on each head, this makes sense as the model could memorize the data pattern in that head. That said, we should see this shift our logit values to be closer to our predicted classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b96743-04be-4925-bce9-c68f24b4380c",
   "metadata": {},
   "source": [
    "#### Output Layer - Node Head\n",
    "With the node head we had an equal balance of positive and negative values in the weights post learning.  This is likely just due to our large step and with time would stabilize.  Overall we did see however a `-2x` multiple of the middle channel compared to the side channels showing us an overall counter balance to what we've seen with other channels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "570afd94-38e1-4b5a-95c5-b7ed5649d750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0651, -0.0767,  0.0768],\n",
       "        [-0.0501,  0.1017, -0.0418]], requires_grad=True)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_head.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c59e3dee-23a2-41c6-98c5-7b0e3deff137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 6, 2]),\n",
       " tensor([[[-0.0580,  0.0906],\n",
       "          [-0.0593,  0.0925],\n",
       "          [-0.0647,  0.1000],\n",
       "          [-0.0559,  0.0878],\n",
       "          [-0.0490,  0.0782],\n",
       "          [-0.0569,  0.0891]],\n",
       " \n",
       "         [[-0.0580,  0.0906],\n",
       "          [-0.0593,  0.0925],\n",
       "          [-0.0647,  0.1000],\n",
       "          [-0.0559,  0.0878],\n",
       "          [-0.0490,  0.0782],\n",
       "          [-0.0569,  0.0891]],\n",
       " \n",
       "         [[-0.0580,  0.0906],\n",
       "          [-0.0593,  0.0925],\n",
       "          [-0.0647,  0.1000],\n",
       "          [-0.0559,  0.0878],\n",
       "          [-0.0490,  0.0782],\n",
       "          [-0.0569,  0.0891]],\n",
       " \n",
       "         [[-0.0580,  0.0906],\n",
       "          [-0.0593,  0.0925],\n",
       "          [-0.0647,  0.1000],\n",
       "          [-0.0559,  0.0878],\n",
       "          [-0.0490,  0.0782],\n",
       "          [-0.0569,  0.0891]]], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_logits = node_head(x)   \n",
    "node_logits.size(), node_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df369c5e-46a9-410e-aa05-ca888a4e8e86",
   "metadata": {},
   "source": [
    "#### Output Layer - Graph Head\n",
    "For our graph head we saw that the weights for the first class were pushed negative and second class positive.  Since 3/4 of our examples are cancerous, seeing the model overly fit to predicting cancerous is likely a safe coarse learning for the model. With more examples and training loops this would be refined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d25575c8-8458-4388-b9ee-65f5542b878e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3]),\n",
       " tensor([[-0.0683,  1.4658, -0.0681],\n",
       "         [-0.0683,  1.4658, -0.0681],\n",
       "         [-0.0683,  1.4658, -0.0681],\n",
       "         [-0.0683,  1.4658, -0.0681]], grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = gene_mask.float().unsqueeze(0)\n",
    "denom = mask.sum(1, keepdim=True).clamp_min(1.0) \n",
    "pooled = (x * mask.unsqueeze(-1)).sum(1) / denom \n",
    "pooled.size(), pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5fc2bbaa-4471-47b6-81d0-71b89c709339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 2]),\n",
       " tensor([[-0.5574,  0.5640],\n",
       "         [-0.5574,  0.5640],\n",
       "         [-0.5574,  0.5640],\n",
       "         [-0.5574,  0.5640]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_logits = graph_head(pooled)  # [B,2]\n",
    "graph_logits.size(), graph_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f80e4a-0ab9-4c31-92d9-d118eabe6bcf",
   "metadata": {},
   "source": [
    "### Updated Loss calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec1ad4b-1191-4437-8e9d-eebd32686ff6",
   "metadata": {},
   "source": [
    "Now we'll calculate the updated loss.  Our first pass's total loss was 0.6931 for each of the heads and a total of 1.3862, on par with random. Since we're passing through the same example and used a fairly high learning rate we should see a significant improvement with just 1 learning pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1b05a226-1598-4941-bfb8-31f26ebd9649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor(0.6932, grad_fn=<NllLossBackward0>),\n",
       "  tensor(0.6931, grad_fn=<NllLossBackward0>)],\n",
       " tensor(1.3863, grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338530be-cd00-4dab-8bb9-66a2de6cc3bf",
   "metadata": {},
   "source": [
    "#### node loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "82e7d820-7fc8-4d16-bb06-c4a63773e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "valid = (gene_mask & (y_node>=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "df00a073-3b4c-4d32-b715-aded599a7ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7072, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nl = F.cross_entropy(node_logits[valid], y_node[valid])\n",
    "losses.append(nl)\n",
    "nl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508afda7-fdf3-4f22-9a0b-691b56b83109",
   "metadata": {},
   "source": [
    "#### graph loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "df05a236-686c-4af6-8673-d3d036e9c16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5624, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gl = F.cross_entropy(graph_logits, y_graph)\n",
    "losses.append(gl)\n",
    "gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a7cd1804-3125-4f85-9ac3-f5d6fd33e8f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2695, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_loss = sum(losses) \n",
    "updated_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3606e784-6c00-4a44-bb19-50ed2dc63dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 round of training resulted in an loss improvment of 0.1167'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'1 round of training resulted in an loss improvment of {loss.item() - updated_loss.item():.4f}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002b2d85-2edb-49be-b145-362a08177dd0",
   "metadata": {},
   "source": [
    "## Training SUCCESS!\n",
    "Our training improved the loss by about **~9%** overall (amount may vary since we didn't set a seed). Interestingly we did see that the loss on the node head got worse but the loss on the graph head outweighed the drop in performance.  If we saw this persist we could change the alpha in our loss sum. There are flaws with this, mainly passing the same example through a second time and a crazy high learning rate, but this helps show the fundamentals of what learning does inside a GCN style GNN model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af9b6c8-ceff-4217-bab3-232a47ffa034",
   "metadata": {},
   "source": [
    "## Logit to Token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe75d505-17ea-4ba8-9aec-b3936bb8b444",
   "metadata": {},
   "source": [
    "For our last piece of code on this notebook, we'll actually now convert our logits for each head into actual class predictions.  For class prediction we apply argmax to the logits. Argmax returns the input index at which an array attains its maximum: \n",
    "$$\n",
    "\\operatorname*{argmax}_x f(x)={x\\mid f(x)\\ge f(y)\\ \\forall y}\n",
    "$$\n",
    "Since we're trying to find the class, argmax will return the index at which the class is maximized. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d01fd9-294c-4932-83f1-925322be8d97",
   "metadata": {},
   "source": [
    "### Graph class prediction \n",
    "As a reminder our graph head predicts whether a graph is cancerous or not.  We can see here that it overly defaulted to true which makes sense given the input data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2f053a07-08b4-433e-a605-5019b73c41d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph preds: [1, 1, 1, 1]  True: [0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Graph preds:\", graph_logits.argmax(-1).tolist(), \" True:\", y_graph.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3c4fbd-f629-4969-9158-c94b39b996b6",
   "metadata": {},
   "source": [
    "### Node class prediction \n",
    "As a reminder our node head predicts the cell type for a node in every single example.  It looks at the gene type and, with the assumption it's upregulated, predicts the cell type it's most commonly upregulated in. We could aggregate across different dimensions using argmax again to get a graph level or gene level predictor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ce1a9122-2b36-4254-b1dd-6b0046c8f8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-node preds (genes only):\n",
      "sample0: CD3D: pred=B true=T X | LCK: pred=B true=T X | ZAP70: pred=B true=T X | CD19: pred=B true=B\n",
      "sample1: CD3D: pred=B true=T X | LCK: pred=B true=B | ZAP70: pred=B true=T X | CD19: pred=B true=B\n",
      "sample2: CD3D: pred=B true=B | LCK: pred=B true=T X | ZAP70: pred=B true=T X | CD19: pred=B true=B\n",
      "sample3: CD3D: pred=B true=T X | LCK: pred=B true=T X | ZAP70: pred=B true=B | CD19: pred=B true=B\n"
     ]
    }
   ],
   "source": [
    "print(\"Per-node preds (genes only):\")\n",
    "label = {0: \"T\", 1: \"B\"}\n",
    "pred = node_logits.argmax(-1)\n",
    "ytrue = y_node\n",
    "valid = (ytrue >= 0)\n",
    "names = node_order\n",
    "\n",
    "\n",
    "for b in range(pred.size(0)):\n",
    "    idxs = torch.nonzero(valid[b], as_tuple=False).squeeze(1).tolist()\n",
    "    parts = []\n",
    "    for i in idxs:\n",
    "        p = pred[b, i].item()\n",
    "        t = ytrue[b, i].item()\n",
    "        cor = '' if p ==  t else ' X'\n",
    "        parts.append(f'{names[i]}: pred={label[p]} true={label[t]}{cor}')\n",
    "    print(f'sample{b}: ' + ' | '.join(parts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
