{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2ea846f-d5a9-4f61-b3b1-fed90e2edde3",
   "metadata": {},
   "source": [
    "# RNN LSTM Explainer\n",
    "\n",
    "The goal is to walk through RNNs (recurrent neural networks), there are 2 flavors: GRUs and LSTMs. We'll focus on Long Short-Term Memory (LSTM). LSTM's have a unique structure where they process data sequentially which enables them to take in both the current and a prior state, making them great for sequential data like time series or streamed data.  The core unit of the LSTM takes in both the data you want to use to make an inference along with any previous context. LSTMs' memory cell uses three “gates”: an input gate, a forget gate, and an output gate. These gates allow the model to decide which information to keep, which to forget, and when to use it. GRU's only have the 2 gates for input and forget. While this is great there are a few dangers of RNNs: primarily that improper setup and training can lead to vanishing/exploding gradients and their sequential nature can consume a lot of resources.  The LSTM structure was built to fight the vanishing gradient problem. When coupled with grdient clipping it can help fight the vanishing/exploding gradient problem.\n",
    "\n",
    "To help display how the LSTMs works, we'll use the first sentence from the [linear algebra wiki page](https://en.wikipedia.org/wiki/Linear_algebra) and [lu decomposition wiki page](https://en.wikipedia.org/wiki/LU_decomposition) as the topic is fitting and it shows us some non-standard patterns.  We'll take on a more simple task to many other notebooks in this repository where we have a few sentences of text and want to predict some other text, in this case the next token. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08f8d05-f237-4a81-8f88-be839e4a9e51",
   "metadata": {},
   "source": [
    "## Text Prep/Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb1237d-fd4f-43bc-ba02-88e82368b385",
   "metadata": {},
   "source": [
    "We'll start with a common preprocessing step of tokenizing the data.  This converts the string text into an array of numbers that can be used during the training loop.  I've built a very subtle byte-pair encoding that has each unique character that appears and the top 5 merges. This keeps our vocab size small and manageable for this example. Typically the vocab size is in the 100K+ range. A great library for this is `tiktoken`. Tokenization simply finds the longest pattern of characters that's in common with what was trained and replaces it with an integer that represents it.  This way we turn the text into a numeric array to simplify computing. import torch\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baeedf57-4218-4b0e-be46-217723a9034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef2240c1-dd4e-4f8a-9fb1-acff3ff200c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBPETokenizer:\n",
    "    def __init__(self, num_merges=5, eot_token='<|endoftext|>'):\n",
    "        self.num_merges = num_merges\n",
    "        self.eot_token = eot_token\n",
    "        self.eot_id = None\n",
    "        self.merges = []\n",
    "        self.pair_ranks = {}\n",
    "        self.vocab = {}\n",
    "        self.id_to_token = {}\n",
    "\n",
    "    def _add_token(self, tok):\n",
    "        if tok in self.vocab:\n",
    "            return self.vocab[tok]\n",
    "        i = len(self.vocab)\n",
    "        self.vocab[tok] = i\n",
    "        self.id_to_token[i] = tok\n",
    "        return i\n",
    "\n",
    "    def _get_bigrams(self, seq):\n",
    "        for i in range(len(seq) - 1):\n",
    "            yield (seq[i], seq[i + 1])\n",
    "\n",
    "    def _merge_once(self, seq, pair):\n",
    "        a, b = pair\n",
    "        out = []\n",
    "        i = 0\n",
    "        while i < len(seq):\n",
    "            if i < len(seq) - 1 and seq[i] == a and seq[i + 1] == b:\n",
    "                out.append(a + b)\n",
    "                i += 2\n",
    "            else:\n",
    "                out.append(seq[i])\n",
    "                i += 1\n",
    "        return out\n",
    "\n",
    "    def train(self, corpus):\n",
    "        # corpus: list[str]\n",
    "        text = ''.join(corpus).lower()\n",
    "        seq = list(text)\n",
    "        merges = []\n",
    "        for _ in range(self.num_merges):\n",
    "            counts = Counter(self._get_bigrams(seq))\n",
    "            if not counts: break\n",
    "            best_pair, _ = counts.most_common(1)[0]\n",
    "            merges.append(best_pair)\n",
    "            seq = self._merge_once(seq, best_pair)\n",
    "        self.merges = merges\n",
    "        self.pair_ranks = {p: i for i, p in enumerate(self.merges)}\n",
    "\n",
    "        self.vocab = {}\n",
    "        self.id_to_token = {}\n",
    "        for ch in sorted(set(text)):\n",
    "            self._add_token(ch)\n",
    "        for a, b in self.merges:\n",
    "            self._add_token(a + b)\n",
    "        self.eot_id = self._add_token(self.eot_token)\n",
    "\n",
    "    def encode(self, text, force_last_eot=True):\n",
    "        # treat literal eot marker as special; remove it from content\n",
    "        if self.eot_token in text:\n",
    "            text = text.replace(self.eot_token, '')\n",
    "        seq = list(text)\n",
    "\n",
    "        # make sure all seen base chars exist\n",
    "        for ch in set(seq):\n",
    "            if ch not in self.vocab:\n",
    "                self._add_token(ch)\n",
    "\n",
    "        # greedy BPE using learned pair ranks\n",
    "        if self.merges:\n",
    "            while True:\n",
    "                best_pair, best_rank = None, None\n",
    "                for p in self._get_bigrams(seq):\n",
    "                    r = self.pair_ranks.get(p)\n",
    "                    if r is not None and (best_rank is None or r < best_rank):\n",
    "                        best_pair, best_rank = p, r\n",
    "                if best_pair is None:\n",
    "                    break\n",
    "                seq = self._merge_once(seq, best_pair)\n",
    "\n",
    "        # ensure all tokens in seq exist in vocab (e.g., if new chars appeared)\n",
    "        for tok in seq:\n",
    "            if tok not in self.vocab:\n",
    "                self._add_token(tok)\n",
    "\n",
    "        ids = [self.vocab[tok] for tok in seq]\n",
    "\n",
    "        # FORCE: append EOT id if not already last\n",
    "        if force_last_eot:\n",
    "            if not ids or ids[-1] != self.eot_id:\n",
    "                ids.append(self.eot_id)\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # drop trailing EOT if present\n",
    "        if ids and self.eot_id is not None and ids[-1] == self.eot_id:\n",
    "            ids = ids[:-1]\n",
    "        toks = [self.id_to_token[i] for i in ids]\n",
    "        return ''.join(toks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b35d09cf-0ce1-47a0-a93f-8213bea8f711",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_example_1 = r'''Linear algebra is central to almost all areas of mathematics. For instance, linear algebra is fundamental in modern presentations of geometry, including for defining basic objects such as lines, planes and rotations. Also, functional analysis, a branch of mathematical analysis, may be viewed as the application of linear algebra to function spaces.'''\n",
    "raw_example_2 = r'''In numerical analysis and linear algebra, lower–upper (LU) decomposition or factorization factors a matrix as the product of a lower triangular matrix and an upper triangular matrix (see matrix multiplication and matrix decomposition).'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7064c634-a608-4a62-93fe-c89219aa5dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 'a'), ('a', 't'), ('i', 'n'), (' ', 'm'), ('i', 'o')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = SimpleBPETokenizer(num_merges=5)\n",
    "tok.train([raw_example_1,raw_example_2])\n",
    "tok.merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "133f8d70-16cd-4327-91a2-4e887612bcdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '(': 1,\n",
       " ')': 2,\n",
       " ',': 3,\n",
       " '.': 4,\n",
       " 'a': 5,\n",
       " 'b': 6,\n",
       " 'c': 7,\n",
       " 'd': 8,\n",
       " 'e': 9,\n",
       " 'f': 10,\n",
       " 'g': 11,\n",
       " 'h': 12,\n",
       " 'i': 13,\n",
       " 'j': 14,\n",
       " 'l': 15,\n",
       " 'm': 16,\n",
       " 'n': 17,\n",
       " 'o': 18,\n",
       " 'p': 19,\n",
       " 'r': 20,\n",
       " 's': 21,\n",
       " 't': 22,\n",
       " 'u': 23,\n",
       " 'v': 24,\n",
       " 'w': 25,\n",
       " 'x': 26,\n",
       " 'y': 27,\n",
       " 'z': 28,\n",
       " '–': 29,\n",
       " ' a': 30,\n",
       " 'at': 31,\n",
       " 'in': 32,\n",
       " ' m': 33,\n",
       " 'io': 34,\n",
       " '<|endoftext|>': 35}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9804d527-b590-4bb5-8158-8cdcbc179719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tok.vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29443925-8749-4b7c-a500-4a3bdbf808d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35, 15, 32,  9,  5, 20, 30, 15, 11,  9,  6, 20,  5,  0, 13, 21,  0,  7,\n",
       "         9, 17, 22, 20,  5, 15,  0, 22, 18, 30, 15, 16, 18, 21, 22, 30, 15, 15,\n",
       "        30, 20,  9,  5, 21,  0, 18, 10, 33, 31, 12,  9, 16, 31, 13,  7, 21,  4,\n",
       "         0, 10, 18, 20,  0, 32, 21, 22,  5, 17,  7,  9,  3,  0, 15, 32,  9,  5,\n",
       "        20, 30, 15, 11,  9,  6, 20,  5,  0, 13, 21,  0, 10, 23, 17,  8,  5, 16,\n",
       "         9, 17, 22,  5, 15,  0, 32, 33, 18,  8,  9, 20, 17,  0, 19, 20,  9, 21,\n",
       "         9, 17, 22, 31, 34, 17, 21,  0, 18, 10,  0, 11,  9, 18, 16,  9, 22, 20,\n",
       "        27,  3,  0, 32,  7, 15, 23,  8, 32, 11,  0, 10, 18, 20,  0,  8,  9, 10,\n",
       "        32, 32, 11,  0,  6,  5, 21, 13,  7,  0, 18,  6, 14,  9,  7, 22, 21,  0,\n",
       "        21, 23,  7, 12, 30, 21,  0, 15, 32,  9, 21,  3,  0, 19, 15,  5, 17,  9,\n",
       "        21, 30, 17,  8,  0, 20, 18, 22, 31, 34, 17, 21,  4, 30, 15, 21, 18,  3,\n",
       "         0, 10, 23, 17,  7, 22, 34, 17,  5, 15, 30, 17,  5, 15, 27, 21, 13, 21,\n",
       "         3, 30,  0,  6, 20,  5, 17,  7, 12,  0, 18, 10, 33, 31, 12,  9, 16, 31,\n",
       "        13,  7,  5, 15, 30, 17,  5, 15, 27, 21, 13, 21,  3, 33,  5, 27,  0,  6,\n",
       "         9,  0, 24, 13,  9, 25,  9,  8, 30, 21,  0, 22, 12,  9, 30, 19, 19, 15,\n",
       "        13,  7, 31, 34, 17,  0, 18, 10,  0, 15, 32,  9,  5, 20, 30, 15, 11,  9,\n",
       "         6, 20,  5,  0, 22, 18,  0, 10, 23, 17,  7, 22, 34, 17,  0, 21, 19,  5,\n",
       "         7,  9, 21,  4, 35, 35, 32,  0, 17, 23, 16,  9, 20, 13,  7,  5, 15, 30,\n",
       "        17,  5, 15, 27, 21, 13, 21, 30, 17,  8,  0, 15, 32,  9,  5, 20, 30, 15,\n",
       "        11,  9,  6, 20,  5,  3,  0, 15, 18, 25,  9, 20, 29, 23, 19, 19,  9, 20,\n",
       "         0,  1, 15, 23,  2,  0,  8,  9,  7, 18, 16, 19, 18, 21, 13, 22, 34, 17,\n",
       "         0, 18, 20,  0, 10,  5,  7, 22, 18, 20, 13, 28, 31, 34, 17,  0, 10,  5,\n",
       "         7, 22, 18, 20, 21, 30, 33, 31, 20, 13, 26, 30, 21,  0, 22, 12,  9,  0,\n",
       "        19, 20, 18,  8, 23,  7, 22,  0, 18, 10, 30,  0, 15, 18, 25,  9, 20,  0,\n",
       "        22, 20, 13,  5, 17, 11, 23, 15,  5, 20, 33, 31, 20, 13, 26, 30, 17,  8,\n",
       "        30, 17,  0, 23, 19, 19,  9, 20,  0, 22, 20, 13,  5, 17, 11, 23, 15,  5,\n",
       "        20, 33, 31, 20, 13, 26,  0,  1, 21,  9,  9, 33, 31, 20, 13, 26, 33, 23,\n",
       "        15, 22, 13, 19, 15, 13,  7, 31, 34, 17, 30, 17,  8, 33, 31, 20, 13, 26,\n",
       "         0,  8,  9,  7, 18, 16, 19, 18, 21, 13, 22, 34, 17,  2,  4, 35])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eot = tok.eot_id\n",
    "tokens = []\n",
    "for example in [raw_example_1, raw_example_2]:\n",
    "    tokens.extend([eot])\n",
    "    tokens.extend(tok.encode(example.lower()))\n",
    "all_tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775f0cb1-fdac-400c-a80d-1a5da154434c",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bd76b6-dfd5-4f08-a289-630af13649f0",
   "metadata": {},
   "source": [
    "A machine learning model forward pass now uses the tokenization information, runs several layers of linear algebra on it, and then \"predicts\" the next token. When it is noisy (like you will see in this example), this process results in gibberish.  The training process changes the noise to pattern during the \"backward pass\" as you'll see.    We'll show 3 steps that are focused on training:\n",
    "1. **Data Loading** `x, y = train_loader.next_batch()` - this step pulls from the raw data enough tokens to complete a forward and backward pass.  If the model is inference only, this step is replaced with taking in the inference input and preparing it similarly as the forward pass.\n",
    "2. **Forward Pass** `logits, loss = model(x, y)` - using the data and the model architecture to predict the next token. When training we also compare against the expected to get loss, but in infrerence, we use the logits to complete the inference task.\n",
    "3. **Back Propagation, aka Backward Pass & Training** `loss.backward(); optimizer.step()` - using differentials to understand what parameters most impact the forward pass' impact on its prediction, comparing that against what is actually right based on the data loading step, and then making very minor adjustments to the impactful parameters with the hope it improves future predictions.\n",
    "\n",
    "The we'll show a final **Forward Pass** with the updated weights we did in #3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da8d4c4-ae00-465d-ae6f-ea6e1be858f0",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209d7e34-19d0-4c9b-bc7c-b89c09052167",
   "metadata": {},
   "source": [
    "To start, we need to get enough data to run the forward and backward passes.  Since our total dataset is likely too big to hold all at once in real practice, we would read just enough file information into memory so that we can run the passes, leaving memory and compute to be used on the passes instead of static data holding. \n",
    "To start, we have to identify the batch size and the model context length to determine how much data we need.  Consequently, these dimensions also form 2 of the 3 dimensions in the initial matrix.\n",
    "- **Batch Size (B)** - This is the number of examples you'll train on in a single pass. \n",
    "- **Context Length (T)** - This is the max number of tokens that a model can use in a single pass to generate the next token. If an example is below this length, it can be padded.\n",
    "  \n",
    "*Ideally both B and T are multiples of 2 to work nicely with chip architecture. This is a common theme across the board*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1349878b-f4cd-4691-93fc-f8f2d76cb28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 2 # Batch\n",
    "T = 8 # context length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96920d53-8372-4f67-a6c2-289dd385690c",
   "metadata": {},
   "source": [
    "To start, we need to pull from our long raw_token list enough tokens for the forward pass. To be able to satisfy training `B` Batches `T` Context length, we need to pull out `B*T` tokens to slide the context window across the examples enough to satisfy the batch size.  Since the training will attempt to predict the last token given the previous tokens in context, we also need 1 more token at the end so that the last training example in the last batch can have the next token to validate against. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0eda05fa-9c31-4618-ae8c-ed1317467795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35, 15, 32,  9,  5, 20, 30, 15, 11,  9,  6, 20,  5,  0, 13, 21,  0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_position = 0\n",
    "tok_for_training = all_tokens[current_position:current_position + B*T +1 ]\n",
    "tok_for_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665501c6-08d2-4e76-b672-a22d40d529f1",
   "metadata": {},
   "source": [
    "Now that we have our initial tokens to train on, we now need to convert it to a matrix that's ready for training. In this step we'll need to create our batches and setup two different arrays: 1/ the input, `x`, tokens that will result in 2/ the output `y` tokens. To create each example in the batch, every `T` tokens will be placed into its own row. \n",
    "\n",
    "Recall that training takes in a string of tokens the length of the context and then predicts the next token. Recall that when we extracted `tok_for_training` we added 1 extra token so that we can evaluate the prediction for the last example. Because of this, the input, `x`, will be all of the tokens up to the second to last element `[:-1]`.  \n",
    "\n",
    "It might be natural to think the output `y` would then just be the last token.But this is actually wasting valuable training loops.  Yes, there is the example that fills the context `T`, but we also have enough tokens in `tok_for_training` where any context length of `n` where `n<T` can also be used for inference since we have the `n+1` token available.  You can think of the following example:\n",
    "\n",
    "sentence: `Hi I am learning`. This sentence contains the following \"next tokens\" that can be learned:\n",
    "1. x: Hi I am  | y: learning\n",
    "2. x: Hi I     | y: am\n",
    "3. x: Hi       | y: I\n",
    "\n",
    "Because we have this triangle to create, our `y` can be much larger.  We can start with the second token and, go all the way to the last element we added for the last example `[1:'`.   \n",
    "\n",
    "\n",
    "We will now put this together and do the following:\n",
    "1. Extract the input `x` and then split it into an example for each batch `B`\n",
    "2. Extract the output `y` and then split it into an example for each batch `B`\n",
    "\n",
    "*Note: View can take `-1` which allows the matrix to infer the dimension so we do not need to pass in `T`, but given how many matrices we'll work with we want to make sure we're controlling the dimensions or erroring out if they do not match our expectations.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f36ec177-74d7-4cb9-b84d-75a10a6c6694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[35, 15, 32,  9,  5, 20, 30, 15],\n",
       "        [11,  9,  6, 20,  5,  0, 13, 21]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=tok_for_training[:-1].view(B, T)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8d87eb2-5bc8-44cb-80d4-806c3a3bf93d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15, 32,  9,  5, 20, 30, 15, 11],\n",
       "        [ 9,  6, 20,  5,  0, 13, 21,  0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=tok_for_training[1:].view(B, T)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd96f05-28c7-482a-bd47-d8226e65d235",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e57349b-3625-46e2-9da3-7a1fbccb43b2",
   "metadata": {},
   "source": [
    "<img src=\"explainer_screenshots/lstm/full_network.png\" width=\"200\">\n",
    "\n",
    "During training, the forward pass takes a string of tokens in and predicts the likelihood that each output token is the next \"n\" tokens.  This step as we'll look at it is focused on training where we'll pass in the input `x`, carry that input through the layers, and generate a matrix of the probability of each token being the next one, something we call `logits`. During the forward pass, since this is an RNN, we will actually pass each example through recursively based on the token length of available. For example, if we have 3 tokens in the example and we are deriving the 4th, we would hit our LSTM block 3 times through:\n",
    "1. First with the embedding for the token `[0]` and no previous token\n",
    "2. Then with the embedding for the token `[1]` and the output of the LSTM unit's processing of token `[0]` as the previous $H_{T-1}$\n",
    "3. Finally with the embedding for the token `[2]` and the output of the LSTM unit's processing of token `[1]` as the previous $H_{T-1}$\n",
    "\n",
    "At the end of the forward pass at the end we then compare the probability to the actual next token in `y` and calculate `loss` based on the difference. \n",
    "\n",
    "*Note that we will do some layer initialization to simplify following along.  In reality layers are often initialized to normal distribution with some adjustments made for parameter sizes to keep the weights properly noisy.  We will not cover initialization in this series*\n",
    "\n",
    "We first rederive the batch size and context size based on the input to improve flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83e785e1-7328-4b6f-83f9-a98247f06bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa762304-604b-458a-b0d5-2d787de76448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_batch, T_context = x.size()\n",
    "B_batch, T_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a6d9984-9890-4b37-b538-5a09fdb7efed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 36)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_embd = 4 # level of embedding of input tokens\n",
    "n_embd, vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbc6e54-7bdf-4518-87de-856d062c94ff",
   "metadata": {},
   "source": [
    "### Input Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf3ccab-d62d-41f8-9539-4a257e9ca2a8",
   "metadata": {},
   "source": [
    "<img src=\"explainer_screenshots/lstm/input_layer.png\" width=\"200\">\n",
    "\n",
    "We'll first create an initial **embedding layer** for our input tokens.  Since the RNN iteratively processes input data, position embedding info is not needed, though, it can always be added to give the model more depth. The embedding weights are `vocab_size X n_embd` and simply store weights that correspond with each token.  The more embedding layers added the more complex data the model can learn. After the embedding layer we'll then perform dropout. The **dropout layer** simply applies 0's  randomly up to the defined level percentage and normalizes the remaining probabilities in the row. Since RNNs recycle the same parameters across timesteps, features can co-adapt and overfit quickly. Variational dropout breaks these temporal co-adaptations and adds needed noise where there’s little implicit regularization. Other model architectures like transformers have strong built-in stabilizers (LayerNorm, residuals, attention) and massive pretraining, so they generally need far less dropout. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1978456a-c933-45be-84bd-ac109731319d",
   "metadata": {},
   "source": [
    "**Embedding** \n",
    "\n",
    "To start we'll initialize our embeddings with a weight of 1.000 so that all inputs are equally weighted. We'll also set our embedding dimension to 4 to allow for some levels of complexity. Since our initiation weights are equal, we expect that the output of embedding will for now be equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be607d75-1447-4339-9145-0f18f7e294b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wte = nn.Embedding(vocab_size, n_embd)\n",
    "torch.nn.init.ones_(wte.weight)\n",
    "wte.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10b37914-4be6-4ed6-ad7b-daa43db954ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 4]),\n",
       " tensor([[[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]]], grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = wte(x)\n",
    "x.shape, x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf83182-f32a-4da0-93c8-8bb4f41cce49",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "Now, before our Recurrent unit, we'll perform dropout. Dropout will randomly zero out any value effectively removing that specific node from impacting prediction. Since this is Bernoulli based dropout, in addition to zeroing out weights the surviving entries are scaled by $1/(1-p)$. During training this helps with generalization and fights fixation. You can quickly see the dropout's impact on the embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfa012d0-91b1-474d-b961-ebfb4ef1889f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dropout(p=0.1, inplace=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout = nn.Dropout(0.1)\n",
    "dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29b56d86-9cfe-4b21-ba08-1b1dba07fc46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.1111, 1.1111, 1.1111, 1.1111],\n",
       "         [1.1111, 1.1111, 1.1111, 1.1111],\n",
       "         [1.1111, 1.1111, 0.0000, 1.1111],\n",
       "         [1.1111, 1.1111, 1.1111, 1.1111],\n",
       "         [1.1111, 1.1111, 1.1111, 1.1111],\n",
       "         [1.1111, 1.1111, 1.1111, 1.1111],\n",
       "         [1.1111, 1.1111, 0.0000, 0.0000],\n",
       "         [1.1111, 1.1111, 1.1111, 1.1111]],\n",
       "\n",
       "        [[1.1111, 0.0000, 1.1111, 1.1111],\n",
       "         [1.1111, 1.1111, 1.1111, 1.1111],\n",
       "         [1.1111, 1.1111, 1.1111, 1.1111],\n",
       "         [1.1111, 1.1111, 1.1111, 1.1111],\n",
       "         [1.1111, 1.1111, 1.1111, 1.1111],\n",
       "         [0.0000, 1.1111, 1.1111, 1.1111],\n",
       "         [1.1111, 1.1111, 1.1111, 1.1111],\n",
       "         [1.1111, 1.1111, 1.1111, 1.1111]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = dropout(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9443d17-96e5-4854-9b99-deb9ee747db1",
   "metadata": {},
   "source": [
    "### Recurrent Unit - LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9c2ecb-580d-402c-b932-f00d799ab5b1",
   "metadata": {},
   "source": [
    "<img src=\"explainer_screenshots/lstm/lstm_details.png\" width=\"600\">\n",
    "\n",
    "The recurrent unit we'll explore in this notebook is the Long-term Short-term memory unit, or LSTM.  Like other recurrent units, this acts iteratively on specific time points of X labeled $X_t$.  If there are more time points, for us meaning more tokens in the string, the unit then takes in that weight, represented as $H_{prev}$, and the next time point, represented again as $X_t$ and reruns the unit, until all time points are exhausted. \n",
    "\n",
    "Our recurrent unit in this case creates 2 outputs: the *memory cell* $C_t$ and the *hidden state* $H_t$.  During recursion, both are input in parallel with our input context $X_t$ and maintained with the memory cell acting as our long term memory but once recursion is complete only the hidden state moves forward.  \n",
    "\n",
    "To manage the impact of the input context $X_t$, memory cell $C_t$ and hidden state $H_t$, LSTMs use 3 gates: input gate $I_t$, forget gate $F_t$, and output gate $O_t$. These gates created by simply applying a linearization and summation to the input context and incoming hidden state,  are wrapped in sigmoid function pushing their values to between $\\left(0,1\\right)$ act as learnable proportions that control how much of each of the input, memory, and hidden state are maintained in a recursion.  By having these 3 gates combined with the use of the Hadamard product to force gradients down to an element-wise level, during training RNNs can learn the impacts of a token and pattern whether its in the context window or if it's been previously seen. You can think of the gates as being a learnable Switch that says \"when making a prediction of Y, how much of X should I use and how much of the history should I use\".  With LSTMs, as the name implies, we have 2 types of history, the long term \n",
    "\n",
    "Mentally, the **Input Gate** $I_t$ determines how much a given input, we'll call the input state $\\tilde{C}$, should be added to the memory cell $C_t$, and, by extension, the hidden state $H_t$.  The input state for us will be a combination of the input context $X_t$ and incoming hidden state $H_t$  as the point of RNNs is to use both the incoming context and the longer term context to make a prediction for the output.  We do this by doing a linearization and summation to the input context and incoming hidden state and then normalizing using $tanh$ to squeeze the values to $\\left(-1,1\\right)$. \n",
    "\n",
    "The remaining part of the memory cell  uses the **Forget Gate** $F_t$.  The forget gate controls how much of the current longer term memory is retained or forgotten during the memory cell update.  The forget gate is multiplied, using the Hadamard product, by the incoming memory cell $C_t$. This product is then summed with the Hadamard product of the input gate and input state to result in our updated memory cell $C_{t+1}$\n",
    "\n",
    "The **Output Gate** $O_t$ then determines how much of the memory cell to use to  influence the current time steps updated hidden step $H_{t+1}$.  To get the hidden step update, we take the Hadamard product of the output gate with a $tanh$ normalization of the memory state which pulls the values to $\\left(-1,1\\right)$.\n",
    "\n",
    "All together this results in the following:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "I_t &= \\sigma\\!\\left(X_t W_{xi} + H_{t} W_{hi} + b_i\\right)\\ \\ \\ \\ \\text{Input Gate},\\\\\n",
    "F_t &= \\sigma\\!\\left(X_t W_{xf} + H_{t} W_{hf} + b_f\\right)\\ \\ \\ \\ \\text{Forget Gate},\\\\\n",
    "O_t &= \\sigma\\!\\left(X_t W_{xo} + H_{t} W_{ho} + b_z\\right)\\ \\ \\ \\ \\text{Output Gate},\\\\\n",
    "\\\\\n",
    "\\tilde{C} &= tanh\\!\\left(X_t W_{xc} + H_{t} W_{hc} + b_c\\right),\\\\\n",
    "C_{t+1} &= F_t\\odot C_t + I_t\\odot \\tilde{C},\\\\\n",
    "H_{t+1} &= O_t\\odot tanh\\left(C_{t+1}\\right).\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0138029-919b-4752-ab26-ed276e0d870b",
   "metadata": {},
   "source": [
    "Mathematically this gives us a nice simple set of formulas, but we need to actually write this as a program. If we look closely we have 4 sets of weights acting, 3 for the gates, and 1 for the input state:\n",
    "* the input $X_t$: $\\{W_{xi}, W_{xf}, W_{xo}, W_{xc}\\}$. *Notice the subscript is \"x\\<gate or state>\"*\n",
    "* the previous $H_{prev}$: $\\{W_{hi}, W_{hf}, W_{ho}, W_{hc}\\}$.\n",
    "\n",
    "To simplify this weight we'll start by creating a linear later that has 4x the hidden layers, separate it, and then complete the rest of the calculations. This allows us to gain efficiencies in learning and improve our code readability.  The algorithmic representation then looks as follows:\n",
    "\n",
    "```\n",
    "xi, xf, xo, xc = x2g(x_t).split(hidden_size, dim=-1)\n",
    "hi, hf, ho, hc = h2g(h_prev).split(hidden_size, dim=-1)\n",
    "\n",
    "#input, forget, and output gate \n",
    "input_gate =  torch.sigmoid(xi + hi)\n",
    "forget_gate =  torch.sigmoid(xf + hf)\n",
    "output_gate =  torch.sigmoid(xo + ho)\n",
    "\n",
    "input_state = torch.tanh(xc + hc)\n",
    "\n",
    "c_next = (forget_gate*c_t) + (input_gate*input_state)\n",
    "h_next = output_gate * torch.tanh(c_next)\n",
    "```\n",
    "**Starting Example** While LSTMs are iterative, we'll first walk through 2 pass of the LSTM. After that we'll show the loop for the rest of the context window/examples. This will help us understand what is happening inside a pass and then how the impact of the weights adds up over the context length. To keep things simple we'll set the `hidden_size` to 3. Let's start by initiating our weights.  To visualize the impact of each set of weights, I'll initialize the current context and the previous context weights to two different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d21e8264-8bef-4002-af8e-fb022891468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb643927-6944-4785-ac3a-935d1f08b17e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 4]),\n",
       " Parameter containing:\n",
       " tensor([[0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000]], requires_grad=True),\n",
       " torch.Size([12]),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2g = nn.Linear(n_embd, 4 * hidden_size, bias=True)\n",
    "torch.nn.init.constant_(x2g.weight, 0.500)\n",
    "torch.nn.init.zeros_(x2g.bias)\n",
    "x2g.weight.size(), x2g.weight, x2g.bias.size(), x2g.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cd4aed3-b539-488f-a4ea-0f081ebc100b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 3]),\n",
       " Parameter containing:\n",
       " tensor([[0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500]], requires_grad=True),\n",
       " torch.Size([12]),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2g = nn.Linear(hidden_size, 4 * hidden_size, bias=True)\n",
    "torch.nn.init.constant_(h2g.weight, 0.250)\n",
    "torch.nn.init.zeros_(h2g.bias)\n",
    "h2g.weight.size(), h2g.weight, h2g.bias.size(), h2g.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abb3605-3815-4c0b-ba74-2705e5d4ad20",
   "metadata": {},
   "source": [
    "#### LSTM - Frist Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14ec0de-3e2d-417a-a013-63a8feb52ccd",
   "metadata": {},
   "source": [
    "##### LSTM - Initialize the hidden state and memory cell $\n",
    "Let's start first by setting up our starting hidden state and memory cell weights.  Since we are at 'time 0' in training, there are no previous states or memory yet. Because of this we'll actually set them to 0's so that there is no impact from a previous state and all of the inference prediction comes from `x_t`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a467e8f-19c3-4e2b-bb38-fa160c4cc9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_next = torch.zeros(B_batch, hidden_size) \n",
    "c_next = torch.zeros(B_batch, hidden_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ea925e-09b9-423c-994c-28bb2061765d",
   "metadata": {},
   "source": [
    "##### LSTM - Weight calculation and incoming linearization\n",
    "\n",
    "Now we'll calculate the impact of the weights by our input `x_t` and hidden layer `h_t` \n",
    "We'll then pass these variables into the linear layer to perform $Ax+b$ and split the result it up into our 4 different linear units. Notice that because our hidden state is currently 0, the weights returned are all 0. In future recursions, these weights will no longer be 0.  \n",
    "\n",
    "Currently our incoming `x` has all 8 examples for our context windows across the 2 batches. With LSTM's we can only feed in 1 example at a time and, since we want sequential weights, 1 token at a time, though we can do multiple batches at a time.  Luckily our examples are incremental (example 1 is token 1, example 2 is tokens 1 and 2) so we can iteratively calculate our LSTM weights per example and use that collectively as the weight for each example.  But to start we'll need to extract just the example for the first time  period `t=0` into `x_t` for each batch and calculate our weights. Since our weights are 1/2 for each value of the cell, you can see that when we multiply that by the example each weight remains consistent. If dropout is observed, the values in each batch can be different, but they'll be the same across the example. \n",
    "\n",
    "Finally, we'll set our memory cell `c_t` to equal the `c_next` we initialized. As a reminder, we've set it to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63fc6b3c-49e8-4802-aa72-1f601f72d8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4]),\n",
       " tensor([[1.1111, 1.1111, 1.1111, 1.1111],\n",
       "         [1.1111, 0.0000, 1.1111, 1.1111]], grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 0\n",
    "x_t = x[:, t, :]\n",
    "x_t.size(), x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1fa665f-42a4-47db-bd43-b6a8a760ab6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]),\n",
       " tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_t = h_next\n",
    "h_t.size(), h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f205cb6-1154-4068-b016-cdbcb8413912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]),\n",
       " tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_t = c_next\n",
    "c_t.size(), h_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2131795-baf8-4c4e-9b3b-e4bc473f2bed",
   "metadata": {},
   "source": [
    "**Linear weight calculation** Now we'll do our calcuation of all 4 weights for both the input and hidden state. Note that the memory cell does not receive a weight as it's impacted more by the input and forget gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d073b52-32ca-4c4c-88c5-4b6117e2a9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('xi',\n",
       " tensor([[2.2222, 2.2222, 2.2222],\n",
       "         [1.6667, 1.6667, 1.6667]], grad_fn=<SplitBackward0>),\n",
       " torch.Size([2, 3]),\n",
       " 'xf',\n",
       " tensor([[2.2222, 2.2222, 2.2222],\n",
       "         [1.6667, 1.6667, 1.6667]], grad_fn=<SplitBackward0>),\n",
       " torch.Size([2, 3]),\n",
       " 'xo',\n",
       " tensor([[2.2222, 2.2222, 2.2222],\n",
       "         [1.6667, 1.6667, 1.6667]], grad_fn=<SplitBackward0>),\n",
       " torch.Size([2, 3]),\n",
       " 'xc',\n",
       " tensor([[2.2222, 2.2222, 2.2222],\n",
       "         [1.6667, 1.6667, 1.6667]], grad_fn=<SplitBackward0>),\n",
       " torch.Size([2, 3]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xi, xf, xo, xc = x2g(x_t).split(hidden_size, dim=-1)\n",
    "'xi',xi,xi.size(),'xf',xf,xf.size(),'xo',xo,xo.size(),'xc',xc,xc.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5d33f2a-3471-434b-8441-8d41befd55c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hi',\n",
       " tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.]], grad_fn=<SplitBackward0>),\n",
       " torch.Size([2, 3]),\n",
       " 'hf',\n",
       " tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.]], grad_fn=<SplitBackward0>),\n",
       " torch.Size([2, 3]),\n",
       " 'ho',\n",
       " tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.]], grad_fn=<SplitBackward0>),\n",
       " torch.Size([2, 3]),\n",
       " 'hc',\n",
       " tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.]], grad_fn=<SplitBackward0>),\n",
       " torch.Size([2, 3]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi, hf, ho, hc = h2g(h_t).split(hidden_size, dim=-1)\n",
    "'hi',hi,hi.size(),'hf',hf,hf.size(),'ho',ho,ho.size(),'hc',hc,hc.size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90888bbb-46bb-4739-a692-d01affb809f9",
   "metadata": {},
   "source": [
    "##### LSTM - Input $I_{t}$, Forget $F_{t}$, and Output $O_{t}$ Gates\n",
    "Recall that the gates in an LSTM controls how much the input state and memory cell persist for the next memory cell and update state. We derive the gates using:\n",
    "$$\n",
    "\\begin{align}\n",
    "I_t = \\sigma\\!\\left(X_t W_{xi} + H_{t} W_{hi} + b_i\\right) \\\\\n",
    "F_t = \\sigma\\!\\left(X_t W_{xf} + H_{t} W_{hf} + b_f\\right) \\\\\n",
    "O_t = \\sigma\\!\\left(X_t W_{xo} + H_{t} W_{ho} + b_z\\right) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "These gate path will become the learnable weights that act as proportions since we use the sigmoid function to pull the values to between $\\left(0,1\\right)$ [source](https://docs.pytorch.org/docs/stable/generated/torch.sigmoid.html). This is achieved with the following function\n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1 + e^{(-x)}}$\n",
    "\n",
    "You'll notice that all our gates have identical values currently.  This is because we used the consistent initiation on the weights. If there's variance in row values at this point it's due to dropout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff52ffa4-3b1c-4b4e-9b4a-68edb185d3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]),\n",
       " tensor([[0.9022, 0.9022, 0.9022],\n",
       "         [0.8411, 0.8411, 0.8411]], grad_fn=<SigmoidBackward0>))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_gate =  torch.sigmoid(xi + hi)\n",
    "input_gate.size(), input_gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23590259-9364-4c7b-b008-5e2fbf37bda4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]),\n",
       " tensor([[0.9022, 0.9022, 0.9022],\n",
       "         [0.8411, 0.8411, 0.8411]], grad_fn=<SigmoidBackward0>))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forget_gate =  torch.sigmoid(xf + hf)\n",
    "forget_gate.size(), forget_gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "243aea3e-56b3-41d2-8475-df718b920fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]),\n",
       " tensor([[0.9022, 0.9022, 0.9022],\n",
       "         [0.8411, 0.8411, 0.8411]], grad_fn=<SigmoidBackward0>))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_gate =  torch.sigmoid(xo + ho)\n",
    "output_gate.size(), output_gate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585195e5-dd56-45d3-8c75-7fe5a14cc75c",
   "metadata": {},
   "source": [
    "##### LSTM - Input State $\\tilde{C}$\n",
    "\n",
    "Next we calculate the input state.  This input state represents the impact of the timepoint's specific context with the impact of the incoming hidden state context. It's calculated as follows:\n",
    "\n",
    "$\\tilde{C} = tanh\\!\\left(X_t W_{xc} + H_{t} W_{hc} + b_c\\right)$\n",
    "\n",
    "Note that while this looks similar to our gates, we are using a tanh normalization instead of the sigmoid.  Tanh pulls the values to between $\\left(-1,1\\right)$ [source](https://docs.pytorch.org/docs/stable/generated/torch.tanh.html). This is achieved with the following function:\n",
    "\n",
    "$\\tanh(x) = \\frac{e^x-e^{-x}}{e^x+e^{-x}}$\n",
    "\n",
    "You can see that since the network is currently untrained since the values are the same across the board.  In fact, if we looked at the values currently without the tanh normalization, it would be identical to the gate raw values. With training these values will diverge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "083c93ca-a3b4-4524-8c99-d263044a71ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]),\n",
       " tensor([[0.9768, 0.9768, 0.9768],\n",
       "         [0.9311, 0.9311, 0.9311]], grad_fn=<TanhBackward0>))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_state = torch.tanh(xc + hc)\n",
    "input_state.size(), input_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0f304c-5f4f-432b-9e17-65e3f6562a33",
   "metadata": {},
   "source": [
    "##### LSTM - Memory Cell $C_t$\n",
    "Now we have all the values that are needed to calculate our updated memory cell. Recall that our memory cell acts as the long term memory and is updated by taking the product from forget gate the input memory cell and adding it with the input state. The calculation is as follows:\n",
    "\n",
    "$C_{next} = F_t\\odot C_t + I_t\\odot \\tilde{C}$\n",
    "\n",
    "Recall that currently our memory cell is `0` since we are on the first iteration.  This means that despite having preset non-zero weights on our forget gate, we still won't have any value to remember, meaning that our memory cell becomes equal to our input state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b08ebe32-a407-420c-a95c-2cfef7cf50e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]),\n",
       " tensor([[0.8813, 0.8813, 0.8813],\n",
       "         [0.7832, 0.7832, 0.7832]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_in = input_gate*input_state\n",
    "i_in.size(), i_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ff419c1-6458-46ce-9fea-44ebfa836d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]),\n",
       " tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_c = forget_gate*c_t\n",
    "f_c.size(), f_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d85edd8d-b052-4e65-bef7-2aefb8dd6ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]),\n",
       " tensor([[0.8813, 0.8813, 0.8813],\n",
       "         [0.7832, 0.7832, 0.7832]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_next = f_c + i_in\t\n",
    "c_next.size(), c_next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a69d2a-70d6-4df9-8391-12f98692fe2b",
   "metadata": {},
   "source": [
    "##### LSTM - Hidden State $H_t$ / Recursion output\n",
    "We now have the updated memory cell that's a combination of the incoming memory cell with the incoming state, itself a combination of the incoming input context and hidden state. We've also already calculated the output gate which controlls how much of the memory cell impacts the next hidden state and recursion output. To generate the hidden state we do the following: \n",
    "\n",
    "$H_{next} = O_t\\odot tanh\\left(C_{next}\\right)$\n",
    "\n",
    "\n",
    "We'll first calcualte the tanh of the hidden state. Recall that tanh will squeeze the memory cell values to between $\\left(-1,1\\right)$ with the following function:\n",
    "\n",
    "$\\tanh(x) = \\frac{e^x-e^{-x}}{e^x+e^{-x}}$\n",
    "\n",
    "We'll then take the product of the output gate with that result to generate our recursion output. In this case, since this is not the final recursion this output becomes the next recursion's incoming hidden state. You'll notice again very consistent behavior since our weights are initialized consistently across all gates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "377016e0-fe1d-4ac6-ac26-f4c27708d7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]),\n",
       " tensor([[0.7071, 0.7071, 0.7071],\n",
       "         [0.6545, 0.6545, 0.6545]], grad_fn=<TanhBackward0>))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tan_c_n = torch.tanh(c_next)\n",
    "tan_c_n.size(), tan_c_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7998ac75-c036-4c28-b4e6-d113cf238fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]),\n",
       " tensor([[0.6379, 0.6379, 0.6379],\n",
       "         [0.5505, 0.5505, 0.5505]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_next = output_gate*tan_c_n\n",
    "h_next.size(), h_next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5088318f-4aa6-47cc-a3c5-3e633b517129",
   "metadata": {},
   "source": [
    "#### LSTM -  Managing examples in a batch\n",
    "Now that we have the weights from our recurrent unit for our first time period `t=0`, since we have more context, we actually have to recur through the rest of the context.  Before we do the next cycle recall that for training we want to take advantage of the fact that each of our batches has an example for each value up to our context length.  We want to train across inputs of various lengths and so, we'll store our current weights that signify an input of just a single token into our context.  As you'll see in our recursion, we'll store the weight at the end of each pass and build up an array of tensors for each example in the batch. to take to our output layer. \n",
    "\n",
    "To start we'll create an empty tensor of `(B x T x hidden_size)`. We'll then update the `[:,t,:]` entry with `h_next` linking back to each recursion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "938d60c5-c2b0-44d5-809b-7d7e790c97d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 3]),\n",
       " tensor([[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs = x.new_empty(B_batch, T_context, hidden_size)\n",
    "hs.size(), hs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf5b508-7022-4e18-ac93-6039eabf65f9",
   "metadata": {},
   "source": [
    "now we'll enter in our first entry at `t=0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5712a754-aaaf-411f-8465-8c9c69569ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6379, 0.6379, 0.6379],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.5505, 0.5505, 0.5505],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000]]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs[:, 0, :] = h_next   \n",
    "hs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f8ceb5-3070-4fdf-851d-51cabee04d71",
   "metadata": {},
   "source": [
    "#### LSTM - Second pass "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5fb533-c74a-4491-8c5c-d79cb32b2a41",
   "metadata": {},
   "source": [
    "Now that we've done the first pass through, let's see how the second pass changes things. The main difference is that in the second pass we'll use the previous pass' outputs,`c_next` & `h_next`, to become the new input memory cell `c_t` and hidden state `h_t`.  This will quickly begin to show the impact now on the gates and calculations as the values are no longer 0 while our X weights are still consistent. We'll start by updating initializing our `h_t` and `w_t` and extracting the next set of inputs for `t=1`.\n",
    "\n",
    "One thing to notice is that despite having a different time period for input `h_t`, the values remain consistent across examples since our initiation weights are uniform.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f79cc3b1-d01d-4e5d-a82f-0b9b0c68ea82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4]),\n",
       " tensor([[1.1111, 1.1111, 1.1111, 1.1111],\n",
       "         [1.1111, 1.1111, 1.1111, 1.1111]], grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 1\n",
    "x_t = x[:, t, :]\n",
    "x_t.size(), x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a062d61c-7e7c-4999-a672-23bc3382bfeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]),\n",
       " tensor([[0.6379, 0.6379, 0.6379],\n",
       "         [0.5505, 0.5505, 0.5505]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_t = h_next\n",
    "h_t.size(), h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88aef728-f946-476f-b037-147b2cdf49d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]),\n",
       " tensor([[0.6379, 0.6379, 0.6379],\n",
       "         [0.5505, 0.5505, 0.5505]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_t = c_next\n",
    "c_t.size(), h_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafec9b1-1abf-4546-9495-5c1143247972",
   "metadata": {},
   "source": [
    "**LSTM second pass - recalculating weights**\n",
    "\n",
    "Notice that with the updated non-zero hidden state we now see contributions from the dot product of the hidden state weights and hidden state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f62f9687-f337-4f4a-a994-46ab785195fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('xi',\n",
       " tensor([[2.2222, 2.2222, 2.2222],\n",
       "         [2.2222, 2.2222, 2.2222]], grad_fn=<SplitBackward0>),\n",
       " torch.Size([2, 3]),\n",
       " 'xf',\n",
       " tensor([[2.2222, 2.2222, 2.2222],\n",
       "         [2.2222, 2.2222, 2.2222]], grad_fn=<SplitBackward0>),\n",
       " torch.Size([2, 3]),\n",
       " 'xo',\n",
       " tensor([[2.2222, 2.2222, 2.2222],\n",
       "         [2.2222, 2.2222, 2.2222]], grad_fn=<SplitBackward0>),\n",
       " torch.Size([2, 3]),\n",
       " 'xc',\n",
       " tensor([[2.2222, 2.2222, 2.2222],\n",
       "         [2.2222, 2.2222, 2.2222]], grad_fn=<SplitBackward0>),\n",
       " torch.Size([2, 3]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xi, xf, xo, xc = x2g(x_t).split(hidden_size, dim=-1)\n",
    "'xi',xi,xi.size(),'xf',xf,xf.size(),'xo',xo,xo.size(),'xc',xc,xc.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c8278092-677f-4c5d-b237-66dd779bae1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hi',\n",
       " tensor([[0.4784, 0.4784, 0.4784],\n",
       "         [0.4129, 0.4129, 0.4129]], grad_fn=<SplitBackward0>),\n",
       " torch.Size([2, 3]),\n",
       " 'hf',\n",
       " tensor([[0.4784, 0.4784, 0.4784],\n",
       "         [0.4129, 0.4129, 0.4129]], grad_fn=<SplitBackward0>),\n",
       " torch.Size([2, 3]),\n",
       " 'ho',\n",
       " tensor([[0.4784, 0.4784, 0.4784],\n",
       "         [0.4129, 0.4129, 0.4129]], grad_fn=<SplitBackward0>),\n",
       " torch.Size([2, 3]),\n",
       " 'hc',\n",
       " tensor([[0.4784, 0.4784, 0.4784],\n",
       "         [0.4129, 0.4129, 0.4129]], grad_fn=<SplitBackward0>),\n",
       " torch.Size([2, 3]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi, hf, ho, hc = h2g(h_t).split(hidden_size, dim=-1)\n",
    "'hi',hi,hi.size(),'hf',hf,hf.size(),'ho',ho,ho.size(),'hc',hc,hc.size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a58ff5-f288-42f4-ac6f-8858e5ec179f",
   "metadata": {},
   "source": [
    "**LSTM second pass - Updated Input $I_{t}$, Forget $F_{t}$, and Output $O_{t}$ Gates**\n",
    "\n",
    "Once again you'll notice the gates are all the same for an example (though may differ across the two batches). This is since we initilized with a consistent weight and we have not yet made changes to improve our prediction. Even with RNNs the changes happen during back-propogation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ea9b554-7452-4723-b236-388db18a7ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]),\n",
       " tensor([[0.9371, 0.9371, 0.9371],\n",
       "         [0.9331, 0.9331, 0.9331]], grad_fn=<SigmoidBackward0>))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_gate =  torch.sigmoid(xi + hi)\n",
    "input_gate.size(), input_gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "84ce203f-737b-40c9-8ef3-836cdcbae817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]),\n",
       " tensor([[0.9371, 0.9371, 0.9371],\n",
       "         [0.9331, 0.9331, 0.9331]], grad_fn=<SigmoidBackward0>))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forget_gate =  torch.sigmoid(xf + hf)\n",
    "forget_gate.size(), forget_gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d2d368ee-5e6b-4b15-bbc4-3b6c17050729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]),\n",
       " tensor([[0.9371, 0.9371, 0.9371],\n",
       "         [0.9331, 0.9331, 0.9331]], grad_fn=<SigmoidBackward0>))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_gate =  torch.sigmoid(xo + ho)\n",
    "output_gate.size(), output_gate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fe9149-7cec-4f17-997a-cadf6f41c0a9",
   "metadata": {},
   "source": [
    "**LSTM second pass - Input State $\\tilde{C}$**\n",
    "\n",
    "We'll continue the trend of consistent values for all the same reasons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4db01714-f5ef-4375-afd3-e8de57efa6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]),\n",
       " tensor([[0.9910, 0.9910, 0.9910],\n",
       "         [0.9898, 0.9898, 0.9898]], grad_fn=<TanhBackward0>))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_state = torch.tanh(xc + hc)\n",
    "input_state.size(), input_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45d65cb-5741-4ead-a7a8-c5eff8049d9e",
   "metadata": {},
   "source": [
    "**LSTM second pass - Memory Cell $C_t$**\n",
    "\n",
    "This time through, we can see that the forget gate comes into play. Since our incoming memory cell is non-zero, we can now see that the updated memory cell is in part the input state and in part the previous memory cell, as determined by the forget gate.  With training this mix will be learned down to the entry level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f37df0f9-e90f-4416-a52a-30b24f2d1416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]),\n",
       " tensor([[0.9287, 0.9287, 0.9287],\n",
       "         [0.9235, 0.9235, 0.9235]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_in = input_gate*input_state\n",
    "i_in.size(), i_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26ade896-6420-4889-8112-402c81f27159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]),\n",
       " tensor([[0.8258, 0.8258, 0.8258],\n",
       "         [0.7308, 0.7308, 0.7308]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_c = forget_gate*c_t\n",
    "f_c.size(), f_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "013cfaea-4ab0-4350-97f9-7c4df624df02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]),\n",
       " tensor([[1.7545, 1.7545, 1.7545],\n",
       "         [1.6543, 1.6543, 1.6543]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_next = f_c + i_in\t\n",
    "c_next.size(), c_next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acb7435-32af-4f5b-86ff-331dd30dd44a",
   "metadata": {},
   "source": [
    "**LSTM second pass - Hidden State $H_t$ / Recursion output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dad4f563-c915-4a3a-89a0-b58dff9a0569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]),\n",
       " tensor([[0.9419, 0.9419, 0.9419],\n",
       "         [0.9294, 0.9294, 0.9294]], grad_fn=<TanhBackward0>))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tan_c_n = torch.tanh(c_next)\n",
    "tan_c_n.size(), tan_c_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6cd5bd3c-cc19-4403-9c19-e9fd36c261e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]),\n",
       " tensor([[0.8826, 0.8826, 0.8826],\n",
       "         [0.8673, 0.8673, 0.8673]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_next = output_gate*tan_c_n\n",
    "h_next.size(), h_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9e1ca4af-982f-4f07-8bd7-18acbbc168cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6379, 0.6379, 0.6379],\n",
       "         [0.8826, 0.8826, 0.8826],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.5505, 0.5505, 0.5505],\n",
       "         [0.8673, 0.8673, 0.8673],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000]]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs[:, 1, :] = h_next   \n",
    "hs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f771609b-dfb8-4a0c-a9ff-ff3aec2e619e",
   "metadata": {},
   "source": [
    "#### LSTM - Recursion \n",
    "Now that we've seen how 2 passes through the LSTM look like, let's write a loop to do the remaining passes.  The loop in this case will start at `t=2` and run the rest of the context length.  We'll capture the predicted weights into `hs` so that we'll have each example's weights and the link back for the backward pass during back propagation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4c0be513-11d4-4856-94cd-665d63b3d8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6379, 0.6379, 0.6379],\n",
       "         [0.8826, 0.8826, 0.8826],\n",
       "         [0.8988, 0.8988, 0.8988],\n",
       "         [0.9451, 0.9451, 0.9451],\n",
       "         [0.9488, 0.9488, 0.9488],\n",
       "         [0.9494, 0.9494, 0.9494],\n",
       "         [0.8609, 0.8609, 0.8609],\n",
       "         [0.9462, 0.9462, 0.9462]],\n",
       "\n",
       "        [[0.5505, 0.5505, 0.5505],\n",
       "         [0.8673, 0.8673, 0.8673],\n",
       "         [0.9340, 0.9340, 0.9340],\n",
       "         [0.9465, 0.9465, 0.9465],\n",
       "         [0.9489, 0.9489, 0.9489],\n",
       "         [0.9150, 0.9150, 0.9150],\n",
       "         [0.9482, 0.9482, 0.9482],\n",
       "         [0.9495, 0.9495, 0.9495]]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for t in range(2,T_context):\n",
    "    \n",
    "    # set input and previous\n",
    "    h_t = h_next\n",
    "    c_t = c_next\n",
    "    x_t = x[:, t, :]\n",
    "\n",
    "    # calculate weights\n",
    "    xi, xf, xo, xc = x2g(x_t).split(hidden_size, dim=-1)\n",
    "    hi, hf, ho, hc = h2g(h_t).split(hidden_size, dim=-1)\n",
    "    \n",
    "    #input, forget, and output gate \n",
    "    input_gate =  torch.sigmoid(xi + hi)\n",
    "    forget_gate =  torch.sigmoid(xf + hf)\n",
    "    output_gate =  torch.sigmoid(xo + ho)\n",
    "    \n",
    "    # input state\n",
    "    input_state = torch.tanh(xc + hc)\n",
    "    \n",
    "    # memory cell\n",
    "    c_next = (forget_gate*c_t) + (input_gate*input_state)\t\n",
    "    \n",
    "    # hidden state / recursion output\n",
    "    h_next = output_gate*torch.tanh(c_next)\n",
    "\n",
    "    # save h_t\n",
    "    hs[:, t, :] = h_next\n",
    "\n",
    "hs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4c77db-2d10-454f-b512-92ac5b024e61",
   "metadata": {},
   "source": [
    "### Final Dropout \n",
    "\n",
    "After completing our recurrent unit, we'll perform another round of dropout. Dropout after the LSTMs regularizes the layer output / readout without corrupting the recurrent dynamics; injecting fresh noise inside the recurrence each step tends to destabilize long-range memory.  Often this step has “locked” dropout, meaning a single mask broadcast across time, which gives the same effect as per-step dropout with a fixed mask but avoids i.i.d. noise at every timestep.  We, however, will be doing default dropout that is not locked to simplify our code. \n",
    "\n",
    "As a reminder, we perform Bernoulli based dropout which both removes entries and adds p to the surviving entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "14428ec8-69e9-4605-a6fa-eb1012337be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 3]),\n",
       " tensor([[[0.7088, 0.7088, 0.0000],\n",
       "          [0.9807, 0.9807, 0.9807],\n",
       "          [0.9987, 0.9987, 0.9987],\n",
       "          [1.0501, 1.0501, 1.0501],\n",
       "          [1.0542, 1.0542, 1.0542],\n",
       "          [0.0000, 1.0549, 1.0549],\n",
       "          [0.9565, 0.9565, 0.9565],\n",
       "          [1.0513, 1.0513, 1.0513]],\n",
       " \n",
       "         [[0.6117, 0.6117, 0.6117],\n",
       "          [0.9636, 0.9636, 0.9636],\n",
       "          [0.0000, 1.0377, 0.0000],\n",
       "          [1.0517, 1.0517, 1.0517],\n",
       "          [1.0543, 1.0543, 1.0543],\n",
       "          [1.0167, 1.0167, 1.0167],\n",
       "          [1.0536, 1.0536, 1.0536],\n",
       "          [1.0550, 1.0550, 1.0550]]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = dropout(hs)\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1de516-4387-4cd9-b3b4-93c9938ff46e",
   "metadata": {},
   "source": [
    "### Output Layers AKA Model Head."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22593c65-be72-436f-9e1c-8620c22c04ab",
   "metadata": {},
   "source": [
    "The LSTM unit is our recurrent unit for this example. In practice, this unit can be scaled by increasing the hidden_size dimension or increasing the complexity and flavors of the recurring unit. Once those layers are complete and we've done dropout normalization, during the forward pass we then start the output process that results in `logits` which is a representation of the probability of each token being the next token given the input.  \n",
    "\n",
    "This layer is also known as the model **head**. This layer is called this because it is a small, task-specific module attached to a model’s shared backbone that maps hidden features to the final outputs.  In our example case, this is a linear layer mapping the backbone to vocab logits. The benefit of this structure is that you can use the shared hidden features and train different heads for different tasks without starting from scratch. An example would be a classifier head, or policy head in RL.\n",
    "\n",
    "<img src=\"explainer_screenshots/lstm/output_layer.png\" width=\"200\">\n",
    "\n",
    "We'll use a similar head that we've used in other examples that focuses on predicting the next token. For our head we want to map to a predicted token which we'll look at as `logits`. In the process to generate `logits` we take the dropout output `x` of the GRU, then project, using a linear layer, to the vocabulary resulting in a `B, T, vocab_size` matrix known as `logits`.  \n",
    "\n",
    "In training, the `logits` are then compared with `y` to see how close the  model is to predicting the correct next token. For inference, the `logits` are then used to drive sampling which is how the next token is then derived. \n",
    "\n",
    "Contrary to our transformer example, we won't be doing weight tying and instead use an initial linear layer.  We'll start initially by setting all of the values equal to 1 meaning they all have equal probability.  This will highlight just how impactful back-propagation and training is.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "27e76d90-e130-4e4f-a221-18ee7461876e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([36, 3]),\n",
       " Parameter containing:\n",
       " tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]], requires_grad=True))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_head = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "torch.nn.init.ones_(lm_head.weight)\n",
    "lm_head.weight.size(), lm_head.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8202a0-64d2-4b94-8ab2-5a69e0602c40",
   "metadata": {},
   "source": [
    "#### Output layer - LM Head aka logits\n",
    "We now project `x` onto the vocabulary resulting in a `B X T X vocab_size` final array `logits`.  This output correlates with the probability of each output token given the input context.  The best way to read  this is:\n",
    "\n",
    "(dimension 0) we have 2 batches B, \n",
    "(dimension 1) each batch has an example for each value between 1 and context length T \n",
    "(dimension 2) for each example we see the probability of each token in our vocabulary\n",
    "\n",
    "Since our output head had consistent weights across all the dimensions, we fully expect that our logits will have equal values across all positions.  In practice this means that our model will have the same probability of a token output as the 'next token' regardless of the preceding text, meaning it's shit. Luckily back-propagation has a way of updating this so that with enough data and time the probabilities change. \n",
    "\n",
    "Note that if we wanted to run inference, after calculating the logits we'd run a softmax to sample a token out of the probability distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "19b23a27-073d-499e-a5d3-5d1cc5d42f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 36]),\n",
       " tensor([[[1.4176, 1.4176, 1.4176, 1.4176, 1.4176, 1.4176, 1.4176, 1.4176,\n",
       "           1.4176, 1.4176, 1.4176, 1.4176, 1.4176, 1.4176, 1.4176, 1.4176,\n",
       "           1.4176, 1.4176, 1.4176, 1.4176, 1.4176, 1.4176, 1.4176, 1.4176,\n",
       "           1.4176, 1.4176, 1.4176, 1.4176, 1.4176, 1.4176, 1.4176, 1.4176,\n",
       "           1.4176, 1.4176, 1.4176, 1.4176],\n",
       "          [2.9420, 2.9420, 2.9420, 2.9420, 2.9420, 2.9420, 2.9420, 2.9420,\n",
       "           2.9420, 2.9420, 2.9420, 2.9420, 2.9420, 2.9420, 2.9420, 2.9420,\n",
       "           2.9420, 2.9420, 2.9420, 2.9420, 2.9420, 2.9420, 2.9420, 2.9420,\n",
       "           2.9420, 2.9420, 2.9420, 2.9420, 2.9420, 2.9420, 2.9420, 2.9420,\n",
       "           2.9420, 2.9420, 2.9420, 2.9420],\n",
       "          [2.9962, 2.9962, 2.9962, 2.9962, 2.9962, 2.9962, 2.9962, 2.9962,\n",
       "           2.9962, 2.9962, 2.9962, 2.9962, 2.9962, 2.9962, 2.9962, 2.9962,\n",
       "           2.9962, 2.9962, 2.9962, 2.9962, 2.9962, 2.9962, 2.9962, 2.9962,\n",
       "           2.9962, 2.9962, 2.9962, 2.9962, 2.9962, 2.9962, 2.9962, 2.9962,\n",
       "           2.9962, 2.9962, 2.9962, 2.9962],\n",
       "          [3.1504, 3.1504, 3.1504, 3.1504, 3.1504, 3.1504, 3.1504, 3.1504,\n",
       "           3.1504, 3.1504, 3.1504, 3.1504, 3.1504, 3.1504, 3.1504, 3.1504,\n",
       "           3.1504, 3.1504, 3.1504, 3.1504, 3.1504, 3.1504, 3.1504, 3.1504,\n",
       "           3.1504, 3.1504, 3.1504, 3.1504, 3.1504, 3.1504, 3.1504, 3.1504,\n",
       "           3.1504, 3.1504, 3.1504, 3.1504],\n",
       "          [3.1627, 3.1627, 3.1627, 3.1627, 3.1627, 3.1627, 3.1627, 3.1627,\n",
       "           3.1627, 3.1627, 3.1627, 3.1627, 3.1627, 3.1627, 3.1627, 3.1627,\n",
       "           3.1627, 3.1627, 3.1627, 3.1627, 3.1627, 3.1627, 3.1627, 3.1627,\n",
       "           3.1627, 3.1627, 3.1627, 3.1627, 3.1627, 3.1627, 3.1627, 3.1627,\n",
       "           3.1627, 3.1627, 3.1627, 3.1627],\n",
       "          [2.1097, 2.1097, 2.1097, 2.1097, 2.1097, 2.1097, 2.1097, 2.1097,\n",
       "           2.1097, 2.1097, 2.1097, 2.1097, 2.1097, 2.1097, 2.1097, 2.1097,\n",
       "           2.1097, 2.1097, 2.1097, 2.1097, 2.1097, 2.1097, 2.1097, 2.1097,\n",
       "           2.1097, 2.1097, 2.1097, 2.1097, 2.1097, 2.1097, 2.1097, 2.1097,\n",
       "           2.1097, 2.1097, 2.1097, 2.1097],\n",
       "          [2.8695, 2.8695, 2.8695, 2.8695, 2.8695, 2.8695, 2.8695, 2.8695,\n",
       "           2.8695, 2.8695, 2.8695, 2.8695, 2.8695, 2.8695, 2.8695, 2.8695,\n",
       "           2.8695, 2.8695, 2.8695, 2.8695, 2.8695, 2.8695, 2.8695, 2.8695,\n",
       "           2.8695, 2.8695, 2.8695, 2.8695, 2.8695, 2.8695, 2.8695, 2.8695,\n",
       "           2.8695, 2.8695, 2.8695, 2.8695],\n",
       "          [3.1540, 3.1540, 3.1540, 3.1540, 3.1540, 3.1540, 3.1540, 3.1540,\n",
       "           3.1540, 3.1540, 3.1540, 3.1540, 3.1540, 3.1540, 3.1540, 3.1540,\n",
       "           3.1540, 3.1540, 3.1540, 3.1540, 3.1540, 3.1540, 3.1540, 3.1540,\n",
       "           3.1540, 3.1540, 3.1540, 3.1540, 3.1540, 3.1540, 3.1540, 3.1540,\n",
       "           3.1540, 3.1540, 3.1540, 3.1540]],\n",
       " \n",
       "         [[1.8352, 1.8352, 1.8352, 1.8352, 1.8352, 1.8352, 1.8352, 1.8352,\n",
       "           1.8352, 1.8352, 1.8352, 1.8352, 1.8352, 1.8352, 1.8352, 1.8352,\n",
       "           1.8352, 1.8352, 1.8352, 1.8352, 1.8352, 1.8352, 1.8352, 1.8352,\n",
       "           1.8352, 1.8352, 1.8352, 1.8352, 1.8352, 1.8352, 1.8352, 1.8352,\n",
       "           1.8352, 1.8352, 1.8352, 1.8352],\n",
       "          [2.8909, 2.8909, 2.8909, 2.8909, 2.8909, 2.8909, 2.8909, 2.8909,\n",
       "           2.8909, 2.8909, 2.8909, 2.8909, 2.8909, 2.8909, 2.8909, 2.8909,\n",
       "           2.8909, 2.8909, 2.8909, 2.8909, 2.8909, 2.8909, 2.8909, 2.8909,\n",
       "           2.8909, 2.8909, 2.8909, 2.8909, 2.8909, 2.8909, 2.8909, 2.8909,\n",
       "           2.8909, 2.8909, 2.8909, 2.8909],\n",
       "          [1.0377, 1.0377, 1.0377, 1.0377, 1.0377, 1.0377, 1.0377, 1.0377,\n",
       "           1.0377, 1.0377, 1.0377, 1.0377, 1.0377, 1.0377, 1.0377, 1.0377,\n",
       "           1.0377, 1.0377, 1.0377, 1.0377, 1.0377, 1.0377, 1.0377, 1.0377,\n",
       "           1.0377, 1.0377, 1.0377, 1.0377, 1.0377, 1.0377, 1.0377, 1.0377,\n",
       "           1.0377, 1.0377, 1.0377, 1.0377],\n",
       "          [3.1550, 3.1550, 3.1550, 3.1550, 3.1550, 3.1550, 3.1550, 3.1550,\n",
       "           3.1550, 3.1550, 3.1550, 3.1550, 3.1550, 3.1550, 3.1550, 3.1550,\n",
       "           3.1550, 3.1550, 3.1550, 3.1550, 3.1550, 3.1550, 3.1550, 3.1550,\n",
       "           3.1550, 3.1550, 3.1550, 3.1550, 3.1550, 3.1550, 3.1550, 3.1550,\n",
       "           3.1550, 3.1550, 3.1550, 3.1550],\n",
       "          [3.1630, 3.1630, 3.1630, 3.1630, 3.1630, 3.1630, 3.1630, 3.1630,\n",
       "           3.1630, 3.1630, 3.1630, 3.1630, 3.1630, 3.1630, 3.1630, 3.1630,\n",
       "           3.1630, 3.1630, 3.1630, 3.1630, 3.1630, 3.1630, 3.1630, 3.1630,\n",
       "           3.1630, 3.1630, 3.1630, 3.1630, 3.1630, 3.1630, 3.1630, 3.1630,\n",
       "           3.1630, 3.1630, 3.1630, 3.1630],\n",
       "          [3.0500, 3.0500, 3.0500, 3.0500, 3.0500, 3.0500, 3.0500, 3.0500,\n",
       "           3.0500, 3.0500, 3.0500, 3.0500, 3.0500, 3.0500, 3.0500, 3.0500,\n",
       "           3.0500, 3.0500, 3.0500, 3.0500, 3.0500, 3.0500, 3.0500, 3.0500,\n",
       "           3.0500, 3.0500, 3.0500, 3.0500, 3.0500, 3.0500, 3.0500, 3.0500,\n",
       "           3.0500, 3.0500, 3.0500, 3.0500],\n",
       "          [3.1607, 3.1607, 3.1607, 3.1607, 3.1607, 3.1607, 3.1607, 3.1607,\n",
       "           3.1607, 3.1607, 3.1607, 3.1607, 3.1607, 3.1607, 3.1607, 3.1607,\n",
       "           3.1607, 3.1607, 3.1607, 3.1607, 3.1607, 3.1607, 3.1607, 3.1607,\n",
       "           3.1607, 3.1607, 3.1607, 3.1607, 3.1607, 3.1607, 3.1607, 3.1607,\n",
       "           3.1607, 3.1607, 3.1607, 3.1607],\n",
       "          [3.1649, 3.1649, 3.1649, 3.1649, 3.1649, 3.1649, 3.1649, 3.1649,\n",
       "           3.1649, 3.1649, 3.1649, 3.1649, 3.1649, 3.1649, 3.1649, 3.1649,\n",
       "           3.1649, 3.1649, 3.1649, 3.1649, 3.1649, 3.1649, 3.1649, 3.1649,\n",
       "           3.1649, 3.1649, 3.1649, 3.1649, 3.1649, 3.1649, 3.1649, 3.1649,\n",
       "           3.1649, 3.1649, 3.1649, 3.1649]]], grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = lm_head(x)\n",
    "\n",
    "logits.shape, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1e4018-31cc-426d-b96e-36d99c133316",
   "metadata": {},
   "source": [
    "## Loss calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1c01b9-d09b-4708-bd9e-3aceeeb9cb18",
   "metadata": {},
   "source": [
    "Now we have to see how good our ~shit~ prediction is.  Since we haven't done training, and we saw that regardless of example we had the same exact logit values, we can expect it's bad. That said, we need to know how bad. For this example we'll use cross entropy, also known as the negative log likelihood of the softmax.  Our loss calculates\n",
    "\n",
    "$$\n",
    "\\ell_i=-\\log\\big(\\mathrm{softmax}(z_i)\\_{y_i}\\big)\n",
    "= -z_{i,y_i}+\\log\\!\\sum_{c=1}^C e^{z_{i,c}},\n",
    "$$\n",
    "\n",
    "\n",
    "To calculate loss we'll pass in the calculated `logits` and our next tokens stored in `y`. The cross entropy function does not respect batches so we'll flatten the `B` dimension for both `logits` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8423d81b-3ceb-44a0-b8dd-7b7ffe5cb977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3dce85cc-5c7e-47bf-8f0e-e23b7f6aa337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16]),\n",
       " tensor([15, 32,  9,  5, 20, 30, 15, 11,  9,  6, 20,  5,  0, 13, 21,  0]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_flat = y.view(-1)\n",
    "y_flat.shape, y_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "03a2daf1-b8cb-4660-9b46-c60d6c95ce58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 36]),\n",
       " tensor([[1.4176, 1.4176, 1.4176, 1.4176, 1.4176, 1.4176, 1.4176, 1.4176, 1.4176,\n",
       "          1.4176, 1.4176, 1.4176, 1.4176, 1.4176, 1.4176, 1.4176, 1.4176, 1.4176,\n",
       "          1.4176, 1.4176, 1.4176, 1.4176, 1.4176, 1.4176, 1.4176, 1.4176, 1.4176,\n",
       "          1.4176, 1.4176, 1.4176, 1.4176, 1.4176, 1.4176, 1.4176, 1.4176, 1.4176],\n",
       "         [2.9420, 2.9420, 2.9420, 2.9420, 2.9420, 2.9420, 2.9420, 2.9420, 2.9420,\n",
       "          2.9420, 2.9420, 2.9420, 2.9420, 2.9420, 2.9420, 2.9420, 2.9420, 2.9420,\n",
       "          2.9420, 2.9420, 2.9420, 2.9420, 2.9420, 2.9420, 2.9420, 2.9420, 2.9420,\n",
       "          2.9420, 2.9420, 2.9420, 2.9420, 2.9420, 2.9420, 2.9420, 2.9420, 2.9420],\n",
       "         [2.9962, 2.9962, 2.9962, 2.9962, 2.9962, 2.9962, 2.9962, 2.9962, 2.9962,\n",
       "          2.9962, 2.9962, 2.9962, 2.9962, 2.9962, 2.9962, 2.9962, 2.9962, 2.9962,\n",
       "          2.9962, 2.9962, 2.9962, 2.9962, 2.9962, 2.9962, 2.9962, 2.9962, 2.9962,\n",
       "          2.9962, 2.9962, 2.9962, 2.9962, 2.9962, 2.9962, 2.9962, 2.9962, 2.9962],\n",
       "         [3.1504, 3.1504, 3.1504, 3.1504, 3.1504, 3.1504, 3.1504, 3.1504, 3.1504,\n",
       "          3.1504, 3.1504, 3.1504, 3.1504, 3.1504, 3.1504, 3.1504, 3.1504, 3.1504,\n",
       "          3.1504, 3.1504, 3.1504, 3.1504, 3.1504, 3.1504, 3.1504, 3.1504, 3.1504,\n",
       "          3.1504, 3.1504, 3.1504, 3.1504, 3.1504, 3.1504, 3.1504, 3.1504, 3.1504],\n",
       "         [3.1627, 3.1627, 3.1627, 3.1627, 3.1627, 3.1627, 3.1627, 3.1627, 3.1627,\n",
       "          3.1627, 3.1627, 3.1627, 3.1627, 3.1627, 3.1627, 3.1627, 3.1627, 3.1627,\n",
       "          3.1627, 3.1627, 3.1627, 3.1627, 3.1627, 3.1627, 3.1627, 3.1627, 3.1627,\n",
       "          3.1627, 3.1627, 3.1627, 3.1627, 3.1627, 3.1627, 3.1627, 3.1627, 3.1627],\n",
       "         [2.1097, 2.1097, 2.1097, 2.1097, 2.1097, 2.1097, 2.1097, 2.1097, 2.1097,\n",
       "          2.1097, 2.1097, 2.1097, 2.1097, 2.1097, 2.1097, 2.1097, 2.1097, 2.1097,\n",
       "          2.1097, 2.1097, 2.1097, 2.1097, 2.1097, 2.1097, 2.1097, 2.1097, 2.1097,\n",
       "          2.1097, 2.1097, 2.1097, 2.1097, 2.1097, 2.1097, 2.1097, 2.1097, 2.1097],\n",
       "         [2.8695, 2.8695, 2.8695, 2.8695, 2.8695, 2.8695, 2.8695, 2.8695, 2.8695,\n",
       "          2.8695, 2.8695, 2.8695, 2.8695, 2.8695, 2.8695, 2.8695, 2.8695, 2.8695,\n",
       "          2.8695, 2.8695, 2.8695, 2.8695, 2.8695, 2.8695, 2.8695, 2.8695, 2.8695,\n",
       "          2.8695, 2.8695, 2.8695, 2.8695, 2.8695, 2.8695, 2.8695, 2.8695, 2.8695],\n",
       "         [3.1540, 3.1540, 3.1540, 3.1540, 3.1540, 3.1540, 3.1540, 3.1540, 3.1540,\n",
       "          3.1540, 3.1540, 3.1540, 3.1540, 3.1540, 3.1540, 3.1540, 3.1540, 3.1540,\n",
       "          3.1540, 3.1540, 3.1540, 3.1540, 3.1540, 3.1540, 3.1540, 3.1540, 3.1540,\n",
       "          3.1540, 3.1540, 3.1540, 3.1540, 3.1540, 3.1540, 3.1540, 3.1540, 3.1540],\n",
       "         [1.8352, 1.8352, 1.8352, 1.8352, 1.8352, 1.8352, 1.8352, 1.8352, 1.8352,\n",
       "          1.8352, 1.8352, 1.8352, 1.8352, 1.8352, 1.8352, 1.8352, 1.8352, 1.8352,\n",
       "          1.8352, 1.8352, 1.8352, 1.8352, 1.8352, 1.8352, 1.8352, 1.8352, 1.8352,\n",
       "          1.8352, 1.8352, 1.8352, 1.8352, 1.8352, 1.8352, 1.8352, 1.8352, 1.8352],\n",
       "         [2.8909, 2.8909, 2.8909, 2.8909, 2.8909, 2.8909, 2.8909, 2.8909, 2.8909,\n",
       "          2.8909, 2.8909, 2.8909, 2.8909, 2.8909, 2.8909, 2.8909, 2.8909, 2.8909,\n",
       "          2.8909, 2.8909, 2.8909, 2.8909, 2.8909, 2.8909, 2.8909, 2.8909, 2.8909,\n",
       "          2.8909, 2.8909, 2.8909, 2.8909, 2.8909, 2.8909, 2.8909, 2.8909, 2.8909],\n",
       "         [1.0377, 1.0377, 1.0377, 1.0377, 1.0377, 1.0377, 1.0377, 1.0377, 1.0377,\n",
       "          1.0377, 1.0377, 1.0377, 1.0377, 1.0377, 1.0377, 1.0377, 1.0377, 1.0377,\n",
       "          1.0377, 1.0377, 1.0377, 1.0377, 1.0377, 1.0377, 1.0377, 1.0377, 1.0377,\n",
       "          1.0377, 1.0377, 1.0377, 1.0377, 1.0377, 1.0377, 1.0377, 1.0377, 1.0377],\n",
       "         [3.1550, 3.1550, 3.1550, 3.1550, 3.1550, 3.1550, 3.1550, 3.1550, 3.1550,\n",
       "          3.1550, 3.1550, 3.1550, 3.1550, 3.1550, 3.1550, 3.1550, 3.1550, 3.1550,\n",
       "          3.1550, 3.1550, 3.1550, 3.1550, 3.1550, 3.1550, 3.1550, 3.1550, 3.1550,\n",
       "          3.1550, 3.1550, 3.1550, 3.1550, 3.1550, 3.1550, 3.1550, 3.1550, 3.1550],\n",
       "         [3.1630, 3.1630, 3.1630, 3.1630, 3.1630, 3.1630, 3.1630, 3.1630, 3.1630,\n",
       "          3.1630, 3.1630, 3.1630, 3.1630, 3.1630, 3.1630, 3.1630, 3.1630, 3.1630,\n",
       "          3.1630, 3.1630, 3.1630, 3.1630, 3.1630, 3.1630, 3.1630, 3.1630, 3.1630,\n",
       "          3.1630, 3.1630, 3.1630, 3.1630, 3.1630, 3.1630, 3.1630, 3.1630, 3.1630],\n",
       "         [3.0500, 3.0500, 3.0500, 3.0500, 3.0500, 3.0500, 3.0500, 3.0500, 3.0500,\n",
       "          3.0500, 3.0500, 3.0500, 3.0500, 3.0500, 3.0500, 3.0500, 3.0500, 3.0500,\n",
       "          3.0500, 3.0500, 3.0500, 3.0500, 3.0500, 3.0500, 3.0500, 3.0500, 3.0500,\n",
       "          3.0500, 3.0500, 3.0500, 3.0500, 3.0500, 3.0500, 3.0500, 3.0500, 3.0500],\n",
       "         [3.1607, 3.1607, 3.1607, 3.1607, 3.1607, 3.1607, 3.1607, 3.1607, 3.1607,\n",
       "          3.1607, 3.1607, 3.1607, 3.1607, 3.1607, 3.1607, 3.1607, 3.1607, 3.1607,\n",
       "          3.1607, 3.1607, 3.1607, 3.1607, 3.1607, 3.1607, 3.1607, 3.1607, 3.1607,\n",
       "          3.1607, 3.1607, 3.1607, 3.1607, 3.1607, 3.1607, 3.1607, 3.1607, 3.1607],\n",
       "         [3.1649, 3.1649, 3.1649, 3.1649, 3.1649, 3.1649, 3.1649, 3.1649, 3.1649,\n",
       "          3.1649, 3.1649, 3.1649, 3.1649, 3.1649, 3.1649, 3.1649, 3.1649, 3.1649,\n",
       "          3.1649, 3.1649, 3.1649, 3.1649, 3.1649, 3.1649, 3.1649, 3.1649, 3.1649,\n",
       "          3.1649, 3.1649, 3.1649, 3.1649, 3.1649, 3.1649, 3.1649, 3.1649, 3.1649]],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_flat = logits.view(-1, logits.size(-1))\n",
    "logits_flat.shape, logits_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4b287d1b-c82f-4f51-bb49-7a542ddd3287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([]), tensor(3.5835, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.cross_entropy(logits_flat, y_flat)\n",
    "loss.shape, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f6a03a-4fae-47bf-8b2f-f4e0ca2fdee8",
   "metadata": {},
   "source": [
    "## Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d587d633-a3ed-49e3-a90a-9c505b079665",
   "metadata": {},
   "source": [
    "We now know just how ~well~ terribly the current model, with its weights and biases, predicts the next token given the input context. We now need to know how to change the different weights and biases to improve the formula.  We could do this by guessing through making minor changes and seeing what improves, or we can think through this more critically.\n",
    "\n",
    "If you review the chain of layers above, you can see that it's a series of formulas.  We can think of this as $f(g(x))$, except with many many more layers and complexities.  Since this is a formula, we can dig into our math toolbox and find a better way to determine what parts need to update.  Recall that in our calculus we learned that differentiation tells us the rate of change in a graph.  So if we treat the loss function $\\mathcal{L}$ as $\\mathcal{L}(f(g(x)))$ taking the partial differential \n",
    "\n",
    "$\\delta=\\partial \\mathcal{L}/\\partial h$\n",
    "\n",
    "at each layer will give us the impact of each weight/bias on our final out (albeit the inverse since our loss function is the negative log likelihood). \n",
    "\n",
    "Lucky for us, each layer of our model already has a placeholder for the partial differential called the **Gradient**. We'll use this field to store it.  We'll start by first zeroing out the gradients. We do this because of the nature of handling partial differentials for multiple dependencies. Recall that in multiple places we had a formula structure of \n",
    "\n",
    "$a+b=c ; a+c= d$\n",
    "\n",
    "In this case $a$ has 2 dependencies and determining the partial derivative of $\\partial d / \\partial a$ requires understanding both the path from $d$ and $c$.  To determine the true impact of a we would sum both partial derivatives together.  Because of this property, the tool we use, the built in `.backwards()` automatically sums gradients, `+=`, so if we do not set the gradient to `0` we then end up with erroneous gradients. \n",
    "\n",
    "Finally, we start `.backwards` from the `loss`, not `logits` as our goal is to minimize loss, we need to ensure we are looking at the calculations that impact loss which requires the whole forward pass to be able to generate the prediction `logits_flat`.  If we think of it as $\\mathcal{L}(f(x))$ where $f(x)$ is the forward pass to generate logits, then a simple chain rule is applied:\n",
    "\n",
    "${\\partial}/{\\partial x} =  \\mathcal{L}'(f(x)) f'(x)$\n",
    "\n",
    "Lets start by zeroing the gradients and leaning on pytorch to calculate the gradients for us. We'll also validate the gradients were `none`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0455dc7a-fab4-4844-bf37-2ac311226907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_head.zero_grad()\n",
    "h2g.zero_grad()\n",
    "x2g.zero_grad()\n",
    "wte.zero_grad()\n",
    "\n",
    "\n",
    "# validate gradients\n",
    "lm_head.weight.grad, wte.weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f54a83-a344-4692-9c3d-ab4978df8cb9",
   "metadata": {},
   "source": [
    "**Auto-Diff** - Now let's see the magic of the gradients populate.  This magic is called auto-differentiation, or auto-diff for short. This allows us to not have to write many layers of nasty code to do the differentiation for us, but, if you're a sadist, you can surely find people who have written out that code (it's not too bad since you just do one layer at a time). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c0b7acd2-55bc-4fe6-9c41-1ddc0ee05eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "119fa3fb-0e8c-4db3-9df8-4a1b361279d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1082, -0.1046, -0.1076],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [-0.1077, -0.1041, -0.1071],\n",
       "         [-0.0366, -0.0330, -0.0360],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [-0.0770, -0.0734, -0.0764],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [-0.0421, -0.0385, -0.0415],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [-0.0399, -0.0363, -0.0393],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [-0.0805, -0.0768, -0.0356],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [-0.0423, -0.1035, -0.0417],\n",
       "         [-0.0422, -0.0386, -0.0416],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236, -0.0387, -0.0417],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [-0.0377, -0.0340, -0.0371],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242]]),\n",
       " tensor([[0.0000e+00, 1.3955e-09, 1.3955e-09, 1.3955e-09],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [2.7494e-09, 2.7494e-09, 2.7494e-09, 2.7494e-09],\n",
       "         [4.0139e-10, 4.0139e-10, 4.0139e-10, 4.0139e-10],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [3.4687e-09, 3.4687e-09, 3.4687e-09, 3.4687e-09],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [5.8984e-09, 0.0000e+00, 5.8984e-09, 5.8984e-09],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [9.1376e-10, 9.1376e-10, 9.1376e-10, 9.1376e-10],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [3.1085e-09, 3.1085e-09, 3.1085e-09, 3.1085e-09],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [2.5113e-09, 2.5113e-09, 2.5113e-09, 2.5113e-09],\n",
       "         [1.8205e-09, 1.8205e-09, 1.8205e-09, 1.8205e-09],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [2.1463e-09, 2.1463e-09, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [2.2270e-09, 2.2270e-09, 0.0000e+00, 2.2270e-09],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [2.0585e-09, 2.0585e-09, 2.0585e-09, 2.0585e-09]]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_head.weight.grad, wte.weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1b1c48-690e-4e1e-81e8-448ac45ad5cd",
   "metadata": {},
   "source": [
    "## Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca073815-41f0-4bb6-bc96-7b7835149ad0",
   "metadata": {},
   "source": [
    "Recall that we discussed at the start that there was an issue called exploding gradients. Exploding gradients occurs because back-propagation through time multiplies gradients by a chain of recurrent Jacobians as follows:\n",
    "\n",
    "$\\nabla_{h_{t-k}} \\mathcal{L} = \\left(\\prod_{i=1}^{k} \\frac{\\partial h_{t-i+1}}{\\partial h_{t-i}}\\right)\\nabla_{h_t}\\mathcal{L}$. \n",
    "\n",
    "This means that any spectral norm (>1) in those factors causes the gradient norm to grow roughly exponentially with sequence length. Even with LSTM gates, poorly conditioned hidden-to-hidden dynamics or large activations can push singular values WELL above 1 where even sigmoid and tanh layers will keep the values at 1. To combat this, we apply gradient clipping. Gradient clipping caps the global gradient, in our case to `1.0`, to prevent runaway steps, numerical overflow, and unstable updates.\n",
    "\n",
    "*Note that this will only solve exploding gradients and does not address vanishing gradients. The LSTM structure and use of memory helps fight vanishing gradients*\n",
    "\n",
    "Since none of our gradients are currently above 1, we will not see an impact of gradient clipping but I did want to introduce the concept as it is an important component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "06f5ad14-65a3-42ec-889a-8e2444e6422c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1082, -0.1046, -0.1076],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [-0.1077, -0.1041, -0.1071],\n",
       "         [-0.0366, -0.0330, -0.0360],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [-0.0770, -0.0734, -0.0764],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [-0.0421, -0.0385, -0.0415],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [-0.0399, -0.0363, -0.0393],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [-0.0805, -0.0768, -0.0356],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [-0.0423, -0.1035, -0.0417],\n",
       "         [-0.0422, -0.0386, -0.0416],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236, -0.0387, -0.0417],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [-0.0377, -0.0340, -0.0371],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242],\n",
       "         [ 0.0236,  0.0273,  0.0242]]),\n",
       " tensor([[0.0000e+00, 1.3955e-09, 1.3955e-09, 1.3955e-09],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [2.7494e-09, 2.7494e-09, 2.7494e-09, 2.7494e-09],\n",
       "         [4.0139e-10, 4.0139e-10, 4.0139e-10, 4.0139e-10],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [3.4687e-09, 3.4687e-09, 3.4687e-09, 3.4687e-09],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [5.8984e-09, 0.0000e+00, 5.8984e-09, 5.8984e-09],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [9.1376e-10, 9.1376e-10, 9.1376e-10, 9.1376e-10],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [3.1085e-09, 3.1085e-09, 3.1085e-09, 3.1085e-09],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [2.5113e-09, 2.5113e-09, 2.5113e-09, 2.5113e-09],\n",
       "         [1.8205e-09, 1.8205e-09, 1.8205e-09, 1.8205e-09],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [2.1463e-09, 2.1463e-09, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [2.2270e-09, 2.2270e-09, 0.0000e+00, 2.2270e-09],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [2.0585e-09, 2.0585e-09, 2.0585e-09, 2.0585e-09]]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.utils.clip_grad_norm_(lm_head.parameters(), 1.0)\n",
    "nn.utils.clip_grad_norm_(h2g.parameters(), 1.0)\n",
    "nn.utils.clip_grad_norm_(x2g.parameters(), 1.0)\n",
    "nn.utils.clip_grad_norm_(wte.parameters(), 1.0)\n",
    "lm_head.weight.grad, wte.weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e2a22f-5ca1-475f-8a34-708d6a914c24",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8564d2f-34d0-4526-80b7-69337e2a7549",
   "metadata": {},
   "source": [
    "The process of learning now requires us to update our weights based on this gradient. To really feel the \"back propagation\" we'll start with the last layer and work backwards, though, since we have all of the gradients calculated already, the order does not matter. Recall that our loss function is the negative log likelihood ratio so our gradient signs are flipped.  If a parameter is important, the gradient will be more negative, and vice versa. The gradients are a ratio of importance of each parameter and we need to know how much of that gradient to apply to our weights. This \"how much\" is referred to as the *learning rate*. In modern training learning rate schedulers and optimizers are used to vary the rate and application by layer and by training round.  We'll keep it simple and use an astronomically high learning rate of `5.000` which applies the gradient directly to the weights via a `-=`. Gradient for the weights and the biases is different as the partial differential with respect to each is different. We need to remember in the layers with bias to apply it to both.\n",
    "\n",
    "*Note that since our vocab is very small, our context is small, and our batch is small, relatively our model is very deep so we will see a lot of exceptionally small gradients*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a2b11f32-0c10-480d-9690-402132913444",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Huge learning rate to emphasize\n",
    "learning_rate = 5.000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f5dd95-3b51-4cb6-8814-65ef9f8b6929",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Let's start with our output layer.  Recall that we initialized the weights to `1.000` so we can quickly see the impact of the gradient update on the weights. Most notable that we'll see is that the entries corresponding with tokens that are present are up weighted and others are downweighted.  Additionally you'll notice that the gradient uses all of the `n_embd` dimensions with different updates on each dimension. Finally, since this is the output layer, we did not include any bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f5c59984-6d36-4e78-b7fc-cf4623294915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1.5410, 1.5229, 1.5380],\n",
       "        [0.8819, 0.8637, 0.8789],\n",
       "        [0.8819, 0.8637, 0.8789],\n",
       "        [0.8819, 0.8637, 0.8789],\n",
       "        [0.8819, 0.8637, 0.8789],\n",
       "        [1.5387, 1.5205, 1.5357],\n",
       "        [1.1830, 1.1649, 1.1800],\n",
       "        [0.8819, 0.8637, 0.8789],\n",
       "        [0.8819, 0.8637, 0.8789],\n",
       "        [1.3851, 1.3670, 1.3821],\n",
       "        [0.8819, 0.8637, 0.8789],\n",
       "        [1.2104, 1.1923, 1.2074],\n",
       "        [0.8819, 0.8637, 0.8789],\n",
       "        [1.1996, 1.1814, 1.1966],\n",
       "        [0.8819, 0.8637, 0.8789],\n",
       "        [1.4023, 1.3841, 1.1778],\n",
       "        [0.8819, 0.8637, 0.8789],\n",
       "        [0.8819, 0.8637, 0.8789],\n",
       "        [0.8819, 0.8637, 0.8789],\n",
       "        [0.8819, 0.8637, 0.8789],\n",
       "        [1.2113, 1.5175, 1.2083],\n",
       "        [1.2111, 1.1930, 1.2081],\n",
       "        [0.8819, 0.8637, 0.8789],\n",
       "        [0.8819, 0.8637, 0.8789],\n",
       "        [0.8819, 0.8637, 0.8789],\n",
       "        [0.8819, 0.8637, 0.8789],\n",
       "        [0.8819, 0.8637, 0.8789],\n",
       "        [0.8819, 0.8637, 0.8789],\n",
       "        [0.8819, 0.8637, 0.8789],\n",
       "        [0.8819, 0.8637, 0.8789],\n",
       "        [0.8819, 1.1934, 1.2085],\n",
       "        [0.8819, 0.8637, 0.8789],\n",
       "        [1.1883, 1.1702, 1.1853],\n",
       "        [0.8819, 0.8637, 0.8789],\n",
       "        [0.8819, 0.8637, 0.8789],\n",
       "        [0.8819, 0.8637, 0.8789]], requires_grad=True)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    lm_head.weight -= learning_rate * lm_head.weight.grad\n",
    "lm_head.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85732c86-fc96-4015-bc28-bb06fa808403",
   "metadata": {},
   "source": [
    "### Recurrent Unit - LSTM\n",
    "Next we'll update our two linear layers inside of the LSTM.  Recall that we initiated h2g to weights `0.2500`, x2g weights to `0.5000` and the biases to.  Since our gradients are so small (1e-9 or so), they won't show up on the weights when we print them, but we can see it in the biases given they were 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "44461eb7-82b4-4383-afab-ef1c18203289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-1.0529e-08, -1.0816e-08, -8.5233e-09, -1.7035e-09, -1.8827e-09,\n",
       "         -1.7154e-09, -6.5187e-08, -7.1495e-08, -6.5682e-08, -7.1856e-09,\n",
       "         -7.3534e-09, -6.2211e-09], requires_grad=True))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    h2g.weight -= learning_rate * h2g.weight.grad\n",
    "    h2g.bias -= learning_rate * h2g.bias.grad\n",
    "h2g.weight, h2g.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "24badddf-11c7-4c52-a31b-d7ef1912c812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-1.0529e-08, -1.0816e-08, -8.5233e-09, -1.7035e-09, -1.8827e-09,\n",
       "         -1.7154e-09, -6.5187e-08, -7.1495e-08, -6.5682e-08, -7.1856e-09,\n",
       "         -7.3534e-09, -6.2211e-09], requires_grad=True))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    x2g.weight -= learning_rate * x2g.weight.grad\n",
    "    x2g.bias -= learning_rate * x2g.bias.grad\n",
    "x2g.weight, x2g.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aab00b-4505-4d2c-b42e-1a60d8e22c65",
   "metadata": {},
   "source": [
    "### Input Layer\n",
    "Finally we'll update our input layer. Recall that we initialized it to `1.000`. You can see that the gradients are once again extremely small.  Also note that the gradients in this case only flow back to the entries for the tokens that were present in our example. This differs from our LM_head since when we calculate back through the recursions we can see that our embeddings select only these rows out of the wte weights meaning our differentiation can only impact them.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b0ced26e-687d-4b68-a373-cffa963a6d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 1.3955e-09, 1.3955e-09, 1.3955e-09],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [2.7494e-09, 2.7494e-09, 2.7494e-09, 2.7494e-09],\n",
       "        [4.0139e-10, 4.0139e-10, 4.0139e-10, 4.0139e-10],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [3.4687e-09, 3.4687e-09, 3.4687e-09, 3.4687e-09],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [5.8984e-09, 0.0000e+00, 5.8984e-09, 5.8984e-09],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [9.1376e-10, 9.1376e-10, 9.1376e-10, 9.1376e-10],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [3.1085e-09, 3.1085e-09, 3.1085e-09, 3.1085e-09],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [2.5113e-09, 2.5113e-09, 2.5113e-09, 2.5113e-09],\n",
       "        [1.8205e-09, 1.8205e-09, 1.8205e-09, 1.8205e-09],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [2.1463e-09, 2.1463e-09, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [2.2270e-09, 2.2270e-09, 0.0000e+00, 2.2270e-09],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [2.0585e-09, 2.0585e-09, 2.0585e-09, 2.0585e-09]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wte.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "19f250b6-4866-4d7d-973b-1997109421f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    wte.weight -= learning_rate * wte.weight.grad\n",
    "wte.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0ff10f-f64f-4801-b9ea-c40e15816684",
   "metadata": {},
   "source": [
    "## Forward Pass with Updated Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5439278f-49d8-47e1-9355-440621636775",
   "metadata": {},
   "source": [
    "Now that we have the updated weights for each layer, let's do another forward pass and compare the loss. Since each layer was previously explained we will instead focus on just showing the outputs of the different layers and the final loss. If you want, you can check the previous outputs in the cached cell outputs above and compare them to see how the weight changes impacted the values at each layer. \n",
    "\n",
    "One key sign that our weights were updated is that you'll see quickly that the values at each layer are no longer as predictable and repetitive.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1450600e-11b1-413c-95d6-51418cc6d445",
   "metadata": {},
   "source": [
    "### Data Re-loading\n",
    "Repulling to a new `x_2`. We'll keep `y` to emphasize the same examples are being used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "00925f19-c83a-4549-b4ca-94f92ad301f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[35, 15, 32,  9,  5, 20, 30, 15],\n",
       "         [11,  9,  6, 20,  5,  0, 13, 21]]),\n",
       " tensor([[15, 32,  9,  5, 20, 30, 15, 11],\n",
       "         [ 9,  6, 20,  5,  0, 13, 21,  0]]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2 = tok_for_training[:-1].view(B, T)\n",
    "x_2, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027c68d0-cf29-431d-85d2-0d0276e55c70",
   "metadata": {},
   "source": [
    "### Input Layer\n",
    "Note that in `wte` since the gradient impact was so small, you hardly see a difference when we have the tensor printed out. In fact, the value is likely so small that we wouldn't even see it unless we selected a value and printed out all of it's impact.  With RNNs, because of the recursion layers, we would need long training and long context to see changes in our input layer initiation. Alternatively, a common step to help improve the input embedding is to do weight-tying between the input and output layers. This ensures that the input and output layers embed in the same space and helps with learning stabilization, but weight tying goes in and out of flavor often.  Because of this we'll cover weight-tying in the GPT notebook I write. \n",
    "\n",
    "Since our weights were barely tweaked, we'll again see very flat values at 1.000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ff392189-88e9-46a8-9ce1-b1aacad80e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 4]),\n",
       " tensor([[[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]]], grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = wte(x_2)\n",
    "x.shape, x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522598e4-600b-49ac-aca8-5ce33a9f8c8e",
   "metadata": {},
   "source": [
    "### Dropout \n",
    "Once again we'll perform dropout. Similar to the explanation above, the impacts of training are beyond the visible decimals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9f0c0ab4-a8da-4b22-ba93-92070f03527a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.1111, 1.1111, 1.1111, 0.0000],\n",
       "         [0.0000, 1.1111, 1.1111, 1.1111],\n",
       "         [1.1111, 1.1111, 1.1111, 1.1111],\n",
       "         [1.1111, 1.1111, 1.1111, 0.0000],\n",
       "         [1.1111, 1.1111, 1.1111, 1.1111],\n",
       "         [1.1111, 1.1111, 1.1111, 1.1111],\n",
       "         [1.1111, 1.1111, 1.1111, 1.1111],\n",
       "         [1.1111, 1.1111, 1.1111, 1.1111]],\n",
       "\n",
       "        [[1.1111, 1.1111, 1.1111, 1.1111],\n",
       "         [1.1111, 1.1111, 1.1111, 1.1111],\n",
       "         [0.0000, 1.1111, 1.1111, 0.0000],\n",
       "         [0.0000, 1.1111, 1.1111, 1.1111],\n",
       "         [1.1111, 1.1111, 1.1111, 1.1111],\n",
       "         [1.1111, 1.1111, 1.1111, 1.1111],\n",
       "         [1.1111, 1.1111, 1.1111, 1.1111],\n",
       "         [1.1111, 1.1111, 1.1111, 1.1111]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = dropout(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a5ddc4-503d-4a8c-8635-2a6b26d4750e",
   "metadata": {},
   "source": [
    "### Recurrent Unit - LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e6d785-4282-431d-829b-e3ade8326816",
   "metadata": {},
   "source": [
    "This time through we'll run the recurrent layer for all time periods and not break out a specific time period.  We still have to reset our hidden layer and our memory cell to `0` since our training batch reset. Also, since the weight and bias updates were so small, we do expect that the output H will still show a lot of uniformity.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3a88d1ae-b4e8-46e0-9538-f94589005472",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_next = torch.zeros(B_batch, hidden_size) \n",
    "c_next = torch.zeros(B_batch, hidden_size) \n",
    "hs = x.new_empty(B_batch, T_context, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f8ce97ab-1e58-4c12-b558-f9c216b341e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5505, 0.5505, 0.5505],\n",
       "         [0.8134, 0.8134, 0.8134],\n",
       "         [0.9292, 0.9292, 0.9292],\n",
       "         [0.9103, 0.9103, 0.9103],\n",
       "         [0.9473, 0.9473, 0.9473],\n",
       "         [0.9493, 0.9493, 0.9493],\n",
       "         [0.9495, 0.9495, 0.9495],\n",
       "         [0.9495, 0.9495, 0.9495]],\n",
       "\n",
       "        [[0.6379, 0.6379, 0.6379],\n",
       "         [0.8826, 0.8826, 0.8826],\n",
       "         [0.8380, 0.8380, 0.8380],\n",
       "         [0.9039, 0.9039, 0.9039],\n",
       "         [0.9468, 0.9468, 0.9468],\n",
       "         [0.9492, 0.9492, 0.9492],\n",
       "         [0.9495, 0.9495, 0.9495],\n",
       "         [0.9495, 0.9495, 0.9495]]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for t in range(T_context):\n",
    "    \n",
    "    # set input and previous\n",
    "    h_t = h_next\n",
    "    c_t = c_next\n",
    "    x_t = x[:, t, :]\n",
    "\n",
    "    # calculate weights\n",
    "    xi, xf, xo, xc = x2g(x_t).split(hidden_size, dim=-1)\n",
    "    hi, hf, ho, hc = h2g(h_t).split(hidden_size, dim=-1)\n",
    "    \n",
    "    #input, forget, and output gate \n",
    "    input_gate =  torch.sigmoid(xi + hi)\n",
    "    forget_gate =  torch.sigmoid(xf + hf)\n",
    "    output_gate =  torch.sigmoid(xo + ho)\n",
    "\n",
    "    # input state\n",
    "    input_state = torch.tanh(xc + hc)\n",
    "\n",
    "    # memory cell\n",
    "    c_next = (forget_gate*c_t) + (input_gate*input_state)\n",
    "\n",
    "    # hidden state / recursion output\n",
    "    h_next = output_gate*torch.tanh(c_next)\n",
    "\n",
    "    # save h_t\n",
    "    hs[:, t, :] = h_next\n",
    "\n",
    "hs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8612c53d-c55b-4940-852d-79498d489245",
   "metadata": {},
   "source": [
    "### Final Dropout\n",
    "As with before, we'll run dropout again before running the model head.  We should similarly see the zeroing out of some values and the probability updates to others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8f4b0ebe-0dab-458c-bba1-0d0617a9b294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6117, 0.6117, 0.0000],\n",
       "         [0.9038, 0.9038, 0.9038],\n",
       "         [1.0325, 1.0325, 1.0325],\n",
       "         [1.0115, 1.0115, 1.0115],\n",
       "         [1.0525, 1.0525, 1.0525],\n",
       "         [1.0547, 1.0547, 1.0547],\n",
       "         [1.0550, 1.0550, 1.0550],\n",
       "         [1.0550, 1.0550, 1.0550]],\n",
       "\n",
       "        [[0.7088, 0.0000, 0.7088],\n",
       "         [0.9807, 0.9807, 0.0000],\n",
       "         [0.9312, 0.0000, 0.9312],\n",
       "         [1.0043, 1.0043, 1.0043],\n",
       "         [1.0521, 1.0521, 1.0521],\n",
       "         [1.0547, 1.0547, 1.0547],\n",
       "         [1.0549, 1.0549, 1.0549],\n",
       "         [1.0550, 1.0550, 1.0550]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = dropout(hs)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f1e47d-27d3-4039-8ea1-208d5fc30f29",
   "metadata": {},
   "source": [
    "### Output Layers AKA Model Head.\n",
    "This is the layer where we expect to see the most updates.  Recall that the weights on the model head had the most notable changes both shifting more positive the values for the token that were present in our training data and shifting more negative the value for the token entries that were not present.  As such, if you look at the entries for each example you should see that the values are no longer equal for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5c2c2354-3b5c-4490-ac42-fd8474355857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 36]),\n",
       " tensor([[[1.8742, 1.0678, 1.0678, 1.0678, 1.0678, 1.8714, 1.4362, 1.0678,\n",
       "           1.0678, 1.6835, 1.0678, 1.4698, 1.0678, 1.4565, 1.0678, 1.7045,\n",
       "           1.0678, 1.0678, 1.0678, 1.0678, 1.6693, 1.4706, 1.0678, 1.0678,\n",
       "           1.0678, 1.0678, 1.0678, 1.0678, 1.0678, 1.0678, 1.2695, 1.0678,\n",
       "           1.4428, 1.0678, 1.0678, 1.0678],\n",
       "          [4.1591, 2.3719, 2.3719, 2.3719, 2.3719, 4.1527, 3.1884, 2.3719,\n",
       "           2.3719, 3.7364, 2.3719, 3.2627, 2.3719, 3.2333, 2.3719, 3.5827,\n",
       "           2.3719, 2.3719, 2.3719, 2.3719, 3.5583, 3.2646, 2.3719, 2.3719,\n",
       "           2.3719, 2.3719, 2.3719, 2.3719, 2.3719, 2.3719, 2.9678, 2.3719,\n",
       "           3.2028, 2.3719, 2.3719, 2.3719],\n",
       "          [4.7515, 2.7098, 2.7098, 2.7098, 2.7098, 4.7442, 3.6425, 2.7098,\n",
       "           2.7098, 4.2686, 2.7098, 3.7274, 2.7098, 3.6939, 2.7098, 4.0931,\n",
       "           2.7098, 2.7098, 2.7098, 2.7098, 4.0651, 3.7296, 2.7098, 2.7098,\n",
       "           2.7098, 2.7098, 2.7098, 2.7098, 2.7098, 2.7098, 3.3905, 2.7098,\n",
       "           3.6590, 2.7098, 2.7098, 2.7098],\n",
       "          [4.6546, 2.6545, 2.6545, 2.6545, 2.6545, 4.6475, 3.5683, 2.6545,\n",
       "           2.6545, 4.1816, 2.6545, 3.6515, 2.6545, 3.6186, 2.6545, 4.0096,\n",
       "           2.6545, 2.6545, 2.6545, 2.6545, 3.9822, 3.6536, 2.6545, 2.6545,\n",
       "           2.6545, 2.6545, 2.6545, 2.6545, 2.6545, 2.6545, 3.3214, 2.6545,\n",
       "           3.5844, 2.6545, 2.6545, 2.6545],\n",
       "          [4.8437, 2.7624, 2.7624, 2.7624, 2.7624, 4.8363, 3.7132, 2.7624,\n",
       "           2.7624, 4.3515, 2.7624, 3.7998, 2.7624, 3.7656, 2.7624, 4.1725,\n",
       "           2.7624, 2.7624, 2.7624, 2.7624, 4.1440, 3.8020, 2.7624, 2.7624,\n",
       "           2.7624, 2.7624, 2.7624, 2.7624, 2.7624, 2.7624, 3.4563, 2.7624,\n",
       "           3.7301, 2.7624, 2.7624, 2.7624],\n",
       "          [4.8538, 2.7681, 2.7681, 2.7681, 2.7681, 4.8464, 3.7210, 2.7681,\n",
       "           2.7681, 4.3605, 2.7681, 3.8077, 2.7681, 3.7734, 2.7681, 4.1812,\n",
       "           2.7681, 2.7681, 2.7681, 2.7681, 4.1526, 3.8099, 2.7681, 2.7681,\n",
       "           2.7681, 2.7681, 2.7681, 2.7681, 2.7681, 2.7681, 3.4635, 2.7681,\n",
       "           3.7378, 2.7681, 2.7681, 2.7681],\n",
       "          [4.8549, 2.7687, 2.7687, 2.7687, 2.7687, 4.8474, 3.7218, 2.7687,\n",
       "           2.7687, 4.3615, 2.7687, 3.8085, 2.7687, 3.7742, 2.7687, 4.1821,\n",
       "           2.7687, 2.7687, 2.7687, 2.7687, 4.1535, 3.8107, 2.7687, 2.7687,\n",
       "           2.7687, 2.7687, 2.7687, 2.7687, 2.7687, 2.7687, 3.4643, 2.7687,\n",
       "           3.7386, 2.7687, 2.7687, 2.7687],\n",
       "          [4.8551, 2.7688, 2.7688, 2.7688, 2.7688, 4.8477, 3.7219, 2.7688,\n",
       "           2.7688, 4.3617, 2.7688, 3.8087, 2.7688, 3.7744, 2.7688, 4.1823,\n",
       "           2.7688, 2.7688, 2.7688, 2.7688, 4.1537, 3.8109, 2.7688, 2.7688,\n",
       "           2.7688, 2.7688, 2.7688, 2.7688, 2.7688, 2.7688, 3.4644, 2.7688,\n",
       "           3.7388, 2.7688, 2.7688, 2.7688]],\n",
       " \n",
       "         [[2.1825, 1.2480, 1.2480, 1.2480, 1.2480, 2.1792, 1.6749, 1.2480,\n",
       "           1.2480, 1.9615, 1.2480, 1.7138, 1.2480, 1.6984, 1.2480, 1.8288,\n",
       "           1.2480, 1.2480, 1.2480, 1.2480, 1.7151, 1.7148, 1.2480, 1.2480,\n",
       "           1.2480, 1.2480, 1.2480, 1.2480, 1.2480, 1.2480, 1.4817, 1.2480,\n",
       "           1.6825, 1.2480, 1.2480, 1.2480],\n",
       "          [3.0047, 1.7119, 1.7119, 1.7119, 1.7119, 3.0001, 2.3025, 1.7119,\n",
       "           1.7119, 2.6989, 1.7119, 2.3563, 1.7119, 2.3350, 1.7119, 2.7326,\n",
       "           1.7119, 1.7119, 1.7119, 1.7119, 2.6761, 2.3576, 1.7119, 1.7119,\n",
       "           1.7119, 1.7119, 1.7119, 1.7119, 1.7119, 1.7119, 2.0351, 1.7119,\n",
       "           2.3129, 1.7119, 1.7119, 1.7119],\n",
       "          [2.8671, 1.6396, 1.6396, 1.6396, 1.6396, 2.8628, 2.2004, 1.6396,\n",
       "           1.6396, 2.5768, 1.6396, 2.2514, 1.6396, 2.2312, 1.6396, 2.4025,\n",
       "           1.6396, 1.6396, 1.6396, 1.6396, 2.2531, 2.2527, 1.6396, 1.6396,\n",
       "           1.6396, 1.6396, 1.6396, 1.6396, 1.6396, 1.6396, 1.9465, 1.6396,\n",
       "           2.2103, 1.6396, 1.6396, 1.6396],\n",
       "          [4.6217, 2.6357, 2.6357, 2.6357, 2.6357, 4.6146, 3.5430, 2.6357,\n",
       "           2.6357, 4.1520, 2.6357, 3.6256, 2.6357, 3.5929, 2.6357, 3.9812,\n",
       "           2.6357, 2.6357, 2.6357, 2.6357, 3.9540, 3.6277, 2.6357, 2.6357,\n",
       "           2.6357, 2.6357, 2.6357, 2.6357, 2.6357, 2.6357, 3.2978, 2.6357,\n",
       "           3.5591, 2.6357, 2.6357, 2.6357],\n",
       "          [4.8415, 2.7611, 2.7611, 2.7611, 2.7611, 4.8341, 3.7115, 2.7611,\n",
       "           2.7611, 4.3495, 2.7611, 3.7980, 2.7611, 3.7638, 2.7611, 4.1706,\n",
       "           2.7611, 2.7611, 2.7611, 2.7611, 4.1421, 3.8002, 2.7611, 2.7611,\n",
       "           2.7611, 2.7611, 2.7611, 2.7611, 2.7611, 2.7611, 3.4547, 2.7611,\n",
       "           3.7283, 2.7611, 2.7611, 2.7611],\n",
       "          [4.8535, 2.7680, 2.7680, 2.7680, 2.7680, 4.8461, 3.7207, 2.7680,\n",
       "           2.7680, 4.3603, 2.7680, 3.8075, 2.7680, 3.7732, 2.7680, 4.1809,\n",
       "           2.7680, 2.7680, 2.7680, 2.7680, 4.1524, 3.8097, 2.7680, 2.7680,\n",
       "           2.7680, 2.7680, 2.7680, 2.7680, 2.7680, 2.7680, 3.4633, 2.7680,\n",
       "           3.7376, 2.7680, 2.7680, 2.7680],\n",
       "          [4.8548, 2.7687, 2.7687, 2.7687, 2.7687, 4.8474, 3.7217, 2.7687,\n",
       "           2.7687, 4.3614, 2.7687, 3.8085, 2.7687, 3.7742, 2.7687, 4.1821,\n",
       "           2.7687, 2.7687, 2.7687, 2.7687, 4.1535, 3.8107, 2.7687, 2.7687,\n",
       "           2.7687, 2.7687, 2.7687, 2.7687, 2.7687, 2.7687, 3.4642, 2.7687,\n",
       "           3.7386, 2.7687, 2.7687, 2.7687],\n",
       "          [4.8551, 2.7688, 2.7688, 2.7688, 2.7688, 4.8476, 3.7219, 2.7688,\n",
       "           2.7688, 4.3617, 2.7688, 3.8087, 2.7688, 3.7744, 2.7688, 4.1823,\n",
       "           2.7688, 2.7688, 2.7688, 2.7688, 4.1537, 3.8109, 2.7688, 2.7688,\n",
       "           2.7688, 2.7688, 2.7688, 2.7688, 2.7688, 2.7688, 3.4644, 2.7688,\n",
       "           3.7388, 2.7688, 2.7688, 2.7688]]], grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = lm_head(x)\n",
    "logits.shape, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ed1cae-6af9-4211-82b1-8930196c48c3",
   "metadata": {},
   "source": [
    "### Updated Loss calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c101fbc-97fc-4dc2-b99e-1cff59d22f5d",
   "metadata": {},
   "source": [
    "Now we'll calculate the updated loss.  Our first pass's loss was 3.5835. Since we're passing through the same example and used a fairly high learning rate we should see a significant improvement with just 1 learning pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0a12e542-dda9-4cb4-9611-bcb157515806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5835, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c2c2eb99-d188-4de7-a4ee-27917e3cec75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([]) tensor(2.9012, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y_flat = y.view(-1)\n",
    "logits_flat = logits.view(-1, logits.size(-1))\n",
    "updated_loss = F.cross_entropy(logits_flat, y_flat)\n",
    "print(updated_loss.shape, updated_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "53363221-ba7b-49f8-8f6a-88c6db647277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 round of training resulted in an loss improvment of 0.6823'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'1 round of training resulted in an loss improvment of {loss.item() - updated_loss.item():.4f}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f56dfda-a142-40b3-b66f-60cb6cae0a8e",
   "metadata": {},
   "source": [
    "# SUCCESS!\n",
    "Our training improved the loss by about **~17%** (amount may vary since we didn't set a seed). There are flaws with this, mainly passing the same example through a second time and a high learning rate, but this helps show the fundamentals of what learning does inside a LSTM style RNN model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b08f0e0-e068-4ab6-a041-00fa9c081dea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
