{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2ea846f-d5a9-4f61-b3b1-fed90e2edde3",
   "metadata": {},
   "source": [
    "# RNN LSTM Explainer\n",
    "\n",
    "The goal is to walk through RNNs (recurring neural networks), there are 2 flavors: GRUs and LSTMs. We'll focus on Long Short-Term Memory (LSTM).\n",
    "\n",
    "To help display the transformation, we'll use the first sentence from the [linear algebra wiki page](https://en.wikipedia.org/wiki/Linear_algebra) and [lu decomposition wiki page](https://en.wikipedia.org/wiki/LU_decomposition) as the topic is fitting and it shows us some non-standard patterns.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08f8d05-f237-4a81-8f88-be839e4a9e51",
   "metadata": {},
   "source": [
    "## Text Prep/Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb1237d-fd4f-43bc-ba02-88e82368b385",
   "metadata": {},
   "source": [
    "we'll start with common preprocessing step of tokenizing the data.  This converts the string text into an array of numbers that can be used during the training loop.  I've built a very subtle byte-pair encdoing that has each unique character that appears and the top 5 merges. This keeps our vocab size small and managable for this example. Typically the vocab size is in the 100K+ range. A great library for this is `tiktoken`. Tokenization simply finds the longest pattern of characters that's in common with what was trained and replaces it with an integer that represents it.  This way we turn the text into a numeric array to simplify computing. import torch\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baeedf57-4218-4b0e-be46-217723a9034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef2240c1-dd4e-4f8a-9fb1-acff3ff200c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBPETokenizer:\n",
    "    def __init__(self, num_merges=5, eot_token='<|endoftext|>'):\n",
    "        self.num_merges = num_merges\n",
    "        self.eot_token = eot_token\n",
    "        self.eot_id = None\n",
    "        self.merges = []\n",
    "        self.pair_ranks = {}\n",
    "        self.vocab = {}\n",
    "        self.id_to_token = {}\n",
    "\n",
    "    def _add_token(self, tok):\n",
    "        if tok in self.vocab:\n",
    "            return self.vocab[tok]\n",
    "        i = len(self.vocab)\n",
    "        self.vocab[tok] = i\n",
    "        self.id_to_token[i] = tok\n",
    "        return i\n",
    "\n",
    "    def _get_bigrams(self, seq):\n",
    "        for i in range(len(seq) - 1):\n",
    "            yield (seq[i], seq[i + 1])\n",
    "\n",
    "    def _merge_once(self, seq, pair):\n",
    "        a, b = pair\n",
    "        out = []\n",
    "        i = 0\n",
    "        while i < len(seq):\n",
    "            if i < len(seq) - 1 and seq[i] == a and seq[i + 1] == b:\n",
    "                out.append(a + b)\n",
    "                i += 2\n",
    "            else:\n",
    "                out.append(seq[i])\n",
    "                i += 1\n",
    "        return out\n",
    "\n",
    "    def train(self, corpus):\n",
    "        # corpus: list[str]\n",
    "        text = ''.join(corpus).lower()\n",
    "        seq = list(text)\n",
    "        merges = []\n",
    "        for _ in range(self.num_merges):\n",
    "            counts = Counter(self._get_bigrams(seq))\n",
    "            if not counts: break\n",
    "            best_pair, _ = counts.most_common(1)[0]\n",
    "            merges.append(best_pair)\n",
    "            seq = self._merge_once(seq, best_pair)\n",
    "        self.merges = merges\n",
    "        self.pair_ranks = {p: i for i, p in enumerate(self.merges)}\n",
    "\n",
    "        self.vocab = {}\n",
    "        self.id_to_token = {}\n",
    "        for ch in sorted(set(text)):\n",
    "            self._add_token(ch)\n",
    "        for a, b in self.merges:\n",
    "            self._add_token(a + b)\n",
    "        self.eot_id = self._add_token(self.eot_token)\n",
    "\n",
    "    def encode(self, text, force_last_eot=True):\n",
    "        # treat literal eot marker as special; remove it from content\n",
    "        if self.eot_token in text:\n",
    "            text = text.replace(self.eot_token, '')\n",
    "        seq = list(text)\n",
    "\n",
    "        # make sure all seen base chars exist\n",
    "        for ch in set(seq):\n",
    "            if ch not in self.vocab:\n",
    "                self._add_token(ch)\n",
    "\n",
    "        # greedy BPE using learned pair ranks\n",
    "        if self.merges:\n",
    "            while True:\n",
    "                best_pair, best_rank = None, None\n",
    "                for p in self._get_bigrams(seq):\n",
    "                    r = self.pair_ranks.get(p)\n",
    "                    if r is not None and (best_rank is None or r < best_rank):\n",
    "                        best_pair, best_rank = p, r\n",
    "                if best_pair is None:\n",
    "                    break\n",
    "                seq = self._merge_once(seq, best_pair)\n",
    "\n",
    "        # ensure all tokens in seq exist in vocab (e.g., if new chars appeared)\n",
    "        for tok in seq:\n",
    "            if tok not in self.vocab:\n",
    "                self._add_token(tok)\n",
    "\n",
    "        ids = [self.vocab[tok] for tok in seq]\n",
    "\n",
    "        # FORCE: append EOT id if not already last\n",
    "        if force_last_eot:\n",
    "            if not ids or ids[-1] != self.eot_id:\n",
    "                ids.append(self.eot_id)\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # drop trailing EOT if present\n",
    "        if ids and self.eot_id is not None and ids[-1] == self.eot_id:\n",
    "            ids = ids[:-1]\n",
    "        toks = [self.id_to_token[i] for i in ids]\n",
    "        return ''.join(toks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b35d09cf-0ce1-47a0-a93f-8213bea8f711",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_example_1 = r'''Linear algebra is central to almost all areas of mathematics. For instance, linear algebra is fundamental in modern presentations of geometry, including for defining basic objects such as lines, planes and rotations. Also, functional analysis, a branch of mathematical analysis, may be viewed as the application of linear algebra to function spaces.'''\n",
    "raw_example_2 = r'''In numerical analysis and linear algebra, lower–upper (LU) decomposition or factorization factors a matrix as the product of a lower triangular matrix and an upper triangular matrix (see matrix multiplication and matrix decomposition).'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7064c634-a608-4a62-93fe-c89219aa5dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 'a'), ('a', 't'), ('i', 'n'), (' ', 'm'), ('i', 'o')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = SimpleBPETokenizer(num_merges=5)\n",
    "tok.train([raw_example_1,raw_example_2])\n",
    "tok.merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "133f8d70-16cd-4327-91a2-4e887612bcdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '(': 1,\n",
       " ')': 2,\n",
       " ',': 3,\n",
       " '.': 4,\n",
       " 'a': 5,\n",
       " 'b': 6,\n",
       " 'c': 7,\n",
       " 'd': 8,\n",
       " 'e': 9,\n",
       " 'f': 10,\n",
       " 'g': 11,\n",
       " 'h': 12,\n",
       " 'i': 13,\n",
       " 'j': 14,\n",
       " 'l': 15,\n",
       " 'm': 16,\n",
       " 'n': 17,\n",
       " 'o': 18,\n",
       " 'p': 19,\n",
       " 'r': 20,\n",
       " 's': 21,\n",
       " 't': 22,\n",
       " 'u': 23,\n",
       " 'v': 24,\n",
       " 'w': 25,\n",
       " 'x': 26,\n",
       " 'y': 27,\n",
       " 'z': 28,\n",
       " '–': 29,\n",
       " ' a': 30,\n",
       " 'at': 31,\n",
       " 'in': 32,\n",
       " ' m': 33,\n",
       " 'io': 34,\n",
       " '<|endoftext|>': 35}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9804d527-b590-4bb5-8158-8cdcbc179719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tok.vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29443925-8749-4b7c-a500-4a3bdbf808d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35, 15, 32,  9,  5, 20, 30, 15, 11,  9,  6, 20,  5,  0, 13, 21,  0,  7,\n",
       "         9, 17, 22, 20,  5, 15,  0, 22, 18, 30, 15, 16, 18, 21, 22, 30, 15, 15,\n",
       "        30, 20,  9,  5, 21,  0, 18, 10, 33, 31, 12,  9, 16, 31, 13,  7, 21,  4,\n",
       "         0, 10, 18, 20,  0, 32, 21, 22,  5, 17,  7,  9,  3,  0, 15, 32,  9,  5,\n",
       "        20, 30, 15, 11,  9,  6, 20,  5,  0, 13, 21,  0, 10, 23, 17,  8,  5, 16,\n",
       "         9, 17, 22,  5, 15,  0, 32, 33, 18,  8,  9, 20, 17,  0, 19, 20,  9, 21,\n",
       "         9, 17, 22, 31, 34, 17, 21,  0, 18, 10,  0, 11,  9, 18, 16,  9, 22, 20,\n",
       "        27,  3,  0, 32,  7, 15, 23,  8, 32, 11,  0, 10, 18, 20,  0,  8,  9, 10,\n",
       "        32, 32, 11,  0,  6,  5, 21, 13,  7,  0, 18,  6, 14,  9,  7, 22, 21,  0,\n",
       "        21, 23,  7, 12, 30, 21,  0, 15, 32,  9, 21,  3,  0, 19, 15,  5, 17,  9,\n",
       "        21, 30, 17,  8,  0, 20, 18, 22, 31, 34, 17, 21,  4, 30, 15, 21, 18,  3,\n",
       "         0, 10, 23, 17,  7, 22, 34, 17,  5, 15, 30, 17,  5, 15, 27, 21, 13, 21,\n",
       "         3, 30,  0,  6, 20,  5, 17,  7, 12,  0, 18, 10, 33, 31, 12,  9, 16, 31,\n",
       "        13,  7,  5, 15, 30, 17,  5, 15, 27, 21, 13, 21,  3, 33,  5, 27,  0,  6,\n",
       "         9,  0, 24, 13,  9, 25,  9,  8, 30, 21,  0, 22, 12,  9, 30, 19, 19, 15,\n",
       "        13,  7, 31, 34, 17,  0, 18, 10,  0, 15, 32,  9,  5, 20, 30, 15, 11,  9,\n",
       "         6, 20,  5,  0, 22, 18,  0, 10, 23, 17,  7, 22, 34, 17,  0, 21, 19,  5,\n",
       "         7,  9, 21,  4, 35, 35, 32,  0, 17, 23, 16,  9, 20, 13,  7,  5, 15, 30,\n",
       "        17,  5, 15, 27, 21, 13, 21, 30, 17,  8,  0, 15, 32,  9,  5, 20, 30, 15,\n",
       "        11,  9,  6, 20,  5,  3,  0, 15, 18, 25,  9, 20, 29, 23, 19, 19,  9, 20,\n",
       "         0,  1, 15, 23,  2,  0,  8,  9,  7, 18, 16, 19, 18, 21, 13, 22, 34, 17,\n",
       "         0, 18, 20,  0, 10,  5,  7, 22, 18, 20, 13, 28, 31, 34, 17,  0, 10,  5,\n",
       "         7, 22, 18, 20, 21, 30, 33, 31, 20, 13, 26, 30, 21,  0, 22, 12,  9,  0,\n",
       "        19, 20, 18,  8, 23,  7, 22,  0, 18, 10, 30,  0, 15, 18, 25,  9, 20,  0,\n",
       "        22, 20, 13,  5, 17, 11, 23, 15,  5, 20, 33, 31, 20, 13, 26, 30, 17,  8,\n",
       "        30, 17,  0, 23, 19, 19,  9, 20,  0, 22, 20, 13,  5, 17, 11, 23, 15,  5,\n",
       "        20, 33, 31, 20, 13, 26,  0,  1, 21,  9,  9, 33, 31, 20, 13, 26, 33, 23,\n",
       "        15, 22, 13, 19, 15, 13,  7, 31, 34, 17, 30, 17,  8, 33, 31, 20, 13, 26,\n",
       "         0,  8,  9,  7, 18, 16, 19, 18, 21, 13, 22, 34, 17,  2,  4, 35])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eot = tok.eot_id\n",
    "tokens = []\n",
    "for example in [raw_example_1, raw_example_2]:\n",
    "    tokens.extend([eot])\n",
    "    tokens.extend(tok.encode(example.lower()))\n",
    "all_tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775f0cb1-fdac-400c-a80d-1a5da154434c",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bd76b6-dfd5-4f08-a289-630af13649f0",
   "metadata": {},
   "source": [
    "A machine learning model forward pass now uses the tokenization information, runs several layers of linear algebra on it, and then \"predicts\" the next token. When it is noisy (like you will see in this example), this process results in gibberish.  The training process changes the noise to pattern during the \"backward pass\" as you'll see.    We'll show 3 steps that are focused on training:\n",
    "1. **Data Loading** `x, y = train_loader.next_batch()` - this step pulls from the raw data enough tokens to complete a forward and backward pass.  If the model is inference only, this step is replaced with taking in the inference input and preparing it similarly as the forward pass.\n",
    "2. **Forward Pass** `logits, loss = model(x, y)` - using the data and the model architecture to predict the next token. When training we also compare against the expected to get loss, but in infrerence, we use the logits to complete the inference task.\n",
    "3. **Backward Pass & Training** `loss.backward(); optimizer.step()` - using differentials to understand what parameters most impact the forward pass' impact on its prediction, comparing that against what is actually right based on the data loading step, and then making very minor adjustments to the impactful parameters with the hope it improves future predictions.\n",
    "\n",
    "The we'll show a final **Forward Pass** with the updated weights we did in #3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da8d4c4-ae00-465d-ae6f-ea6e1be858f0",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209d7e34-19d0-4c9b-bc7c-b89c09052167",
   "metadata": {},
   "source": [
    "To start, we need to get enough data to run the forward and backward passes.  Since our total dataset is likely too big to hold all at once in real practice, we would read just enough file information into memory so that we can run the passes, leaving memory and compute to be used on the passes instead of static data holding. \n",
    "To start, we have to identify the batch size and the model context length to determine how much data we need.  Consequently, these dimensions also form 2 of the 3 dimensions in the initial matrix.\n",
    "- **Batch Size (B)** - This is the number of examples you'll train on in a single pass. \n",
    "- **Context Length (T)** - This is the max number of tokens that a model can use in a single pass to generat the next token. If an example is below this length, it can be padded.\n",
    "  \n",
    "*Ideally both B and T are multiples of 2 to work nicely with chip architecture. This is a common theme across the board*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1349878b-f4cd-4691-93fc-f8f2d76cb28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 2 # Batch\n",
    "T = 8 # context length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0eda05fa-9c31-4618-ae8c-ed1317467795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35, 15, 32,  9,  5, 20, 30, 15, 11,  9,  6, 20,  5,  0, 13, 21,  0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_position = 0\n",
    "tok_for_training = all_tokens[current_position:current_position + B*T +1 ]\n",
    "tok_for_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3cc8223-0f42-4756-af7c-3af62ce59551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35, 15, 32,  9,  5, 20, 30, 15, 11,  9,  6, 20,  5,  0, 13, 21,  0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_for_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f36ec177-74d7-4cb9-b84d-75a10a6c6694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[35, 15, 32,  9,  5, 20, 30, 15],\n",
       "        [11,  9,  6, 20,  5,  0, 13, 21]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=tok_for_training[:-1].view(B, T)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8d87eb2-5bc8-44cb-80d4-806c3a3bf93d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15, 32,  9,  5, 20, 30, 15, 11],\n",
       "        [ 9,  6, 20,  5,  0, 13, 21,  0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=tok_for_training[1:].view(B, T)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd96f05-28c7-482a-bd47-d8226e65d235",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83e785e1-7328-4b6f-83f9-a98247f06bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa762304-604b-458a-b0d5-2d787de76448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_batch, T_context = x.size()\n",
    "B_batch, T_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a6d9984-9890-4b37-b538-5a09fdb7efed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 36)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_embd = 4 # level of embedding of input tokens\n",
    "n_embd, vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf3ccab-d62d-41f8-9539-4a257e9ca2a8",
   "metadata": {},
   "source": [
    "**Embedding input**\n",
    "\n",
    "Same as with transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be607d75-1447-4339-9145-0f18f7e294b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wte = nn.Embedding(vocab_size, n_embd)\n",
    "torch.nn.init.ones_(wte.weight)\n",
    "wte.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10b37914-4be6-4ed6-ad7b-daa43db954ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 4]),\n",
       " tensor([[[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]]], grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = wte(x)\n",
    "x.shape, x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf83182-f32a-4da0-93c8-8bb4f41cce49",
   "metadata": {},
   "source": [
    "**Dropout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfa012d0-91b1-474d-b961-ebfb4ef1889f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dropout(p=0.1, inplace=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout = nn.Dropout(0.1)\n",
    "dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29b56d86-9cfe-4b21-ba08-1b1dba07fc46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.1111, 1.1111, 1.1111, 1.1111],\n",
       "         [1.1111, 1.1111, 1.1111, 1.1111],\n",
       "         [0.0000, 1.1111, 1.1111, 0.0000],\n",
       "         [1.1111, 1.1111, 0.0000, 1.1111],\n",
       "         [1.1111, 1.1111, 1.1111, 1.1111],\n",
       "         [0.0000, 1.1111, 1.1111, 1.1111],\n",
       "         [1.1111, 1.1111, 0.0000, 1.1111],\n",
       "         [1.1111, 1.1111, 1.1111, 1.1111]],\n",
       "\n",
       "        [[1.1111, 1.1111, 0.0000, 1.1111],\n",
       "         [1.1111, 1.1111, 1.1111, 1.1111],\n",
       "         [1.1111, 1.1111, 1.1111, 1.1111],\n",
       "         [1.1111, 1.1111, 1.1111, 1.1111],\n",
       "         [1.1111, 1.1111, 1.1111, 1.1111],\n",
       "         [1.1111, 1.1111, 1.1111, 1.1111],\n",
       "         [1.1111, 1.1111, 0.0000, 0.0000],\n",
       "         [1.1111, 1.1111, 1.1111, 1.1111]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = dropout(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fbacb6-0960-4e47-b541-342263b58c57",
   "metadata": {},
   "source": [
    "**Recurrent Block**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9c2ecb-580d-402c-b932-f00d799ab5b1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In an LSTM at time (t): (i_t) is the input gate, (f_t) is the forget gate, (o_t) is the output gate, and (g_t) is the candidate cell content (sometimes written $\\tilde{c}_t)$. (c_t) is the cell state after update, and (h_t) is the hidden state (the exposed output).\n",
    "\n",
    "Using the usual affine “gate packs,” $x2g$ denotes the learned input-to-gates projection $W_x x_t + b_x \\in \\mathbb{R}^{4H}$ and $h2g$ the hidden-to-gates projection $W_h h_{t-1} + b_h \\in \\mathbb{R}^{4H}$. Their sum is split into four (H)-wide chunks to get the preactivations for the four gate blocks.\n",
    "\n",
    "The standard equations $with (x_t \\in \\mathbb{R}^{B\\times d}), (h_{t-1},h_t,c_{t-1},c_t \\in \\mathbb{R}^{B\\times H})$ are:\n",
    "$\n",
    "\\begin{aligned}\n",
    "[z_i, z_f, z_g, z_o] = x2g(x_t) + h2g(h_{t-1}) \\quad\\in \\mathbb{R}^{B\\times 4H},\\\n",
    "i_t &= \\sigma(z_i),\\qquad f_t=\\sigma(z_f),\\qquad g_t=\\tanh(z_g),\\qquad o_t=\\sigma(z_o),\\\n",
    "c_t &= f_t \\odot c_{t-1} + i_t \\odot g_t,\\\n",
    "h_t &= o_t \\odot \\tanh(c_t).\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Intuition: (i_t) decides how much new information (g_t) to write, (f_t) decides how much of the old cell (c_{t-1}) to keep, (o_t) decides how much of the updated cell to expose through (h_t).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d21e8264-8bef-4002-af8e-fb022891468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb643927-6944-4785-ac3a-935d1f08b17e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 4]),\n",
       " Parameter containing:\n",
       " tensor([[0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000]], requires_grad=True),\n",
       " torch.Size([20]),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2g = nn.Linear(n_embd, 4 * hidden_size, bias=True)\n",
    "torch.nn.init.constant_(x2g.weight, 0.500)\n",
    "torch.nn.init.zeros_(x2g.bias)\n",
    "x2g.weight.size(), x2g.weight, x2g.bias.size(), x2g.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cd4aed3-b539-488f-a4ea-0f081ebc100b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 5]),\n",
       " Parameter containing:\n",
       " tensor([[0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500, 0.2500]], requires_grad=True),\n",
       " torch.Size([20]),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2g = nn.Linear(hidden_size, 4 * hidden_size, bias=True)\n",
    "torch.nn.init.constant_(h2g.weight, 0.250)\n",
    "torch.nn.init.zeros_(h2g.bias)\n",
    "h2g.weight.size(), h2g.weight, h2g.bias.size(), h2g.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ea925e-09b9-423c-994c-28bb2061765d",
   "metadata": {},
   "source": [
    "since first pass, incoming weight (h_t) are seroed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f09c99f-ab91-4dcb-8f0b-295c3f2a30a2",
   "metadata": {},
   "source": [
    "first pass (future will loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2f7b252-dae3-4243-8b6c-ec16a292c1fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1111, 1.1111, 1.1111, 1.1111],\n",
       "        [1.1111, 1.1111, 0.0000, 1.1111]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_0 = x[:, 0, :]\n",
    "x_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0083fbeb-d96c-4643-ac4e-44ddd0ac852d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_t = torch.zeros(B_batch, hidden_size) \n",
    "h_t = torch.zeros(B_batch, hidden_size) \n",
    "c_t, h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c513c4b-9e62-411c-9ba8-8d8b3b3d2821",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_prev, c_prev = h_t, c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59708dfc-3e0c-4110-ae16-6b178b18cb69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 20]),\n",
       " tensor([[2.2222, 2.2222, 2.2222, 2.2222, 2.2222, 2.2222, 2.2222, 2.2222, 2.2222,\n",
       "          2.2222, 2.2222, 2.2222, 2.2222, 2.2222, 2.2222, 2.2222, 2.2222, 2.2222,\n",
       "          2.2222, 2.2222],\n",
       "         [1.6667, 1.6667, 1.6667, 1.6667, 1.6667, 1.6667, 1.6667, 1.6667, 1.6667,\n",
       "          1.6667, 1.6667, 1.6667, 1.6667, 1.6667, 1.6667, 1.6667, 1.6667, 1.6667,\n",
       "          1.6667, 1.6667]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gi = x2g(x_0)\n",
    "gi.size(), gi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f3b72c5-5547-4605-b0ca-0a1cc9cc0c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 20]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "        grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gh = h2g(h_prev)\n",
    "gh.size(), gh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58799445-c5d7-49a8-9314-58feefba9e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 20])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = gi + gh\n",
    "i_t, f_t, g_t, o_t = g.chunk(4, dim=-1)\n",
    "\n",
    "g.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e66698d-437d-4594-b81c-a7acf090d602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_t tensor([[2.2222, 2.2222, 2.2222, 2.2222, 2.2222],\n",
      "        [1.6667, 1.6667, 1.6667, 1.6667, 1.6667]], grad_fn=<SplitBackward0>) torch.Size([2, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.9022, 0.9022, 0.9022, 0.9022, 0.9022],\n",
       "        [0.8411, 0.8411, 0.8411, 0.8411, 0.8411]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('i_t', i_t, i_t.size())\n",
    "i_t = torch.sigmoid(i_t)\n",
    "i_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c14f6950-7d12-466e-a9e9-91245f4d9a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_t tensor([[2.2222, 2.2222, 2.2222, 2.2222, 2.2222],\n",
      "        [1.6667, 1.6667, 1.6667, 1.6667, 1.6667]], grad_fn=<SplitBackward0>) torch.Size([2, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.9022, 0.9022, 0.9022, 0.9022, 0.9022],\n",
       "        [0.8411, 0.8411, 0.8411, 0.8411, 0.8411]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('f_t', f_t, f_t.size())\n",
    "f_t = torch.sigmoid(f_t)\n",
    "f_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c142258c-745c-4222-8093-3f80c3291e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g_t tensor([[2.2222, 2.2222, 2.2222, 2.2222, 2.2222],\n",
      "        [1.6667, 1.6667, 1.6667, 1.6667, 1.6667]], grad_fn=<SplitBackward0>) torch.Size([2, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.9768, 0.9768, 0.9768, 0.9768, 0.9768],\n",
       "        [0.9311, 0.9311, 0.9311, 0.9311, 0.9311]], grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('g_t', g_t, g_t.size())\n",
    "g_t = torch.tanh(g_t)\n",
    "g_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c152a49-94dd-4d2b-a5b2-a2ac522a9b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o_t tensor([[2.2222, 2.2222, 2.2222, 2.2222, 2.2222],\n",
      "        [1.6667, 1.6667, 1.6667, 1.6667, 1.6667]], grad_fn=<SplitBackward0>) torch.Size([2, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.9022, 0.9022, 0.9022, 0.9022, 0.9022],\n",
       "        [0.8411, 0.8411, 0.8411, 0.8411, 0.8411]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('o_t', o_t, o_t.size())\n",
    "o_t = torch.sigmoid(o_t)\n",
    "o_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a3613db-8171-4b5d-927c-e213399d2fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5]),\n",
       " tensor([[0.8813, 0.8813, 0.8813, 0.8813, 0.8813],\n",
       "         [0.7832, 0.7832, 0.7832, 0.7832, 0.7832]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_t = (f_t * c_prev) + (i_t * g_t)\n",
    "c_t.size(), c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "729c70bb-abd9-462b-acae-98229d69b8b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5]),\n",
       " tensor([[0.7071, 0.7071, 0.7071, 0.7071, 0.7071],\n",
       "         [0.6545, 0.6545, 0.6545, 0.6545, 0.6545]], grad_fn=<TanhBackward0>))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_t_tanh = torch.tanh(c_t)\n",
    "c_t_tanh.size(), c_t_tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99d66f20-4f07-4ca8-b586-e9d82bca89db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5]),\n",
       " tensor([[0.6379, 0.6379, 0.6379, 0.6379, 0.6379],\n",
       "         [0.5505, 0.5505, 0.5505, 0.5505, 0.5505]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_t = o_t * c_t_tanh\n",
    "\n",
    "h_t.size(), h_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5088318f-4aa6-47cc-a3c5-3e633b517129",
   "metadata": {},
   "source": [
    "## now do it recursively for the rest of the batch size\n",
    "\n",
    "first we'll need to keep track of h_t for each loop for training, so let's start collecting them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5712a754-aaaf-411f-8465-8c9c69569ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[0.6379, 0.6379, 0.6379, 0.6379, 0.6379]],\n",
       " \n",
       "         [[0.5505, 0.5505, 0.5505, 0.5505, 0.5505]]],\n",
       "        grad_fn=<UnsqueezeBackward0>)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs = []\n",
    "hs.append(h_t.unsqueeze(1))\n",
    "hs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4124348-ff19-4342-bb51-4df571ef4c30",
   "metadata": {},
   "source": [
    "start at 1 since we did the \"0\" pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c0be513-11d4-4856-94cd-665d63b3d8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 1\n",
      "tensor([[1.7892, 1.7892, 1.7892, 1.7892, 1.7892],\n",
      "        [1.6855, 1.6855, 1.6855, 1.6855, 1.6855]], grad_fn=<AddBackward0>) tensor([[0.9017, 0.9017, 0.9017, 0.9017, 0.9017],\n",
      "        [0.8854, 0.8854, 0.8854, 0.8854, 0.8854]], grad_fn=<MulBackward0>)\n",
      "t: 2\n",
      "tensor([[2.5001, 2.5001, 2.5001, 2.5001, 2.5001],\n",
      "        [2.5901, 2.5901, 2.5901, 2.5901, 2.5901]], grad_fn=<AddBackward0>) tensor([[0.8915, 0.8915, 0.8915, 0.8915, 0.8915],\n",
      "        [0.9546, 0.9546, 0.9546, 0.9546, 0.9546]], grad_fn=<MulBackward0>)\n",
      "t: 3\n",
      "tensor([[3.2886, 3.2886, 3.2886, 3.2886, 3.2886],\n",
      "        [3.4738, 3.4738, 3.4738, 3.4738, 3.4738]], grad_fn=<AddBackward0>) tensor([[0.9390, 0.9390, 0.9390, 0.9390, 0.9390],\n",
      "        [0.9663, 0.9663, 0.9663, 0.9663, 0.9663]], grad_fn=<MulBackward0>)\n",
      "t: 4\n",
      "tensor([[4.1474, 4.1474, 4.1474, 4.1474, 4.1474],\n",
      "        [4.3314, 4.3314, 4.3314, 4.3314, 4.3314]], grad_fn=<AddBackward0>) tensor([[0.9671, 0.9671, 0.9671, 0.9671, 0.9671],\n",
      "        [0.9683, 0.9683, 0.9683, 0.9683, 0.9683]], grad_fn=<MulBackward0>)\n",
      "t: 5\n",
      "tensor([[4.8666, 4.8666, 4.8666, 4.8666, 4.8666],\n",
      "        [5.1626, 5.1626, 5.1626, 5.1626, 5.1626]], grad_fn=<AddBackward0>) tensor([[0.9465, 0.9465, 0.9465, 0.9465, 0.9465],\n",
      "        [0.9686, 0.9686, 0.9686, 0.9686, 0.9686]], grad_fn=<MulBackward0>)\n",
      "t: 6\n",
      "tensor([[5.5395, 5.5395, 5.5395, 5.5395, 5.5395],\n",
      "        [5.5948, 5.5948, 5.5948, 5.5948, 5.5948]], grad_fn=<AddBackward0>) tensor([[0.9453, 0.9453, 0.9453, 0.9453, 0.9453],\n",
      "        [0.9107, 0.9107, 0.9107, 0.9107, 0.9107]], grad_fn=<MulBackward0>)\n",
      "t: 7\n",
      "tensor([[6.3269, 6.3269, 6.3269, 6.3269, 6.3269],\n",
      "        [6.3712, 6.3712, 6.3712, 6.3712, 6.3712]], grad_fn=<AddBackward0>) tensor([[0.9678, 0.9678, 0.9678, 0.9678, 0.9678],\n",
      "        [0.9664, 0.9664, 0.9664, 0.9664, 0.9664]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for t in range(1,T):\n",
    "    x_t = x[:, t, :]\n",
    "    h_prev, c_prev = h_t, c_t\n",
    "    gi = x2g(x_t)\n",
    "    gh = h2g(h_prev)\n",
    "    g = gi + gh\n",
    "    i_t, f_t, g_t, o_t = g.chunk(4, dim=-1)\n",
    "    i_t = torch.sigmoid(i_t)\n",
    "    f_t = torch.sigmoid(f_t)\n",
    "    g_t = torch.tanh(g_t)\n",
    "    o_t = torch.sigmoid(o_t)\n",
    "\n",
    "    c_t = (f_t * c_prev) + (i_t * g_t)\n",
    "    \n",
    "    c_t_tanh = torch.tanh(c_t)\n",
    "    h_t = o_t * c_t_tanh\n",
    "\n",
    "    print(f't: {t}')\n",
    "    print(c_t, h_t)\n",
    "    hs.append(h_t.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de5f7269-332d-48e5-ba64-d36de54ac446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[0.6379, 0.6379, 0.6379, 0.6379, 0.6379]],\n",
       " \n",
       "         [[0.5505, 0.5505, 0.5505, 0.5505, 0.5505]]],\n",
       "        grad_fn=<UnsqueezeBackward0>),\n",
       " tensor([[[0.9017, 0.9017, 0.9017, 0.9017, 0.9017]],\n",
       " \n",
       "         [[0.8854, 0.8854, 0.8854, 0.8854, 0.8854]]],\n",
       "        grad_fn=<UnsqueezeBackward0>),\n",
       " tensor([[[0.8915, 0.8915, 0.8915, 0.8915, 0.8915]],\n",
       " \n",
       "         [[0.9546, 0.9546, 0.9546, 0.9546, 0.9546]]],\n",
       "        grad_fn=<UnsqueezeBackward0>),\n",
       " tensor([[[0.9390, 0.9390, 0.9390, 0.9390, 0.9390]],\n",
       " \n",
       "         [[0.9663, 0.9663, 0.9663, 0.9663, 0.9663]]],\n",
       "        grad_fn=<UnsqueezeBackward0>),\n",
       " tensor([[[0.9671, 0.9671, 0.9671, 0.9671, 0.9671]],\n",
       " \n",
       "         [[0.9683, 0.9683, 0.9683, 0.9683, 0.9683]]],\n",
       "        grad_fn=<UnsqueezeBackward0>),\n",
       " tensor([[[0.9465, 0.9465, 0.9465, 0.9465, 0.9465]],\n",
       " \n",
       "         [[0.9686, 0.9686, 0.9686, 0.9686, 0.9686]]],\n",
       "        grad_fn=<UnsqueezeBackward0>),\n",
       " tensor([[[0.9453, 0.9453, 0.9453, 0.9453, 0.9453]],\n",
       " \n",
       "         [[0.9107, 0.9107, 0.9107, 0.9107, 0.9107]]],\n",
       "        grad_fn=<UnsqueezeBackward0>),\n",
       " tensor([[[0.9678, 0.9678, 0.9678, 0.9678, 0.9678]],\n",
       " \n",
       "         [[0.9664, 0.9664, 0.9664, 0.9664, 0.9664]]],\n",
       "        grad_fn=<UnsqueezeBackward0>)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d583b860-2977-49cb-8945-9c60eeb3d479",
   "metadata": {},
   "source": [
    "Combine out recurring inputs into the 2 different batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5214c260-55b7-4c85-a1b2-97aa5fb09603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6379, 0.6379, 0.6379, 0.6379, 0.6379],\n",
       "         [0.9017, 0.9017, 0.9017, 0.9017, 0.9017],\n",
       "         [0.8915, 0.8915, 0.8915, 0.8915, 0.8915],\n",
       "         [0.9390, 0.9390, 0.9390, 0.9390, 0.9390],\n",
       "         [0.9671, 0.9671, 0.9671, 0.9671, 0.9671],\n",
       "         [0.9465, 0.9465, 0.9465, 0.9465, 0.9465],\n",
       "         [0.9453, 0.9453, 0.9453, 0.9453, 0.9453],\n",
       "         [0.9678, 0.9678, 0.9678, 0.9678, 0.9678]],\n",
       "\n",
       "        [[0.5505, 0.5505, 0.5505, 0.5505, 0.5505],\n",
       "         [0.8854, 0.8854, 0.8854, 0.8854, 0.8854],\n",
       "         [0.9546, 0.9546, 0.9546, 0.9546, 0.9546],\n",
       "         [0.9663, 0.9663, 0.9663, 0.9663, 0.9663],\n",
       "         [0.9683, 0.9683, 0.9683, 0.9683, 0.9683],\n",
       "         [0.9686, 0.9686, 0.9686, 0.9686, 0.9686],\n",
       "         [0.9107, 0.9107, 0.9107, 0.9107, 0.9107],\n",
       "         [0.9664, 0.9664, 0.9664, 0.9664, 0.9664]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.cat(hs, dim=1) \n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4c77db-2d10-454f-b512-92ac5b024e61",
   "metadata": {},
   "source": [
    "**Dropout** to fight vanishing / exploding gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "14428ec8-69e9-4605-a6fa-eb1012337be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 5]),\n",
       " tensor([[[0.7088, 0.7088, 0.7088, 0.7088, 0.7088],\n",
       "          [1.0018, 1.0018, 1.0018, 1.0018, 1.0018],\n",
       "          [0.9906, 0.0000, 0.9906, 0.9906, 0.9906],\n",
       "          [1.0434, 1.0434, 1.0434, 1.0434, 1.0434],\n",
       "          [1.0746, 1.0746, 1.0746, 1.0746, 1.0746],\n",
       "          [1.0517, 1.0517, 1.0517, 1.0517, 1.0517],\n",
       "          [1.0503, 1.0503, 1.0503, 1.0503, 1.0503],\n",
       "          [1.0754, 1.0754, 1.0754, 1.0754, 1.0754]],\n",
       " \n",
       "         [[0.6117, 0.6117, 0.6117, 0.6117, 0.6117],\n",
       "          [0.9837, 0.9837, 0.9837, 0.9837, 0.9837],\n",
       "          [1.0607, 1.0607, 1.0607, 0.0000, 1.0607],\n",
       "          [1.0737, 1.0737, 1.0737, 1.0737, 1.0737],\n",
       "          [0.0000, 1.0759, 1.0759, 1.0759, 1.0759],\n",
       "          [1.0763, 1.0763, 1.0763, 1.0763, 1.0763],\n",
       "          [1.0118, 1.0118, 1.0118, 1.0118, 0.0000],\n",
       "          [0.0000, 1.0738, 1.0738, 1.0738, 1.0738]]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = dropout(x)\n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22593c65-be72-436f-9e1c-8620c22c04ab",
   "metadata": {},
   "source": [
    "**Output Head**\n",
    "projects down from the hiden size to the vocab for us to get logits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "27e76d90-e130-4e4f-a221-18ee7461876e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([36, 5]),\n",
       " Parameter containing:\n",
       " tensor([[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]], requires_grad=True))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_head = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "torch.nn.init.ones_(lm_head.weight)\n",
    "lm_head.weight.size(), lm_head.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "19b23a27-073d-499e-a5d3-5d1cc5d42f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 36]),\n",
       " tensor([[[3.5441, 3.5441, 3.5441, 3.5441, 3.5441, 3.5441, 3.5441, 3.5441,\n",
       "           3.5441, 3.5441, 3.5441, 3.5441, 3.5441, 3.5441, 3.5441, 3.5441,\n",
       "           3.5441, 3.5441, 3.5441, 3.5441, 3.5441, 3.5441, 3.5441, 3.5441,\n",
       "           3.5441, 3.5441, 3.5441, 3.5441, 3.5441, 3.5441, 3.5441, 3.5441,\n",
       "           3.5441, 3.5441, 3.5441, 3.5441],\n",
       "          [5.0092, 5.0092, 5.0092, 5.0092, 5.0092, 5.0092, 5.0092, 5.0092,\n",
       "           5.0092, 5.0092, 5.0092, 5.0092, 5.0092, 5.0092, 5.0092, 5.0092,\n",
       "           5.0092, 5.0092, 5.0092, 5.0092, 5.0092, 5.0092, 5.0092, 5.0092,\n",
       "           5.0092, 5.0092, 5.0092, 5.0092, 5.0092, 5.0092, 5.0092, 5.0092,\n",
       "           5.0092, 5.0092, 5.0092, 5.0092],\n",
       "          [3.9624, 3.9624, 3.9624, 3.9624, 3.9624, 3.9624, 3.9624, 3.9624,\n",
       "           3.9624, 3.9624, 3.9624, 3.9624, 3.9624, 3.9624, 3.9624, 3.9624,\n",
       "           3.9624, 3.9624, 3.9624, 3.9624, 3.9624, 3.9624, 3.9624, 3.9624,\n",
       "           3.9624, 3.9624, 3.9624, 3.9624, 3.9624, 3.9624, 3.9624, 3.9624,\n",
       "           3.9624, 3.9624, 3.9624, 3.9624],\n",
       "          [5.2168, 5.2168, 5.2168, 5.2168, 5.2168, 5.2168, 5.2168, 5.2168,\n",
       "           5.2168, 5.2168, 5.2168, 5.2168, 5.2168, 5.2168, 5.2168, 5.2168,\n",
       "           5.2168, 5.2168, 5.2168, 5.2168, 5.2168, 5.2168, 5.2168, 5.2168,\n",
       "           5.2168, 5.2168, 5.2168, 5.2168, 5.2168, 5.2168, 5.2168, 5.2168,\n",
       "           5.2168, 5.2168, 5.2168, 5.2168],\n",
       "          [5.3728, 5.3728, 5.3728, 5.3728, 5.3728, 5.3728, 5.3728, 5.3728,\n",
       "           5.3728, 5.3728, 5.3728, 5.3728, 5.3728, 5.3728, 5.3728, 5.3728,\n",
       "           5.3728, 5.3728, 5.3728, 5.3728, 5.3728, 5.3728, 5.3728, 5.3728,\n",
       "           5.3728, 5.3728, 5.3728, 5.3728, 5.3728, 5.3728, 5.3728, 5.3728,\n",
       "           5.3728, 5.3728, 5.3728, 5.3728],\n",
       "          [5.2584, 5.2584, 5.2584, 5.2584, 5.2584, 5.2584, 5.2584, 5.2584,\n",
       "           5.2584, 5.2584, 5.2584, 5.2584, 5.2584, 5.2584, 5.2584, 5.2584,\n",
       "           5.2584, 5.2584, 5.2584, 5.2584, 5.2584, 5.2584, 5.2584, 5.2584,\n",
       "           5.2584, 5.2584, 5.2584, 5.2584, 5.2584, 5.2584, 5.2584, 5.2584,\n",
       "           5.2584, 5.2584, 5.2584, 5.2584],\n",
       "          [5.2516, 5.2516, 5.2516, 5.2516, 5.2516, 5.2516, 5.2516, 5.2516,\n",
       "           5.2516, 5.2516, 5.2516, 5.2516, 5.2516, 5.2516, 5.2516, 5.2516,\n",
       "           5.2516, 5.2516, 5.2516, 5.2516, 5.2516, 5.2516, 5.2516, 5.2516,\n",
       "           5.2516, 5.2516, 5.2516, 5.2516, 5.2516, 5.2516, 5.2516, 5.2516,\n",
       "           5.2516, 5.2516, 5.2516, 5.2516],\n",
       "          [5.3768, 5.3768, 5.3768, 5.3768, 5.3768, 5.3768, 5.3768, 5.3768,\n",
       "           5.3768, 5.3768, 5.3768, 5.3768, 5.3768, 5.3768, 5.3768, 5.3768,\n",
       "           5.3768, 5.3768, 5.3768, 5.3768, 5.3768, 5.3768, 5.3768, 5.3768,\n",
       "           5.3768, 5.3768, 5.3768, 5.3768, 5.3768, 5.3768, 5.3768, 5.3768,\n",
       "           5.3768, 5.3768, 5.3768, 5.3768]],\n",
       " \n",
       "         [[3.0586, 3.0586, 3.0586, 3.0586, 3.0586, 3.0586, 3.0586, 3.0586,\n",
       "           3.0586, 3.0586, 3.0586, 3.0586, 3.0586, 3.0586, 3.0586, 3.0586,\n",
       "           3.0586, 3.0586, 3.0586, 3.0586, 3.0586, 3.0586, 3.0586, 3.0586,\n",
       "           3.0586, 3.0586, 3.0586, 3.0586, 3.0586, 3.0586, 3.0586, 3.0586,\n",
       "           3.0586, 3.0586, 3.0586, 3.0586],\n",
       "          [4.9187, 4.9187, 4.9187, 4.9187, 4.9187, 4.9187, 4.9187, 4.9187,\n",
       "           4.9187, 4.9187, 4.9187, 4.9187, 4.9187, 4.9187, 4.9187, 4.9187,\n",
       "           4.9187, 4.9187, 4.9187, 4.9187, 4.9187, 4.9187, 4.9187, 4.9187,\n",
       "           4.9187, 4.9187, 4.9187, 4.9187, 4.9187, 4.9187, 4.9187, 4.9187,\n",
       "           4.9187, 4.9187, 4.9187, 4.9187],\n",
       "          [4.2427, 4.2427, 4.2427, 4.2427, 4.2427, 4.2427, 4.2427, 4.2427,\n",
       "           4.2427, 4.2427, 4.2427, 4.2427, 4.2427, 4.2427, 4.2427, 4.2427,\n",
       "           4.2427, 4.2427, 4.2427, 4.2427, 4.2427, 4.2427, 4.2427, 4.2427,\n",
       "           4.2427, 4.2427, 4.2427, 4.2427, 4.2427, 4.2427, 4.2427, 4.2427,\n",
       "           4.2427, 4.2427, 4.2427, 4.2427],\n",
       "          [5.3685, 5.3685, 5.3685, 5.3685, 5.3685, 5.3685, 5.3685, 5.3685,\n",
       "           5.3685, 5.3685, 5.3685, 5.3685, 5.3685, 5.3685, 5.3685, 5.3685,\n",
       "           5.3685, 5.3685, 5.3685, 5.3685, 5.3685, 5.3685, 5.3685, 5.3685,\n",
       "           5.3685, 5.3685, 5.3685, 5.3685, 5.3685, 5.3685, 5.3685, 5.3685,\n",
       "           5.3685, 5.3685, 5.3685, 5.3685],\n",
       "          [4.3035, 4.3035, 4.3035, 4.3035, 4.3035, 4.3035, 4.3035, 4.3035,\n",
       "           4.3035, 4.3035, 4.3035, 4.3035, 4.3035, 4.3035, 4.3035, 4.3035,\n",
       "           4.3035, 4.3035, 4.3035, 4.3035, 4.3035, 4.3035, 4.3035, 4.3035,\n",
       "           4.3035, 4.3035, 4.3035, 4.3035, 4.3035, 4.3035, 4.3035, 4.3035,\n",
       "           4.3035, 4.3035, 4.3035, 4.3035],\n",
       "          [5.3814, 5.3814, 5.3814, 5.3814, 5.3814, 5.3814, 5.3814, 5.3814,\n",
       "           5.3814, 5.3814, 5.3814, 5.3814, 5.3814, 5.3814, 5.3814, 5.3814,\n",
       "           5.3814, 5.3814, 5.3814, 5.3814, 5.3814, 5.3814, 5.3814, 5.3814,\n",
       "           5.3814, 5.3814, 5.3814, 5.3814, 5.3814, 5.3814, 5.3814, 5.3814,\n",
       "           5.3814, 5.3814, 5.3814, 5.3814],\n",
       "          [4.0473, 4.0473, 4.0473, 4.0473, 4.0473, 4.0473, 4.0473, 4.0473,\n",
       "           4.0473, 4.0473, 4.0473, 4.0473, 4.0473, 4.0473, 4.0473, 4.0473,\n",
       "           4.0473, 4.0473, 4.0473, 4.0473, 4.0473, 4.0473, 4.0473, 4.0473,\n",
       "           4.0473, 4.0473, 4.0473, 4.0473, 4.0473, 4.0473, 4.0473, 4.0473,\n",
       "           4.0473, 4.0473, 4.0473, 4.0473],\n",
       "          [4.2953, 4.2953, 4.2953, 4.2953, 4.2953, 4.2953, 4.2953, 4.2953,\n",
       "           4.2953, 4.2953, 4.2953, 4.2953, 4.2953, 4.2953, 4.2953, 4.2953,\n",
       "           4.2953, 4.2953, 4.2953, 4.2953, 4.2953, 4.2953, 4.2953, 4.2953,\n",
       "           4.2953, 4.2953, 4.2953, 4.2953, 4.2953, 4.2953, 4.2953, 4.2953,\n",
       "           4.2953, 4.2953, 4.2953, 4.2953]]], grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = lm_head(x)\n",
    "\n",
    "logits.shape, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1c01b9-d09b-4708-bd9e-3aceeeb9cb18",
   "metadata": {},
   "source": [
    "**Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8423d81b-3ceb-44a0-b8dd-7b7ffe5cb977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3dce85cc-5c7e-47bf-8f0e-e23b7f6aa337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16]),\n",
       " tensor([15, 32,  9,  5, 20, 30, 15, 11,  9,  6, 20,  5,  0, 13, 21,  0]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_flat = y.view(-1)\n",
    "y_flat.shape, y_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "03a2daf1-b8cb-4660-9b46-c60d6c95ce58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 36]),\n",
       " tensor([[3.5441, 3.5441, 3.5441, 3.5441, 3.5441, 3.5441, 3.5441, 3.5441, 3.5441,\n",
       "          3.5441, 3.5441, 3.5441, 3.5441, 3.5441, 3.5441, 3.5441, 3.5441, 3.5441,\n",
       "          3.5441, 3.5441, 3.5441, 3.5441, 3.5441, 3.5441, 3.5441, 3.5441, 3.5441,\n",
       "          3.5441, 3.5441, 3.5441, 3.5441, 3.5441, 3.5441, 3.5441, 3.5441, 3.5441],\n",
       "         [5.0092, 5.0092, 5.0092, 5.0092, 5.0092, 5.0092, 5.0092, 5.0092, 5.0092,\n",
       "          5.0092, 5.0092, 5.0092, 5.0092, 5.0092, 5.0092, 5.0092, 5.0092, 5.0092,\n",
       "          5.0092, 5.0092, 5.0092, 5.0092, 5.0092, 5.0092, 5.0092, 5.0092, 5.0092,\n",
       "          5.0092, 5.0092, 5.0092, 5.0092, 5.0092, 5.0092, 5.0092, 5.0092, 5.0092],\n",
       "         [3.9624, 3.9624, 3.9624, 3.9624, 3.9624, 3.9624, 3.9624, 3.9624, 3.9624,\n",
       "          3.9624, 3.9624, 3.9624, 3.9624, 3.9624, 3.9624, 3.9624, 3.9624, 3.9624,\n",
       "          3.9624, 3.9624, 3.9624, 3.9624, 3.9624, 3.9624, 3.9624, 3.9624, 3.9624,\n",
       "          3.9624, 3.9624, 3.9624, 3.9624, 3.9624, 3.9624, 3.9624, 3.9624, 3.9624],\n",
       "         [5.2168, 5.2168, 5.2168, 5.2168, 5.2168, 5.2168, 5.2168, 5.2168, 5.2168,\n",
       "          5.2168, 5.2168, 5.2168, 5.2168, 5.2168, 5.2168, 5.2168, 5.2168, 5.2168,\n",
       "          5.2168, 5.2168, 5.2168, 5.2168, 5.2168, 5.2168, 5.2168, 5.2168, 5.2168,\n",
       "          5.2168, 5.2168, 5.2168, 5.2168, 5.2168, 5.2168, 5.2168, 5.2168, 5.2168],\n",
       "         [5.3728, 5.3728, 5.3728, 5.3728, 5.3728, 5.3728, 5.3728, 5.3728, 5.3728,\n",
       "          5.3728, 5.3728, 5.3728, 5.3728, 5.3728, 5.3728, 5.3728, 5.3728, 5.3728,\n",
       "          5.3728, 5.3728, 5.3728, 5.3728, 5.3728, 5.3728, 5.3728, 5.3728, 5.3728,\n",
       "          5.3728, 5.3728, 5.3728, 5.3728, 5.3728, 5.3728, 5.3728, 5.3728, 5.3728],\n",
       "         [5.2584, 5.2584, 5.2584, 5.2584, 5.2584, 5.2584, 5.2584, 5.2584, 5.2584,\n",
       "          5.2584, 5.2584, 5.2584, 5.2584, 5.2584, 5.2584, 5.2584, 5.2584, 5.2584,\n",
       "          5.2584, 5.2584, 5.2584, 5.2584, 5.2584, 5.2584, 5.2584, 5.2584, 5.2584,\n",
       "          5.2584, 5.2584, 5.2584, 5.2584, 5.2584, 5.2584, 5.2584, 5.2584, 5.2584],\n",
       "         [5.2516, 5.2516, 5.2516, 5.2516, 5.2516, 5.2516, 5.2516, 5.2516, 5.2516,\n",
       "          5.2516, 5.2516, 5.2516, 5.2516, 5.2516, 5.2516, 5.2516, 5.2516, 5.2516,\n",
       "          5.2516, 5.2516, 5.2516, 5.2516, 5.2516, 5.2516, 5.2516, 5.2516, 5.2516,\n",
       "          5.2516, 5.2516, 5.2516, 5.2516, 5.2516, 5.2516, 5.2516, 5.2516, 5.2516],\n",
       "         [5.3768, 5.3768, 5.3768, 5.3768, 5.3768, 5.3768, 5.3768, 5.3768, 5.3768,\n",
       "          5.3768, 5.3768, 5.3768, 5.3768, 5.3768, 5.3768, 5.3768, 5.3768, 5.3768,\n",
       "          5.3768, 5.3768, 5.3768, 5.3768, 5.3768, 5.3768, 5.3768, 5.3768, 5.3768,\n",
       "          5.3768, 5.3768, 5.3768, 5.3768, 5.3768, 5.3768, 5.3768, 5.3768, 5.3768],\n",
       "         [3.0586, 3.0586, 3.0586, 3.0586, 3.0586, 3.0586, 3.0586, 3.0586, 3.0586,\n",
       "          3.0586, 3.0586, 3.0586, 3.0586, 3.0586, 3.0586, 3.0586, 3.0586, 3.0586,\n",
       "          3.0586, 3.0586, 3.0586, 3.0586, 3.0586, 3.0586, 3.0586, 3.0586, 3.0586,\n",
       "          3.0586, 3.0586, 3.0586, 3.0586, 3.0586, 3.0586, 3.0586, 3.0586, 3.0586],\n",
       "         [4.9187, 4.9187, 4.9187, 4.9187, 4.9187, 4.9187, 4.9187, 4.9187, 4.9187,\n",
       "          4.9187, 4.9187, 4.9187, 4.9187, 4.9187, 4.9187, 4.9187, 4.9187, 4.9187,\n",
       "          4.9187, 4.9187, 4.9187, 4.9187, 4.9187, 4.9187, 4.9187, 4.9187, 4.9187,\n",
       "          4.9187, 4.9187, 4.9187, 4.9187, 4.9187, 4.9187, 4.9187, 4.9187, 4.9187],\n",
       "         [4.2427, 4.2427, 4.2427, 4.2427, 4.2427, 4.2427, 4.2427, 4.2427, 4.2427,\n",
       "          4.2427, 4.2427, 4.2427, 4.2427, 4.2427, 4.2427, 4.2427, 4.2427, 4.2427,\n",
       "          4.2427, 4.2427, 4.2427, 4.2427, 4.2427, 4.2427, 4.2427, 4.2427, 4.2427,\n",
       "          4.2427, 4.2427, 4.2427, 4.2427, 4.2427, 4.2427, 4.2427, 4.2427, 4.2427],\n",
       "         [5.3685, 5.3685, 5.3685, 5.3685, 5.3685, 5.3685, 5.3685, 5.3685, 5.3685,\n",
       "          5.3685, 5.3685, 5.3685, 5.3685, 5.3685, 5.3685, 5.3685, 5.3685, 5.3685,\n",
       "          5.3685, 5.3685, 5.3685, 5.3685, 5.3685, 5.3685, 5.3685, 5.3685, 5.3685,\n",
       "          5.3685, 5.3685, 5.3685, 5.3685, 5.3685, 5.3685, 5.3685, 5.3685, 5.3685],\n",
       "         [4.3035, 4.3035, 4.3035, 4.3035, 4.3035, 4.3035, 4.3035, 4.3035, 4.3035,\n",
       "          4.3035, 4.3035, 4.3035, 4.3035, 4.3035, 4.3035, 4.3035, 4.3035, 4.3035,\n",
       "          4.3035, 4.3035, 4.3035, 4.3035, 4.3035, 4.3035, 4.3035, 4.3035, 4.3035,\n",
       "          4.3035, 4.3035, 4.3035, 4.3035, 4.3035, 4.3035, 4.3035, 4.3035, 4.3035],\n",
       "         [5.3814, 5.3814, 5.3814, 5.3814, 5.3814, 5.3814, 5.3814, 5.3814, 5.3814,\n",
       "          5.3814, 5.3814, 5.3814, 5.3814, 5.3814, 5.3814, 5.3814, 5.3814, 5.3814,\n",
       "          5.3814, 5.3814, 5.3814, 5.3814, 5.3814, 5.3814, 5.3814, 5.3814, 5.3814,\n",
       "          5.3814, 5.3814, 5.3814, 5.3814, 5.3814, 5.3814, 5.3814, 5.3814, 5.3814],\n",
       "         [4.0473, 4.0473, 4.0473, 4.0473, 4.0473, 4.0473, 4.0473, 4.0473, 4.0473,\n",
       "          4.0473, 4.0473, 4.0473, 4.0473, 4.0473, 4.0473, 4.0473, 4.0473, 4.0473,\n",
       "          4.0473, 4.0473, 4.0473, 4.0473, 4.0473, 4.0473, 4.0473, 4.0473, 4.0473,\n",
       "          4.0473, 4.0473, 4.0473, 4.0473, 4.0473, 4.0473, 4.0473, 4.0473, 4.0473],\n",
       "         [4.2953, 4.2953, 4.2953, 4.2953, 4.2953, 4.2953, 4.2953, 4.2953, 4.2953,\n",
       "          4.2953, 4.2953, 4.2953, 4.2953, 4.2953, 4.2953, 4.2953, 4.2953, 4.2953,\n",
       "          4.2953, 4.2953, 4.2953, 4.2953, 4.2953, 4.2953, 4.2953, 4.2953, 4.2953,\n",
       "          4.2953, 4.2953, 4.2953, 4.2953, 4.2953, 4.2953, 4.2953, 4.2953, 4.2953]],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_flat = logits.view(-1, logits.size(-1))\n",
    "logits_flat.shape, logits_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4b287d1b-c82f-4f51-bb49-7a542ddd3287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([]), tensor(3.5835, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.cross_entropy(logits_flat, y_flat)\n",
    "loss.shape, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d587d633-a3ed-49e3-a90a-9c505b079665",
   "metadata": {},
   "source": [
    "## Back Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0455dc7a-fab4-4844-bf37-2ac311226907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_head.zero_grad()\n",
    "h2g.zero_grad()\n",
    "x2g.zero_grad()\n",
    "wte.zero_grad()\n",
    "\n",
    "\n",
    "# validate gradients\n",
    "lm_head.weight.grad, wte.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c0b7acd2-55bc-4fe6-9c41-1ddc0ee05eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "119fa3fb-0e8c-4db3-9df8-4a1b361279d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0240, -0.1084, -0.1066, -0.1085, -0.1084],\n",
       "         [ 0.0240,  0.0260,  0.0277,  0.0259,  0.0260],\n",
       "         [ 0.0240,  0.0260,  0.0277,  0.0259,  0.0260],\n",
       "         [ 0.0240,  0.0260,  0.0277,  0.0259,  0.0260],\n",
       "         [ 0.0240,  0.0260,  0.0277,  0.0259,  0.0260],\n",
       "         [-0.1083, -0.1063, -0.1046, -0.1064, -0.1064],\n",
       "         [-0.0375, -0.0355, -0.0338, -0.0356, -0.0355],\n",
       "         [ 0.0240,  0.0260,  0.0277,  0.0259,  0.0260],\n",
       "         [ 0.0240,  0.0260,  0.0277,  0.0259,  0.0260],\n",
       "         [-0.0762, -0.0122, -0.0724, -0.0743, -0.0742],\n",
       "         [ 0.0240,  0.0260,  0.0277,  0.0259,  0.0260],\n",
       "         [-0.0432, -0.0412, -0.0395, -0.0413, -0.0413],\n",
       "         [ 0.0240,  0.0260,  0.0277,  0.0259,  0.0260],\n",
       "         [-0.0433, -0.0413, -0.0396, -0.0414, -0.0413],\n",
       "         [ 0.0240,  0.0260,  0.0277,  0.0259,  0.0260],\n",
       "         [-0.0860, -0.0839, -0.0822, -0.0841, -0.0840],\n",
       "         [ 0.0240,  0.0260,  0.0277,  0.0259,  0.0260],\n",
       "         [ 0.0240,  0.0260,  0.0277,  0.0259,  0.0260],\n",
       "         [ 0.0240,  0.0260,  0.0277,  0.0259,  0.0260],\n",
       "         [ 0.0240,  0.0260,  0.0277,  0.0259,  0.0260],\n",
       "         [-0.1095, -0.1075, -0.1057, -0.0413, -0.1075],\n",
       "         [-0.0393, -0.0372, -0.0355, -0.0374,  0.0260],\n",
       "         [ 0.0240,  0.0260,  0.0277,  0.0259,  0.0260],\n",
       "         [ 0.0240,  0.0260,  0.0277,  0.0259,  0.0260],\n",
       "         [ 0.0240,  0.0260,  0.0277,  0.0259,  0.0260],\n",
       "         [ 0.0240,  0.0260,  0.0277,  0.0259,  0.0260],\n",
       "         [ 0.0240,  0.0260,  0.0277,  0.0259,  0.0260],\n",
       "         [ 0.0240,  0.0260,  0.0277,  0.0259,  0.0260],\n",
       "         [ 0.0240,  0.0260,  0.0277,  0.0259,  0.0260],\n",
       "         [ 0.0240,  0.0260,  0.0277,  0.0259,  0.0260],\n",
       "         [-0.0417, -0.0397, -0.0380, -0.0399, -0.0398],\n",
       "         [ 0.0240,  0.0260,  0.0277,  0.0259,  0.0260],\n",
       "         [-0.0386, -0.0366, -0.0349, -0.0367, -0.0367],\n",
       "         [ 0.0240,  0.0260,  0.0277,  0.0259,  0.0260],\n",
       "         [ 0.0240,  0.0260,  0.0277,  0.0259,  0.0260],\n",
       "         [ 0.0240,  0.0260,  0.0277,  0.0259,  0.0260]]),\n",
       " tensor([[9.4943e-10, 9.4943e-10, 9.4943e-10, 9.4943e-10],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [2.5746e-09, 2.5746e-09, 2.5746e-09, 2.5746e-09],\n",
       "         [9.0144e-10, 9.0144e-10, 9.0144e-10, 9.0144e-10],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [5.4284e-09, 5.4284e-09, 2.4594e-09, 5.4284e-09],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.0006e-08, 1.0006e-08, 0.0000e+00, 1.0006e-08],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [2.0383e-09, 2.0383e-09, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [3.7163e-09, 3.7163e-09, 3.7163e-09, 3.7163e-09],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.6787e-09, 3.9578e-09, 3.9578e-09, 3.9578e-09],\n",
       "         [1.6406e-09, 1.6406e-09, 1.6406e-09, 1.6406e-09],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [1.5446e-09, 1.5446e-09, 0.0000e+00, 1.5446e-09],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 3.4116e-09, 3.4116e-09, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [4.6491e-09, 4.6491e-09, 4.6491e-09, 4.6491e-09]]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_head.weight.grad, wte.weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb42053-0ac1-4a63-bc94-926a8f22eeb8",
   "metadata": {},
   "source": [
    "**Gradient clipping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "df8265d8-4b8f-4ca5-807d-98c8cb5192c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5869e-08)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.utils.clip_grad_norm_(lm_head.parameters(), 1.0)\n",
    "nn.utils.clip_grad_norm_(h2g.parameters(), 1.0)\n",
    "nn.utils.clip_grad_norm_(x2g.parameters(), 1.0)\n",
    "nn.utils.clip_grad_norm_(wte.parameters(), 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "785ebbb2-5581-43df-a8ca-82916426aa0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5812)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.utils.clip_grad_norm_(lm_head.parameters(), 1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8564d2f-34d0-4526-80b7-69337e2a7549",
   "metadata": {},
   "source": [
    "## Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b11f32-0c10-480d-9690-402132913444",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Huge learning rate to emphasize\n",
    "learning_rate = 5.000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c59984-6d36-4e78-b7fc-cf4623294915",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    lm_head.weight -= learning_rate * lm_head.weight.grad\n",
    "lm_head.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44461eb7-82b4-4383-afab-ef1c18203289",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    h2g.weight -= learning_rate * h2g.weight.grad\n",
    "    h2g.bias -= learning_rate * h2g.bias.grad\n",
    "h2g.weight, h2g.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24badddf-11c7-4c52-a31b-d7ef1912c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x2g.weight -= learning_rate * x2g.weight.grad\n",
    "    x2g.bias -= learning_rate * x2g.bias.grad\n",
    "x2g.weight, x2g.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f58304-61da-46de-862d-b7e807e61d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    wte.weight -= learning_rate * wte.weight.grad\n",
    "wte.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0ff10f-f64f-4801-b9ea-c40e15816684",
   "metadata": {},
   "source": [
    "## Forward Pass with Updated Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00925f19-c83a-4549-b4ca-94f92ad301f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = tok_for_training[:-1].view(B, T)\n",
    "x_2, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027c68d0-cf29-431d-85d2-0d0276e55c70",
   "metadata": {},
   "source": [
    "## Input projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff392189-88e9-46a8-9ce1-b1aacad80e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = wte(x_2)\n",
    "x.shape, x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522598e4-600b-49ac-aca8-5ce33a9f8c8e",
   "metadata": {},
   "source": [
    "**Dropout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0c0ab4-a8da-4b22-ba93-92070f03527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dropout(x)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a5ddc4-503d-4a8c-8635-2a6b26d4750e",
   "metadata": {},
   "source": [
    "**Recurrent Block** Collapsed Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e6d785-4282-431d-829b-e3ade8326816",
   "metadata": {},
   "source": [
    "h_t, c_t still resets to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a88d1ae-b4e8-46e0-9538-f94589005472",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_t = torch.zeros(B_batch, hidden_size) \n",
    "h_t = torch.zeros(B_batch, hidden_size) \n",
    "hs = []\n",
    "c_t, h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ce97ab-1e58-4c12-b558-f9c216b341e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(T):\n",
    "    x_t = x[:, t, :]\n",
    "    h_prev, c_prev = h_t, c_t\n",
    "    gi = x2g(x_t)\n",
    "    gh = h2g(h_prev)\n",
    "    g = gi + gh\n",
    "    i_t, f_t, g_t, o_t = g.chunk(4, dim=-1)\n",
    "    i_t = torch.sigmoid(i_t)\n",
    "    f_t = torch.sigmoid(f_t)\n",
    "    g_t = torch.tanh(g_t)\n",
    "    o_t = torch.sigmoid(o_t)\n",
    "\n",
    "    c_t = (f_t * c_prev) + (i_t * g_t)\n",
    "    \n",
    "    c_t_tanh = torch.tanh(c_t)\n",
    "    h_t = o_t * c_t_tanh\n",
    "\n",
    "    print(f't: {t}')\n",
    "    print(c_t, h_t)\n",
    "    hs.append(h_t.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3232e9b6-7205-4e1a-982b-f67d9352d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f5de2c-9cfb-4292-a7e9-4e45c2e2e913",
   "metadata": {},
   "source": [
    "combine weights back together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e35fdc-c488-4dac-87a1-dcc12a0e2abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.cat(hs, dim=1) \n",
    "x.size(), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8612c53d-c55b-4940-852d-79498d489245",
   "metadata": {},
   "source": [
    "**Dropout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4b0ebe-0dab-458c-bba1-0d0617a9b294",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dropout(x)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f1e47d-27d3-4039-8ea1-208d5fc30f29",
   "metadata": {},
   "source": [
    "**Head**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2c2354-3b5c-4490-ac42-fd8474355857",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = lm_head(x)\n",
    "logits.shape, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ed1cae-6af9-4211-82b1-8930196c48c3",
   "metadata": {},
   "source": [
    "### Updated Loss calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c101fbc-97fc-4dc2-b99e-1cff59d22f5d",
   "metadata": {},
   "source": [
    "Now we'll calculate the updated loss.  Our first pass's loss was 3.5835. Since we're passing through the same example and used a fairly high learning rate we should see a significant improvement with just 1 learning pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a12e542-dda9-4cb4-9611-bcb157515806",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c2eb99-d188-4de7-a4ee-27917e3cec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_flat = y.view(-1)\n",
    "logits_flat = logits.view(-1, logits.size(-1))\n",
    "updated_loss = F.cross_entropy(logits_flat, y_flat)\n",
    "print(updated_loss.shape, updated_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53363221-ba7b-49f8-8f6a-88c6db647277",
   "metadata": {},
   "outputs": [],
   "source": [
    "f'1 round of training resulted in an loss improvment of {loss.item() - updated_loss.item():.4f}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f56dfda-a142-40b3-b66f-60cb6cae0a8e",
   "metadata": {},
   "source": [
    "# SUCCESS!\n",
    "Our training improved the loss by about **~1.3%%** (amount may vary since we didn't set a seed). There are flaws with this, mainly passing the same example through a second time, but this helps show the fundamentals of what learning does inside a GPT-2 style model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e4ccfa-7d97-4c63-b46b-8ac6a44a5dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
